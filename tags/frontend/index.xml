<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>frontend on Cosmo</title>
    <link>https://cosmo1995.github.io/tags/frontend/</link>
    <description>Recent content in frontend on Cosmo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://cosmo1995.github.io/tags/frontend/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kaldi中的特征变换</title>
      <link>https://cosmo1995.github.io/p/kaldi%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2/</guid>
      <description>Kaldi中的特征空间变换 Kaldi中的特征空间变化的方法包含两类：
Speaker Independent：
 线性判别分析LDA(Linear Discriminant Analysis) 拼帧Frame Splicing &amp;amp; 差分Delta 最大似然线性变换MLLT(Maximum Likelihood Linear Transform)/半绑定协方差STC(Semi-tied Covariance)  Speaker Adaptive：
 特征空间最大似然线性回归fMLLR(feature-space Maximum Likelihood Linear Regression) 线性声道长度归一化LVTLN(Linear Vocal Tract Length Normalization) 倒谱均值方差归一化CMVN(Cepstral Mean and Variance Normalization)  Speaker Independent： Delta 为了更好的识别语音，我们在静态MFCC特征的基础上附加一阶（delta）和二阶（delta-delta）差分，以补偿HMM的声学模型做出的条件独立性假设。
LDA+MLLT 首先对原始的MFCC特征进行拼帧(Frame Splicing)，将相邻的N帧拼接起来，如下面的例子将7帧拼接到一起。
steps/train_lda_mllt.sh --cmd &amp;#34;train.cmd&amp;#34; \  --splice-opts &amp;#34;--left-context=3 --right-context=3&amp;#34; \  2500 15000 $data $lang $tri1_ali $tri_2b 在HMM-GMM模型中，通过GMM对HMM的发射概率进行建模。为了减少GMM模型的参数量，我们通常使用对角协方差(diagonal covariance)矩阵，这就要特征的各个维度之间是独立的(independent)。MFCC特征即使通过DCT去相关后仍存在一定的相关性，而拼帧后的特征间的相关性更强了。
因此将拼接后的帧通过LDA进行降维和去相关。LDA的核心思想为投影后类内方差最小，类间的方差最大，这里的类指的是声学模型的状态(如pdf-id)。
LDA降维后的特征通过MLLT再次进行去相关，MLLT引入了一种新形式的协方差矩阵，这种方法的每个高斯分量有两个方差矩阵:
 对角协方差矩阵$\Sigma_{diag}^{(m)}$ 半绑定非对角矩阵$H$，可以在多个高斯分量之间共享  第m个高斯分量的协方差矩阵可以描述为： $$ \Sigma^{(m)}=H\Sigma^{(m)}{diag}H^{T} $$ 令$A=H^{-1}$，则有： $$ \Sigma^{(m)-1}=A^{T}\Sigma^{(m-1)}{diag}A $$ 训练过程为EM算法：</description>
    </item>
    
    <item>
      <title>MFCC和FBANK特征</title>
      <link>https://cosmo1995.github.io/p/mfcc%E5%92%8Cfbank%E7%89%B9%E5%BE%81/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/mfcc%E5%92%8Cfbank%E7%89%B9%E5%BE%81/</guid>
      <description>MFCC（Mel Frequency Cepstral Coefficent）和FBANK特征在语音和说话人识别中被广泛使用。FBANK和MFCC计算的主要过程一致，MFCC是在FBANK的基础上做DCT变换。MFCC特征提取的过程可以分为预处理和倒谱两部分。
预处理 预加重 第一步是对语音信号应用预加重，以放大高频部分。将语音信号通过一个高通滤波器：
$$ y(t) = x(t) - \alpha x(t-1) $$ 其中滤波系数$\alpha$一般取0.95或0.97。
预加重滤波器在几种方面有用：（1）平衡频谱，因为高频通常比低频具有较小的能量；（2）避免在傅立叶变换操作期间出现数值问题；（3）还可改善信噪比（SNR）。
分帧 因为语音信号是快速变化的，而FFT适用于分析平稳的信号。为了简化起见，我们假设音频信号在短时间范围内变化不大（当我们说它不变时，我们指的是统计上的，即统计上是平稳的，显然样本在不断变化。即使是短时间尺度）。我们将语音分成20-40ms帧（一般取帧长为25ms），如果帧过短，将没有足够的样本来获得可靠的频谱估计；如果帧过长，则信号在整个帧中变化太大。 为确保声学特征参数的平滑性，帧移一般为10ms，即相邻两帧之间有15ms的重叠。
加窗 特征提取时，每次取长为25ms的语音，进行离散傅立叶变换计算出一帧，接着步移10ms继续计算下一帧，相当于加了矩形窗。而棱角分明的矩形窗容易造成频谱泄露，可以选择使用汉明窗（Hamming Window）、汉宁窗(Hanning Window)等。
汉明窗： $$ w[n]=(1-\alpha)-\alpha cos(\frac{2\pi n}{N-1}) $$ 实践中$\alpha$一般取0.46，其中$0\leq n \leq N-1$，$N$为窗的长度。
通过应用汉明窗，可以降低傅立叶变换后旁瓣的强度(主瓣是变换为频谱之后振幅最大的那个波峰部分，而周围的小的波峰部分叫旁瓣)，取得更高质量的频谱。
这里也解释了为什么要帧移是10ms, 相邻帧之间有15ms的重叠, 由于帧与帧连接处的信号因为加窗而弱化。
倒谱 在对原始语音进行预加重、分帧、加窗等预处理后，就可以进行倒谱分析了。我们对每一帧语音进行FFT，得到每一帧语音的频谱图(spectrum)。

频谱图中，峰值表示语音的主要频率成分，我们把这些峰值称为共振峰（formants），而共振峰包含了声音的辨识属性。共振峰点的通过一条平滑曲线连接起来，这条曲线叫做频谱的包络（Spectral Envelope）。

我们可以这么理解，将原始的频谱由两部分组成：包络和细节。包络包含了语音的主要信息，因此我们的目的就是把包络部分提取出来，即分离频谱的包络和细节。
频谱用$X[k]$表示，包络为$H[k]$，细节为$E[k]$，则有 $$ X[k]=H[k]*E[k] $$ 在两边取对数，将乘法转换为加法，便于分离，有： $$ logX[k]=logH[k] + logE[k] $$ 因此我们的目的是在给定$logX[k]$的基础上，求$logH[k]$和$logE[k]$，使得$logX[k]=logH[k] + logE[k]$。
为了达到这一目的，我们在对数频谱上做FFT，频谱上做FFT相当于IFFT。在对数频谱上做IFFT就相当于在一个伪频率（pseudo-frequency）坐标轴上面描述信号。

由上面这个图我们可以看到，包络是主要是低频成分，我们把它看成是一个每秒4个周期的正弦信号。这样我们在伪坐标轴上面的4Hz的地方给它一个峰值。而频谱的细节部分主要是高频。我们把它看成是一个每秒100个周期的正弦信号。这样我们在伪坐标轴上面的100Hz的地方给它一个峰值。把它俩叠加起来就是原来的频谱信号了。
在实际中我们已经知道$logX[k]$，所以可以得到$x[k]$。而$h[k]$是$x[k]$的低频部分，那么我们将$x[k]$通过一个低通滤波器就可以得到$h[k]$了。
$x[k]$实际上就是倒谱Cepstrum（这个是一个新造出来的词，把频谱的单词spectrum的前面四个字母顺序倒过来就是倒谱的单词了）。而我们所关心的$h[k]$就是倒谱的低频部分，它在语音识别中被广泛用于描述特征。
那现在总结下倒谱分析，它实际上是这样一个过程：
1）将原语音信号经过傅里叶变换得到频谱：$X[k]=H[k]E[k]$
只考虑幅度就是：$||X[k] ||=||H[k]||\ ||E[k]||$
2）我们在两边取对数：$log||X[k] ||= log ||H[k] ||+ log ||E[k] ||$</description>
    </item>
    
  </channel>
</rss>
