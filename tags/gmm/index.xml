<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gmm on Cosmo</title>
    <link>https://cosmo1995.github.io/tags/gmm/</link>
    <description>Recent content in gmm on Cosmo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://cosmo1995.github.io/tags/gmm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>HMM-GMM和HMM-DNN</title>
      <link>https://cosmo1995.github.io/p/hmm-gmm%E5%92%8Chmm-dnn/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/hmm-gmm%E5%92%8Chmm-dnn/</guid>
      <description>HMM-GMM模型 模型分析 原始音频信号，经过【预加重-分帧-加窗-fft-mel滤波器组-DCT】，得到MFCC特征作为输入信号。假设有一句80帧的原始语音，这80帧特征序列就是观察序列：
$$ O = [o_1,o_2,&amp;hellip;,o_T],\ T=80 $$ 给定观察序列$O$，估计HMM-GMM模型的参数，这就是HMM中的训练问题。
 输入：$O = [o_1,o_2,&amp;hellip;,o_T]$，即80帧MFCC特征 目标：估计HMM-GMM模型参数$\lambda=(A,B)$，使得$P(O|\lambda)$值最大 输出：通过模型计算每一帧属于S IH K S这四个音素中某一个状态(3状态)的概率  A是状态转移概率，B是观察概率，也就是发射概率。我们使用GMM模型对观察概率建模，所以HMM-GMM模型的参数：
 HMM参数：状态转移概率 GMM参数：高斯分量系数、均值向量和协方差矩阵  同时，需要额外说明的是：HMM-GMM模型的训练是无监督的。因为我们并不知道哪一帧输入特征对应哪个音素的哪一个状态。训练的目的就是找到帧对应状态的情况，并更新状态的gmm参数。把每一帧都归到某个状态上，本质上是进行聚类，是无监督训练。
单音素GMM-HMM模型的训练通过Viterbi训练(嵌入式训练)，把“S IH K S”对应的GMM模型嵌入到整段音频中去训练。
模型训练 对于HMM和GMM这种含有隐变量的模型，我们使用EM算法进行训练。训练的步骤如下图：

初始化对齐 初始化对齐的目的是为Viterbi对齐提供初始参数A、B。
一开始不知道一段语音的哪些帧对应哪些状态，我们就进行平均分配。前面提到的“ six”语音一共80帧，包含四个音素“S IH K S”，每个音素分配到20帧，每个音素又有三个状态组成，每个状态分配6或者7帧。这样就初始化了每个状态对应的输入数据。
就是假设前0-20帧数据都是“S”这个音素的发音，20-40帧数据都是“IH”这个音素的发音，40-60帧是“K”这个音素的发音，60-80是“S”这个音素的发音。但这只是一个假设，事实到底如此我们还不知道。我们可以在这个初始对齐下进一步优化。
初始化模型 HMM模型参数$λ=(A,B,\Pi)$。
 $A$：在初始化对齐后就可以统计出状态1-&amp;gt;状态1的转移次数，状态1-&amp;gt;状态2的转移次数，转移次数/总转移次数就是转移概率。 $B$：初始化后，状态1对应前6帧数据，根据这6帧数据来计算状态1对应的GMM模型参数，得到初始均值向量和协方差矩阵 。K-Means来初始化GMM的高斯分量系数。 $\Pi$：就是[1,0,0,0&amp;hellip;]，一开始在状态1的概率是100%。在语音识别应用中由于HMM是从左到右的模型，第一个必然是状态1，即$P(q_0=1)=1$。所以没有$\Pi$这个参数了。  重新对齐 需要重新对齐，向真实情况逼近的重新对齐。viterbi算法根据初始化模型$λ=(A,B,Π)$来计算。它记录每个时刻的每个可能状态的之前最优路径概率，同时记录最优路径的前一个状态，不断向后迭代，找出最后一个时间点的最大概率值对应的状态，如何向前回溯，得到最优路径。得到最优路径就得到最优的状态转移情况，哪些帧对应哪些状态就变了。
转移概率A就变了。哪些帧对应哪些状态变了导致状态对应的GMM参数自然就变了，也可以跟着更新均值 和方差，即发射概率B变了。
迭代 新的A和新的B又可以进行下一次的Viterbi算法，寻找新的最优路径，得到新的对齐，新的对齐继续改变着参数A、B。如此循环迭代直到收敛或者达到固定的轮数，则GMM-HMM模型训练完成。
HMM-DNN模型 HMM-GMM建模能力有限，无法准确的表征语音内部复杂的结构，所以识别率低。随着深度学习的崛起，研究人员将其逐步应用于语音识别中。最开始便是DNN代替了GMM来进行观察状态概率的输出，实现HMM-DNN声学模型框架，大大提高了识别率。
GMM与DNN对比 HMM-DNN用DNN替换了GMM来对输入语音信号的观察概率进行建模。
  GMM为了减少参数量，一般使用对角协方差矩阵建模，要求特征维度间的独立性，因此GMM使用的特征是MFCC，这个特征已经做了去相关性处理；DNN使用的特征是FBank，这个特征保持着相关性。
  GMM是生成模型，采用无监督学习；DNN是判别模型，采用有监督学习。
  
（DNN输入可采用连续的拼接帧）</description>
    </item>
    
    <item>
      <title>Kaldi源码之gmm_2</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_2/</link>
      <pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_2/</guid>
      <description>estimate-full-gmm和estimate-diag-gmm用来对GMM进行极大似然估计(MLE)，二者仅在协方差矩阵数据结构不同，下面均以estimate-full-gmm为例。
GMM GMM的分布函数为
$$ p(x|\mu,\Sigma) = \sum_{m=1}^{M} c_m \mathcal{N} (x;\mu_m,\Sigma_m) $$
其中$c_m$为第$m$个component的混合系数,$\sum_{m=1}^M c_m =1 $。
定义指示函数$1_{[z_t=m]}$，如果$x_t$来自第m个component，则$1_{[z_t=m]} = 1$，否则$1_{[z_t=m]} = 0$。
现在假设$1_{[z_t=m]}$已知，即$x_t$属于哪一个component是已知的话，我们就可以统计每一个component对应的帧数：
$$ N_m=\sum^N_{t=1} 1_{[z_t=m]} $$
那么GMM参数可以估计为：
$$ \begin {aligned} &amp;amp;c_m = \frac{\sum_{t=1}^N 1_{[z_t=m]}}{N} = \frac{N_m}{N} \cr &amp;amp;\mu_m= \frac{\sum_{t=1}^N 1_{[z_t=m]} \mathbf{x_t}}{N_m} \cr &amp;amp;\Sigma_m= \frac{\sum_{t=1}^N 1_{[z_t=m]}(\mathbf{x_t}-\mu_m)(\mathbf{x_t}-\mu_m)^T}{N_m} &amp;amp;\end {aligned} $$
然而$1_{[z_t=m]}$是未知的，我们并不知道每一帧$x_t$属于哪一个component。但是，我们可以计算后验概率$P(m|x_t)$，即第m个 component生成$x_t$的概率。 $$ P(m|x_t) = \frac{P(x_t|m)P(m)} {P(x_t)} = \frac{P(x_t|m)P(m)} {\sum^M_{n=1}P(x_t|n)P(n)} $$
其中后验概率$P(m|\mathbf{x_t})$也称为component occupation probability或者responsibility。
此时，我们可以通过后验概率$P(m|x_t)$计算每一个component对应的&amp;quot;soft counts&amp;quot;: $N_m^*$
$$ N_m^* = \sum_{t=1}^N P(m|x_t) $$</description>
    </item>
    
    <item>
      <title>Kaldi源码之gmm_1</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_1/</link>
      <pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_1/</guid>
      <description>full-gmm和diag-gmm定义了GMM对象，两者不同之处在于协方差矩阵$\Sigma$的表达形式。diag-gmm为了减少计算量，认为输入特征的各个维度间是彼此独立的， 此时协方差矩阵变成对角矩阵，对角元素即为方差。
am-diag-gmm中定义了一个向量densities_，向量中存储的是diag-gmm对象，am-diag-gmm针对的是声学模型中的所有GMM。
full-gmm &amp;amp; diag-gmm 在FullGmm类中，成员变量为
Vector&amp;lt;BaseFloat&amp;gt; gconsts_; bool valid_gconsts_; Vector&amp;lt;BaseFloat&amp;gt; weights_; std::vector&amp;lt;SpMatrix&amp;lt;BaseFloat&amp;gt; &amp;gt; inv_covars_; Matrix&amp;lt;BaseFloat&amp;gt; means_invcovars_; 在DiagGmm类中，成员变量为
Vector&amp;lt;BaseFloat&amp;gt; gconsts_; bool valid_gconsts_; Vector&amp;lt;BaseFloat&amp;gt; weights_; Matrix&amp;lt;SpMatrix&amp;lt;BaseFloat&amp;gt; &amp;gt; inv_covars_; Matrix&amp;lt;BaseFloat&amp;gt; means_invcovars_; full-gmm和diag-gmm中并没有直接定义均值向量$\mu$和协方差矩阵$\Sigma$，而是定义了协方差矩阵的逆矩阵inv_covars——$\Sigma^{-1}$，以及均值向量和协方差逆矩阵的乘积向量means_invcovars——$\mu^T\Sigma^{-1}$。
在full-gmm中inv_covars中存储的是$\Sigma^{-1}$的下三角矩阵，而diag-gmm中存储的则为$\Sigma^{-1}$的对角向量。
注：
PackedMatrix：压缩矩阵，仅存储矩阵的下三角部分，对于$n\times n$的矩阵仅需存储$ \frac{n(n+1)}{2} $个元素
SpMatrix： SymmetricPackedMatrix 对称矩阵，继承自PackedMatrix
TpMatrix： TriangularPackedMatrix 三角矩阵，继承自PackedMatrix
由于对称矩阵和三角矩阵都只需存储矩阵的上三角或者下三角，因此可以使用PackedMatrix节约存储空间。
gconsts_ GMM的概率密度函数$(pdf)$为：
$$ \begin {aligned} f(x;\mu,\Sigma) &amp;amp;= \sum_{m=1}^{M} c_m \mathcal{N} (x;\mu_m,\Sigma_m) \cr &amp;amp;= \sum_{m=1}^{M}\frac{c_m}{(2\pi)^{\frac{D}{2}}{\lvert{\Sigma_m}\rvert}^{\frac{1}{2}}}exp[-\frac{1}{2}(x -\mu_m)^T\Sigma^{-1}_m(\mathbf x-\mu_m)] \end {aligned} $$
其中，$c_m$为GMM第$m$个分量(component)的混合系数，${\Sigma_m}$是协方差矩阵，${\mu_m}$是均值向量，$D$是数据的维度，并且有$\sum_{m=1}^M {c_m}= 1$。
对于离散语音数据，$f(\mathbf x;\mu,\Sigma)即P(\mathbf x|\mu,\Sigma)$，$loglike$为
$$ log P(x|\mu,\Sigma) = \sum_{t=1}^N log \sum_{m=1}^{M} c_m \mathcal{N} (x_t;\mu_m,\Sigma_m) $$</description>
    </item>
    
  </channel>
</rss>
