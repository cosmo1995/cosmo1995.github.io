<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Cosmo</title>
    <link>https://cosmo1995.github.io/post/</link>
    <description>Recent content in Posts on Cosmo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://cosmo1995.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>流式注意力模型</title>
      <link>https://cosmo1995.github.io/p/%E6%B5%81%E5%BC%8F%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/%E6%B5%81%E5%BC%8F%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/</guid>
      <description>端到端模型 E2E模型由于将声学模型（AM），发音词典和语言模型（LM）集成到一个神经网络中而简化了训练过程，因此最近已成为语音识别的流行选择。常用的端到端方法有：CTC(Connectionist Temporal Classification)、AED(Attention Encoder Decoder)、RNN-T(Recurrent Neural Network Transducer)。其中，由于AED的训练效率和良好的识别率，AED成为最受欢迎的方法，但是AED不适用于在线流式语音识别。 CTC可以用于在线识别，但是由于其帧独立性假设，一般需要额外的语言模型（LM）或与AED结合才能获得良好的WER结果。 RNN-T既支持流式ASR，又没有帧独立性假设，但是其训练内存消耗较大，并且训练不稳定，往往需要CTC/CE模型初始化。
Transformer AED模型的典型代表就是Transformer模型，Transformer一经提出就因为其优秀的识别率获得广泛使用，但是Transformer模型并不适用于在线流式模型，因为：
 Encoder中self-attention和Decoder中encoder-decoder attention 需要完整的语音序列 时空复杂度$O(T^2)$  
Encoder self-attention 关于Encoder self-attention，常见的做法包含两种：基于look-ahead的方法和基于chunk的方法如下所示。前者为每个帧设置一个于look-ahead窗口，以获得必要的上下文信息。 但是，延迟时间将随层数线性增加。 基于chunk的方法将语音分割成几个固定长度的chunk，并将它们逐个输入Encoder中，相邻的chunk之间存在重叠以提高性能。
　
　延迟随着层数线性增加 通过chunk控制延迟
　未解决$O(T^2)$复杂度问题　$O(T^2)$&amp;ndash;&amp;gt;$O(TW)$　chunk-based
基于chunk的方法一般通过设置三角mask矩阵来达到分块的目的。
def subsequent_mask(size, device=&amp;#34;cpu&amp;#34;, dtype=datatype): ret = torch.ones(size, size, device=device, dtype=dtype) return torch.tril(ret, out=ret) 
Decode encoder-decoder attention Vanilla attention 输入序列${x_1,x_2&amp;hellip;x_T}$输入到Encoder中，得到隐藏状态序列$h={h_1,h_2&amp;hellip;h_T}$，$h$一般称之为 memory。Decoder根据memory，输出序列${y_1,y_2&amp;hellip;y_U}$，直到输出&amp;lt;eos&amp;gt;。在计算$y_i$时，将前一个时刻的Decoder状态$s_{i-1}$和隐藏状态序列中每一个值$h_j$通过非线性函数$a(·)$计算相似度$e_{i,j}$。再对输出做softmax获得在memory上的概率分布作为权重值，获得权重后对memory序列加权求和即得到attention。由于memory序列和输入序列值一一对应的，因此这些attention分布会在输出和输入之间建立软对齐。最后，Decoder根据$s_{i-1}$和$c_i$输入$s_i$，并输出$y_i$。
 $$ \begin{aligned} e_{i,j} &amp;amp;= a(s_{i-1},j) \hspace{100cm}　\cr \alpha_{i,j} &amp;amp; = \frac{exp(e_{i,j})}{\sum_{k=1}^Texp(e_{i,k})} \cr c_i &amp;amp;= \sum_{j=1}^T\alpha_{i,j}h_j \cr s_i &amp;amp;= f(s_{i-1},y_{i-1},c_i) \cr y_i &amp;amp;= g(s_i,c_i) \end{aligned} $$</description>
    </item>
    
    <item>
      <title>HMM-GMM和HMM-DNN</title>
      <link>https://cosmo1995.github.io/p/hmm-gmm%E5%92%8Chmm-dnn/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/hmm-gmm%E5%92%8Chmm-dnn/</guid>
      <description>HMM-GMM模型 模型分析 原始音频信号经过【预加重-分帧-加窗-fft-mel滤波-DCT】，得到MFCC特征作为输入信号。假设有一段80帧的语音，内容为&amp;quot;six&amp;quot;，对应的音素为“S IH K S”，提取MFCC后的80帧特征序列就是观察序列：
$$ O = [o_1,o_2,&amp;hellip;,o_T],\ T=80 $$ 给定观察序列$O$，估计HMM-GMM模型的参数，这就是HMM中的训练问题。
 输入：$O = [o_1,o_2,&amp;hellip;,o_T]$，即80帧MFCC特征 目标：估计HMM-GMM模型参数$\lambda=(A,B)$，使得$P(O|\lambda)$值最大 输出：通过模型计算每一帧属于S IH K S这四个音素中某一个状态(3状态)的概率  A是状态转移概率，B是观察概率，也就是发射概率。我们使用GMM对发射概率建模，所以HMM-GMM模型的参数：
 HMM参数：状态转移概率 GMM参数：高斯分量系数、均值向量和协方差矩阵  同时，需要额外说明的是：HMM-GMM模型的训练是无监督的，因为我们并不知道哪一帧输入特征对应哪个音素的哪一个状态。训练的目的就是找到帧对应状态的情况，并更新状态的模型参数。把每一帧都归到某个状态上，本质上是进行聚类，是无监督训练。
单音素GMM-HMM模型的训练通过Viterbi训练(嵌入式训练)，把“S IH K S”对应的GMM模型嵌入到整段音频中去训练。
模型训练 对于HMM和GMM这种含有隐变量的模型，我们使用EM算法进行训练。训练的步骤如下图：

初始化GMM 根据topo和sets.int确定GMM的数量，每个GMM初始均为单高斯。
初始化对齐 初始化对齐的目的是为Viterbi对齐提供初始参数A、B。
一开始不知道一段语音的哪些帧对应哪些状态，我们就进行平均分配。前面提到的“six”语音一共80帧，包含四个音素“S IH K S”，每个音素分配到20帧，每个音素又由三个状态组成，每个状态分配6或者7帧。这样就初始化了每个状态对应的输入数据。
如下图，平均对齐后，前1-20帧数据对应“S”音素，21-40帧数据对应“IH”音素，41-60帧对应“K”音素，61-80对应“S”音素。

初始化模型 HMM-GMM模型参数$λ=(A,B,\Pi)$。
 $A$：在初始化对齐后就可以统计出状态1-&amp;gt;状态1的转移次数，状态1-&amp;gt;状态2的转移次数，转移次数/总转移次数就是转移概率。 $B$：更新GMM参数 [见https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_2/] $\Pi$：就是[1,0,0,0&amp;hellip;]，一开始在状态1的概率是100%。在语音识别应用中由于HMM是从左到右的模型，第一个必然是状态1，即$P(q_0=1)=1$。所以没有$\Pi$这个参数了。  重新对齐 需要向真实情况逼近的重新对齐。Viterbi算法根据初始化模型$λ=(A,B,Π)$来计算。它记录每个时刻的每个可能状态的之前最优路径概率，同时记录最优路径的前一个状态，不断向后迭代，找出最后一个时间点的最大概率值对应的状态，再向前回溯到起点，得到最优路径。
得到最优路径就得到最优的状态转移情况，哪些帧对应哪些状态就变了。转移概率$A$就变了。哪些帧对应哪些状态变了导致状态对应的GMM参数自然就变了，即发射概率$B$变了。
迭代 新的$A$和新的$B$又可以进行下一次的Viterbi算法，寻找新的最优路径，得到新的对齐，新的对齐继续改变着参数$A$、$B$。如此循环迭代直到收敛或者达到固定的轮数，则GMM-HMM模型训练完成。
HMM-DNN模型 HMM-GMM建模能力有限，无法准确的表征语音内部复杂的结构，所以识别率低。随着深度学习的崛起，研究人员将其逐步应用于语音识别中。最开始便是DNN代替了GMM来进行观察状态概率的输出，实现HMM-DNN声学模型框架，大大提高了识别率。
GMM与DNN对比 HMM-DNN用DNN替换了GMM来对输入语音信号的观察概率进行建模。
  GMM为了减少参数量，一般使用对角协方差矩阵建模，要求特征维度间的独立性，因此GMM使用的特征是MFCC，这个特征已经做了去相关性处理；DNN使用的特征是FBank，这个特征保持着相关性。
  GMM是生成模型，采用无监督学习；DNN是判别模型，采用有监督学习。</description>
    </item>
    
    <item>
      <title>Chain模型</title>
      <link>https://cosmo1995.github.io/p/chain%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Thu, 11 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/chain%E6%A8%A1%E5%9E%8B/</guid>
      <description>Chain训练流程 基于chain model的Lattice-free MMI训练流程：

 分子和分母状态序列用FST表示（类似HCLG） 分母的前向后向计算在GPU中进行 从训练的alignment中，通过4-gram的phone构建LM，而不是word级别的LM 帧率降到原来的1/3，训练和解码速度更快（每3帧输出1次），也就是说帧移变成了30ms，而不是10ms 由于帧率的降低，HMM的结构上做了一些调整，引入了类似CTC里面blank的状态，同时转移概率设为0.5 无需交叉熵系统初始化，可以从头开始训练  HMM topology chain model由于降低了帧率（每帧30ms），因此需要修改HMM拓扑结构。 修改后HMM可以通过一次转移进行遍历（相对于传统模型的3次转移）。 HMM拓扑包含一个仅出现一次的状态Sp，以及一个可以出现零次或多次的状态Sb。
这种做法实际上是借鉴了CTC的思想，引入了blank用来吸收不确定的边界。但CTC只有一个blank，而chain model中每一个建模单元都有自己的blank。如下图所示：

对应kaldi中的结构定义为：
&amp;lt;Topology&amp;gt;&amp;lt;TopologyEntry&amp;gt;&amp;lt;ForPhones&amp;gt;1 2 3 4 5 6 7 8 …&amp;lt;/ForPhones&amp;gt;&amp;lt;State&amp;gt; 0 &amp;lt;ForwardPdfClass&amp;gt; 0 &amp;lt;SelfLoopPdfClass&amp;gt; 1 &amp;lt;Transition&amp;gt; 0 0.5 &amp;lt;Transition&amp;gt; 1 0.5 &amp;lt;/State&amp;gt;&amp;lt;State&amp;gt; 1 &amp;lt;/State&amp;gt;&amp;lt;/TopologyEntry&amp;gt;&amp;lt;/Topology&amp;gt;在kaldi中，把Sp和Sb看做同一个状态(都对应state 0)，只是pdfclass不同。ForwardPdfClass表示Sp，SelfLoopPdfClass表示Sb。
Chain model训练 chain model实际上也是一种序列鉴别性训练的方法，所以它也要构造分母fst和分子fst。
ps：这里不用分母词图(lattice)和分子词图(lattice)的表述，一、因为chain model(lattice free)不需要构建分母词图，而是用类似于HCLG这样的fst结构代替分母词图。二、同时chain model为了将每个句子分成一小块chunk，也会把分子lattice转换成分子fst(因为fst可以保留时间对齐信息，方便根据时间切分成块)
一、为每个句子构建训练图，然后解码得到每个句子的可能对齐结果(lattice) 这一步类似于构建解码用的HCLG,但解码用的HCLG是所有句子通用的。而我们这里构建的HCLG是根据每个句子的标签构建的，所以图会比较小，每个句子有自己的一个HCLG图。具体构建训练图的方法可参考：http://kaldi-asr.org/doc/graph_recipe_train.html。
有了训练图之后，我们就可以在上面解码，得到每个句子所有可能的对齐方式。kaldi中将所有可能的对齐用lattice这样的数据结构来保存，这样可以节省存储空间。得到的lattice(CompactLattice)的格式如下：

第一列和第二列是lattice结点编号，第三列是word，接下来的两个数字分别是语言模型概率和声学模型概率，结下来以”_“分隔的每个数字都是每一帧对应的transition-id。
为什么不是使用Viterbi对齐呢？</description>
    </item>
    
    <item>
      <title>区分性训练</title>
      <link>https://cosmo1995.github.io/p/%E5%8C%BA%E5%88%86%E6%80%A7%E8%AE%AD%E7%BB%83/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/%E5%8C%BA%E5%88%86%E6%80%A7%E8%AE%AD%E7%BB%83/</guid>
      <description>前言 先回顾下语音识别的基本公式，我们要在给定语音观测序列$O$的情况下，求词序列$W$的概率$P(W|O)$，概率最大的词序列$W^{*}$即为最可能的识别结果。 $$ W^{*} = \arg\max_{W} P(W|O) $$ 根据贝叶斯公式，有： $$ P(W|O)=\frac{P(O|W)P(W)}{P(O)} $$
其中$P(O|W)$为声学模型，$P(W)$为语言模型，$P(O)$与识别结果无关可以忽略。故： $$ W^{*} = \arg\max_{W} P(O|W)P(W) $$
最大似然估计 基于最大似然的声学模型训练：
$$ \begin {aligned} \theta_{MLE} &amp;amp;= \arg\max_{\theta} P_\theta(O|W) \cr &amp;amp;= \arg\max_{\theta} \prod_u P_\theta(O_u|W_u) \cr &amp;amp;= \arg\max_{\theta} \sum_u \log P_\theta(O_u|W_u) \end {aligned} $$ MLE的优点：
 MLE训练方法简单，一个较高精度的语音识别声学模型能迅速地训练得到 MLE采用的EM等算法使得其不需要精细的参考文本音段时间标注，并在每一步迭代中确保对目标函数的优化 MLE对训练资源的消耗较小  MLE做出的假设：
 要求所学习模拟的分布是已知的，模型假设必须是正确的。就是说建模时指定的概率密度函数要能够代表实际语音的“真实”分布 训练时数据应趋向于无穷多，可以经由无穷多的数据估计出模型的“真实”参数 解码时需要的语言模型要事先已知，且参数要完全“真实”  这三点实际上是不可能达到的，
 语音参数的“真实”分布是不可测的，更谈不上通常意义上的指数族函数(GMM)来充分模拟 训练数据无穷多本就是天方夜谭 解码中语言模型存在的问题与声学模型几乎完全一样，也达不到“真实”参数的要求  结论：
在现实条件下通过MLE训练得到最优分类器是不可能的，因此训练得到的ASR模型的识别率一般。
区分性训练 MLE由于上述问题的存在，识别效果不是很理想。为了在现实条件下能得到较优的分类器，区分性训练DT被提出。区分性训练的目标就是提高正确路径得分的同时降低其他路径得分，从而加大打分差异，就显得“区分开了”。MLE训练更重视调整模型参数以反映训练数据的概率分布，而区分性训练则更重视调整模型之间的分类面，以更好的根据设定的准则对训练数据进行分类。
区分性训练的常用的准则为MMI最大互信息，并且和MLE不同的是，MMI直接最大化后验概率$P(W|O)$：
$$ \begin{aligned} \theta_{MMI} &amp;amp;= \arg\max_{\theta} P_\theta(W|O) \cr &amp;amp;= \arg\max_{\theta} \prod_u P_\theta(W_u|O_u) \cr &amp;amp;= \arg\max_{\theta} \sum_u \log P_\theta(W_u|O_u) \cr &amp;amp;= \arg\max_{\theta} \sum_u \log \frac {P_\theta(O_u|W_u)P(W_u)}{P(O_u)} \cr &amp;amp;= \arg\max_{\theta} \sum_u \log \frac {P_\theta(O_u|W_u)P(W_u)}{\sum_W P_\theta(O_u|W)P(W)} \end{aligned} $$</description>
    </item>
    
    <item>
      <title>Kaldi中的特征变换</title>
      <link>https://cosmo1995.github.io/p/kaldi%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2/</guid>
      <description>Kaldi中的特征空间变换 Kaldi中的特征空间变化的方法包含两类：
Speaker Independent：
 线性判别分析LDA(Linear Discriminant Analysis) 拼帧Frame Splicing &amp;amp; 差分Delta 最大似然线性变换MLLT(Maximum Likelihood Linear Transform)/半绑定协方差STC(Semi-tied Covariance)  Speaker Adaptive：
 特征空间最大似然线性回归fMLLR(feature-space Maximum Likelihood Linear Regression) 线性声道长度归一化LVTLN(Linear Vocal Tract Length Normalization) 倒谱均值方差归一化CMVN(Cepstral Mean and Variance Normalization)  Speaker Independent： Delta 为了更好的识别语音，我们在静态MFCC特征的基础上附加一阶（delta）和二阶（delta-delta）差分，以补偿HMM的声学模型做出的条件独立性假设。
LDA+MLLT 首先对原始的MFCC特征进行拼帧(Frame Splicing)，将相邻的N帧拼接起来，如下面的例子将7帧拼接到一起。
steps/train_lda_mllt.sh --cmd &amp;#34;train.cmd&amp;#34; \  --splice-opts &amp;#34;--left-context=3 --right-context=3&amp;#34; \  2500 15000 $data $lang $tri1_ali $tri_2b 在HMM-GMM模型中，通过GMM对HMM的发射概率进行建模。为了减少GMM模型的参数量，我们通常使用对角协方差(diagonal covariance)矩阵，这就要特征的各个维度之间是独立的(independent)。MFCC特征即使通过DCT去相关后仍存在一定的相关性，而拼帧后的特征间的相关性更强了。
因此将拼接后的帧通过LDA进行降维和去相关。LDA的核心思想为投影后类内方差最小，类间的方差最大，这里的类指的是声学模型的状态(如pdf-id)。
LDA降维后的特征通过MLLT再次进行去相关，MLLT引入了一种新形式的协方差矩阵，这种方法的每个高斯分量有两个方差矩阵:
 对角协方差矩阵$\Sigma_{diag}^{(m)}$ 半绑定非对角矩阵$H$，可以在多个高斯分量之间共享  第m个高斯分量的协方差矩阵可以描述为： $$ \Sigma^{(m)}=H\Sigma^{(m)}_{diag}H^{T} $$</description>
    </item>
    
    <item>
      <title>MFCC和FBANK特征</title>
      <link>https://cosmo1995.github.io/p/mfcc%E5%92%8Cfbank%E7%89%B9%E5%BE%81/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/mfcc%E5%92%8Cfbank%E7%89%B9%E5%BE%81/</guid>
      <description>MFCC（Mel Frequency Cepstral Coefficent）和FBANK特征在语音和说话人识别中被广泛使用。FBANK和MFCC计算的主要过程一致，MFCC是在FBANK的基础上做DCT变换。MFCC特征提取的过程可以分为预处理和倒谱两部分。
预处理 预加重 第一步是对语音信号应用预加重，以放大高频部分。将语音信号通过一个高通滤波器：
$$ y(t) = x(t) - \alpha x(t-1) $$ 其中滤波系数$\alpha$一般取0.95或0.97。
预加重滤波器在几种方面有用：（1）平衡频谱，因为高频通常比低频具有较小的能量；（2）避免在傅立叶变换操作期间出现数值问题；（3）还可改善信噪比（SNR）。
分帧 因为语音信号是快速变化的，而FFT适用于分析平稳的信号。为了简化起见，我们假设音频信号在短时间范围内变化不大（当我们说它不变时，我们指的是统计上的，即统计上是平稳的，显然样本在不断变化。即使是短时间尺度）。我们将语音分成20-40ms帧（一般取帧长为25ms），如果帧过短，将没有足够的样本来获得可靠的频谱估计；如果帧过长，则信号在整个帧中变化太大。 为确保声学特征参数的平滑性，帧移一般为10ms，即相邻两帧之间有15ms的重叠。
加窗 特征提取时，每次取长为25ms的语音，进行离散傅立叶变换计算出一帧，接着步移10ms继续计算下一帧，相当于加了矩形窗。而棱角分明的矩形窗容易造成频谱泄露，可以选择使用汉明窗（Hamming Window）、汉宁窗(Hanning Window)等。
汉明窗： $$ w[n]=(1-\alpha)-\alpha cos(\frac{2\pi n}{N-1}) $$ 实践中$\alpha$一般取0.46，其中$0\leq n \leq N-1$，$N$为窗的长度。
通过应用汉明窗，可以降低傅立叶变换后旁瓣的强度(主瓣是变换为频谱之后振幅最大的那个波峰部分，而周围的小的波峰部分叫旁瓣)，取得更高质量的频谱。
这里也解释了为什么要帧移是10ms, 相邻帧之间有15ms的重叠, 由于帧与帧连接处的信号因为加窗而弱化。
倒谱 在对原始语音进行预加重、分帧、加窗等预处理后，就可以进行倒谱分析了。我们对每一帧语音进行FFT，得到每一帧语音的频谱图(spectrum)。

频谱图中，峰值表示语音的主要频率成分，我们把这些峰值称为共振峰（formants），而共振峰包含了声音的辨识属性。共振峰点的通过一条平滑曲线连接起来，这条曲线叫做频谱的包络（Spectral Envelope）。

我们可以这么理解，将原始的频谱由两部分组成：包络和细节。包络包含了语音的主要信息，因此我们的目的就是把包络部分提取出来，即分离频谱的包络和细节。
频谱用$X[k]$表示，包络为$H[k]$，细节为$E[k]$，则有 $$ X[k]=H[k]*E[k] $$ 在两边取对数，将乘法转换为加法，便于分离，有： $$ logX[k]=logH[k] + logE[k] $$ 因此我们的目的是在给定$logX[k]$的基础上，求$logH[k]$和$logE[k]$，使得$logX[k]=logH[k] + logE[k]$。
为了达到这一目的，我们在对数频谱上做FFT，频谱上做FFT相当于IFFT。在对数频谱上做IFFT就相当于在一个伪频率（pseudo-frequency）坐标轴上面描述信号。

由上面这个图我们可以看到，包络是主要是低频成分，我们把它看成是一个每秒4个周期的正弦信号。这样我们在伪坐标轴上面的4Hz的地方给它一个峰值。而频谱的细节部分主要是高频。我们把它看成是一个每秒100个周期的正弦信号。这样我们在伪坐标轴上面的100Hz的地方给它一个峰值。把它俩叠加起来就是原来的频谱信号了。
在实际中我们已经知道$logX[k]$，所以可以得到$x[k]$。而$h[k]$是$x[k]$的低频部分，那么我们将$x[k]$通过一个低通滤波器就可以得到$h[k]$了。
$x[k]$实际上就是倒谱Cepstrum（这个是一个新造出来的词，把频谱的单词spectrum的前面四个字母顺序倒过来就是倒谱的单词了）。而我们所关心的$h[k]$就是倒谱的低频部分，它在语音识别中被广泛用于描述特征。
那现在总结下倒谱分析，它实际上是这样一个过程：
1）将原语音信号经过傅里叶变换得到频谱：$X[k]=H[k]E[k]$
只考虑幅度就是：$||X[k] ||=||H[k]||\ ||E[k]||$
2）我们在两边取对数：$log||X[k] ||= log ||H[k] ||+ log ||E[k] ||$</description>
    </item>
    
    <item>
      <title>Kaldi源码之决策树4_构建决策树</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%914_%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91/</link>
      <pubDate>Sun, 08 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%914_%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91/</guid>
      <description>到现在为止，程序acc-tree-stats累积好了构建决策树所需的统计量，程序cluster-phones和compile-questions自动生成好了构建决策树所需的问题集。我们根据sets.int生成好了roots.int文件，那么我们就可以开始构建决策树，对三音素GMM的状态进行绑定。这次笔记的主要内容是讲解Kaldi如何构建决策树，实现对三音素GMM状态的绑定。 在这个笔记中，首先我会介绍构建决策树的主程序build-tree和主函数BuildTree，然后介绍主函数中用到的核心函数GetStubMap和SplitDecisionTree。
build-tree  作用：构建决策树 输入：累积的统计量treeacc、问题集questions.qst、roots.int、topo 输出：决策树tree  build-tree $context_opts --verbose=1 --max-leaves=$numleaves \  --cluster-thresh=$cluster_thresh $dir/treeacc $lang/phones/roots.int \  $dir/questions.qst $lang/topo $dir/tree   读取roots.int，得到
1)vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt; phone_sets，其一个元素包含roots.int的一行上的所有音素
2)vector&amp;lt;bool&amp;gt; is_shared_root，其一个元素指明该行音素的HMM状态是否共享决策树树根
3)vector&amp;lt;bool&amp;gt; is_split_root，其一个元素指明是否对该行音素对应的决策树树根进行划分
  读取topo文件，得到保存HMM拓扑结构的对象HmmTopology topo
  读取treeacc，得到累积的统计量BuildTreeStatsType stats
  读取questions.qst，得到Questions qo
  std::vector&amp;lt;int32&amp;gt; phone2num_pdf_classes; topo.GetPhoneToNumPdfClasses(&amp;amp;phone2num_pdf_classes); 调用topo.GetPhoneToNumPdfClasses得到phone2num_pdf_classes，其元素保存每个音素对应的HMM状态数。
to_pdf = BuildTree(qo, phone_sets, phone2num_pdf_classes, is_shared_root, is_split_root, stats, thresh, max_leaves, cluster_thresh, P) 调用BuildTree，返回保存整个大决策树的 to_pdf
BuildTree EventMap *tree_stub = GetStubMap(P phone_sets phone2num_pdf_classes share_roots &amp;amp;num_leaves); .</description>
    </item>
    
    <item>
      <title>Kaldi源码之决策树3_构建问题集</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%913_%E6%9E%84%E5%BB%BA%E9%97%AE%E9%A2%98%E9%9B%86/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%913_%E6%9E%84%E5%BB%BA%E9%97%AE%E9%A2%98%E9%9B%86/</guid>
      <description>前面我们已经通过acc-tree-stats累积好了构建决策树所需的统计量，要建立一颗决策树，我们还需要构建问题集。在HTK中，问题集是人工定义的；而在kaldi中，问题集是通过训练数据自动生成的。kaldi中，通过cluster-phones生成问题集。
cluster-phones  作用：多个音素或多个音素集进行聚类。 输入：决策树相关统计量treeacc，多个音素集sets.int 输出：自动生成的问题集（每个问题由多个音素组成） 示例：  cluster-phones $context_opts $dir/treeacc $lang/phones/sets.int \  $dir/questions.int  过程：   context_opts指定context-width和central-position，默认的三音素参数N=3，P=1；从treeacc中读取统计量到BuildTreeStatsType stats；从sets.int读取phone set 保存到phone_sets；读取pdf_class_list，该变量指定构建问题集所考虑的HMM状态，默认为1，也就是只考虑三状态HMM的中间状态。 若指定的mode为questions，调用AutomaticallyObtainQuestions()自动生成问题集保存到phone_sets_out；若指定的model为k-means，调用KMeansClusterPhones()。 将上述函数自动生成的phone_sets_out写到questions.int。  sets.txt通过utils/prepare_lang.sh生成，其中每一行为相同base的音素，sets.int 和sets.txt实例：
sil	1spn	2AA0 AA1 AA2	3 4 5AE0 AE1 AE2	6 7 8...	...AutomaticallyObtainQuestions void AutomaticallyObtainQuestions(BuildTreeStatsType &amp;amp;stats, const std::vector&amp;lt;std::vector&amp;lt;int32&amp;gt; &amp;gt; &amp;amp;phone_sets_in, const std::vector&amp;lt;int32&amp;gt; &amp;amp;all_hmm_positions_in, int32 P, std::vector&amp;lt;std::vector&amp;lt;int32&amp;gt; &amp;gt; *questions_out ) 通过对音素自动进行聚类，从而获取问题集；它把音素聚类成一棵树，并且对树中的每一个结点，把从该结点可以到达的所有叶子结点合在一起构成一个问题（该树的一个叶子结点保存着一些音素，一个问题就是一个音素的集合）。
std::vector&amp;lt;std::vector&amp;lt;int32&amp;gt; &amp;gt; phone_sets(phone_sets_in); std::vector&amp;lt;int32&amp;gt; phones; for (size_t i = 0; i &amp;lt; phone_sets.</description>
    </item>
    
    <item>
      <title>Kaldi源码之决策树2_累积统计量</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%912_%E7%B4%AF%E7%A7%AF%E7%BB%9F%E8%AE%A1%E9%87%8F/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%912_%E7%B4%AF%E7%A7%AF%E7%BB%9F%E8%AE%A1%E9%87%8F/</guid>
      <description>Build-Tree-Questions 在构建决策树时，我们需要知道的所有信息就是从训练数据的对齐中得到的所有EventType（三音素+HMM状态id），和每个EventType对应的统计量即Clusterable对象。很自然的，我们可以把这两者的对应关系保存成一个对pair&amp;lt;EventType, Clusterable*&amp;gt;，然后把所有的这些对保存成一个向量BuildTreeStatsType。
typedef std::vector&amp;lt;std::pair&amp;lt;EventType, Clusterable*&amp;gt; &amp;gt; BuildTreeStatsType; acc-tree-stats   作用：为决策树的构建累积相关的统计量
  输入：声学模型、特征、对齐
  输出：统计量tree.acc文件
  acc-tree-stats $context_opts \  --ci-phones=$ciphonelist $alidir/final.mdl &amp;#34;$feats&amp;#34; \  &amp;#34;ark:gunzip -c $alidir/ali.JOB.gz|&amp;#34; $dir/JOB.treeacc 输入的声学模型一般为单音素训练得到的GMM模型。acc-tree-stats的核心就是AccumulateTreeStats函数。
AccumulateTreeStats void AccumulateTreeStats(const TransitionModel &amp;amp;trans_model, BaseFloat var_floor, int N, // context window size.  int P, // central position.  const std::vector&amp;lt;int32&amp;gt; &amp;amp;ci_phones, //const AccumulateTreeStatsInfo &amp;amp;info,  const std::vector&amp;lt;int32&amp;gt; &amp;amp;alignment, const Matrix&amp;lt;BaseFloat&amp;gt; &amp;amp;features, std::map&amp;lt;EventType, GaussClusterable*&amp;gt; *stats){ .</description>
    </item>
    
    <item>
      <title>Kaldi源码之决策树1_理论和描述</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%911_%E7%90%86%E8%AE%BA%E5%92%8C%E6%8F%8F%E8%BF%B0/</link>
      <pubDate>Sat, 08 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%911_%E7%90%86%E8%AE%BA%E5%92%8C%E6%8F%8F%E8%BF%B0/</guid>
      <description>triphone模型中，为了解决数据稀疏和参数量太大的问题，需要进行聚类和状态绑定。kaldi通过决策树进行状态绑定，即将相似的HMM状态聚类到同一个pdf，以减少总的pdf数。
决策树理论 决策树是一种自上而下的聚类方法。决策树包含叶子结点和非叶子结点，每一个叶子结点代表一类。
决策树的每一个结点包含一些状态的集合，我们可以计算该状态集生成对应观测帧的似然。在triphone模型中，将一系列HMM states输入到决策树的根节点中，对这些状态进行提问，根据问题的答案分为左子结点或者右子结点。对于每一个子结点，可以计算该结点的新似然。决策树分类的根据就是使得分裂后的子节点似然之和相比分裂之前增量最大。同时我们通过设置某些阈值来控制决策树何时停止分裂，例如分裂前后似然的增量阈值，结点对应的state occupancy等。
一般我们会将具有相同中间音素的triphone的同一个位置的所有状态构建一个决策树，如果有63个monophone，那么我们会构建189个决策树(kaldi中这189个决策树会放在一个大的决策树下)。下图为所有中间音素为&amp;quot;zh&amp;quot;的triphone第3个HMM state进行决策树聚类的例子。
决策树的似然等于每一个叶子结点的似然之和。若$L$为决策树某一个结点的似然，则有 $$ L=\sum_{t \in T}{\sum_{s \in S}\gamma_s(t) logP(x_t;\mu_S,\Sigma_S)} $$ 其中$S$为该结点对应的状态集，$T$为状态集$S$对应的所有帧，$\gamma_s(t)$为帧$x_t$由状态$s$生成的后验概率，$P$为概率分布。
若$P$为高斯分布，则有： $$ \begin {aligned} L &amp;amp;= -\frac{1}{2} (log[(2\pi)^D|\Sigma_S|] + D) \cdot \sum_{s \in S}\sum_{t \in T} \gamma_s(t) \cr &amp;amp;= -\frac{1}{2}(Dlog(2\pi) + \sum_{i=1}^D{log\Sigma_{ii}} + D) \cdot \sum_{s \in S}\sum_{t \in T} \gamma_s(t) \end {aligned} $$
triphone模型通过决策树分裂时，首先将具有相同中间音素的triphone的同一个位置的所有状态作为根节点，然后生成一系列可选问题集。对每一个叶子结点，选择使得分裂前后的似然增量最大的问题，即选择$\Delta L$最大的问题进行分裂。重复分裂步骤直到不满足设定的阈值。 $$ \Delta L = L(parent) - L(leftchild) - L(rightchild) $$
根据$L$的计算公式，要想计算决策树某个结点的似然，我们只需要知道该结点对应状态集$S$的协方差$\Sigma_S$，以及状态集中所有状态的state occupancy之和$\sum_{s \in S}\sum_{t \in T} \gamma_s(t)$。</description>
    </item>
    
    <item>
      <title>Kaldi之单音素GMM训练</title>
      <link>https://cosmo1995.github.io/p/kaldi%E4%B9%8B%E5%8D%95%E9%9F%B3%E7%B4%A0gmm%E8%AE%AD%E7%BB%83/</link>
      <pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E4%B9%8B%E5%8D%95%E9%9F%B3%E7%B4%A0gmm%E8%AE%AD%E7%BB%83/</guid>
      <description>前情回顾 GMM1和GMM2可知，GMM模型的更新公式如下：
$$ \begin {aligned} &amp;amp;c_m=\frac{occupancy_m}{N} \cr &amp;amp;\mu_m=\frac{mean\_accumulator_m} {occupancy_m} \cr &amp;amp;\Sigma_m=\frac{covariance\_accumulator_m} {occupancy_m} - \mu_m \mu_m^T &amp;amp;\end {aligned} $$
其中，
$$ \begin {aligned} occupancy_m &amp;amp;= \sum_{t=1}^N P(m|x_t) \cr mean\_accumulator_m &amp;amp;= \sum_{t=1}^N P(m|x_t) x_t \cr covariance\_ accumulator_m &amp;amp;= \sum_{t=1}^N P(m|x_t) x_t x_t^T \end {aligned} $$
后验概率$P(m|x_t)$又可以表示为： $$ \begin{aligned} P(m|x_t) &amp;amp;= \frac{exp(loglike(\mu_m,\Sigma_m|x_t))} {\sum_{n=1}^M exp(loglike(\mu_n,\Sigma_n|x_t)} \cr &amp;amp;= softmax(loglike(\mu_m,\Sigma_m|x_t)) \end{aligned} $$ 因此，可以通过计算GMM每一个分量component的对数似然LogLikelihood，再应用softmax来获得对应的后验概率，在此基础上更新GMM参数。
而第m个分量component的对数似然LogLikelihood可以表示为： $$ loglike_m = gconsts_m +\mu_m^T\Sigma^{-1}_mx - \frac{1}{2}x^T\Sigma_m^{-1}x $$ 其中： $$ gconsts_m = logc_m - \frac{D}{2}log2\pi - \frac{1}{2}(log|\Sigma_m| + \mu_m^T\Sigma_m^{-1}\mu_m) $$ 故和GMM模型相关的统计量包括：</description>
    </item>
    
    <item>
      <title>Kaldi源码之hmm</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bhmm/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bhmm/</guid>
      <description>kaldi中TransitionModel用来更新HMM的转移概率，通过hmm-topology描述每个phone的拓扑结构，包括phone对应的state和以及state之间的初始转移概率。通过transition-model定义了transition的基本单元transition-state和转移路径transition-id。
hmm-topology 以yesno的topo为例，yesno中有三个音素：SIL、Y、N ，phone id 分别为1、2、3。其中SIL有0~4共5个状态，5为non-emitting。yes/no均含有0~2共3个状态，3为non-emitting。topo中还定义了每个phone对应的状态之间的初始转移概率。
Kaldi通过HmmTopology类表示topo结构，在HmmTopology中定义了HmmState结构体来描述HMM-state，包含HMM-state对应的pdf-class和转移的目标状态、初始转移概率。
struct HmmState { int32 pdf_class; std::vector&amp;lt;std::pair&amp;lt;int32, BaseFloat&amp;gt; &amp;gt; transitions; ... } typedef std::vector&amp;lt;HmmState&amp;gt; TopologyEntry; TopologyEntry中包含一个phone对应的所有HMM-state以及这些HMM-state之间的转移路径和转移概率，即一个phone的完整topo结构。
hmm-topology的成员变量为：
std::vector&amp;lt;int32&amp;gt; phones_; std::vector&amp;lt;int32&amp;gt; phone2idx_; std::vector&amp;lt;TopologyEntry&amp;gt; entries_; hmm-topology类包含了上图中完整的topo信息，包括所有音素、每个音素对应的HMM-state和HMM-state间的转移路径和转移概率。
transition-model transition中相关的概念：
 phone：音素，id是从1开始的整数。 HMM-state：每个音素的state的id，从0开始的整数。 pdf-id：每个state相对应的GMM概率密度函数id，这个值是全局唯一从0开始的整数。在triphone中，pdf-id会替换为决策树聚类后的实际pdf-id。pdf-id又可以分为forward-pdf-id和self-loop-pdf-id，默认pdf-id=forward-pdf-id=self-loop-pdf-id。 transition-state：抽象出来的转移状态，对于monophone，和HMM-state一一对应；对于triphone，对应于上下文音素绑定的状态。用(phone,HMM-state,pdf-id)表示，从1开始。 transition-index：表示一个transition-state的转移路径的index，在每个状态内从0开始的整数。 transition-id：所有transition-state的转移路径的id，全局唯一从1开始的整数，跟(transition-state,transition-index)一一对应。  通过show-transitions可以查看这些变量的具体值，如下表。
transition-model中定义了triples,state2id_和id2state，分别表示transition-state对应的triples，每个transition-state对应的的第一个transition-id，每个transition-id对应的transition-state。
std::vector&amp;lt;Triple&amp;gt; triples_; // indexed by transition-state - 1 std::vector&amp;lt;int32&amp;gt; state2id_; // indexed by transition-state std::vector&amp;lt;int32&amp;gt; id2state_; // indexed by transition-id ... 通过这三个变量，transition-model中实现了一系列transition概念的转换，包括：
(phone, HMM-state, pdf-id) &amp;lt;&amp;mdash;&amp;gt; transition-state
(transition-state, transition-index) &amp;lt;&amp;mdash;&amp;gt; transition-id</description>
    </item>
    
    <item>
      <title>Kaldi源码之gmm_2</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_2/</link>
      <pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_2/</guid>
      <description>estimate-full-gmm和estimate-diag-gmm用来对GMM进行极大似然估计(MLE)，二者仅在协方差矩阵数据结构不同，下面均以estimate-full-gmm为例。
GMM GMM的分布函数为
$$ p(x|\mu,\Sigma) = \sum_{m=1}^{M} c_m \mathcal{N} (x;\mu_m,\Sigma_m) $$
其中$c_m$为第$m$个component的混合系数,$\sum_{m=1}^M c_m =1 $。
定义指示函数$1_{[z_t=m]}$，如果$x_t$来自第m个component，则$1_{[z_t=m]} = 1$，否则$1_{[z_t=m]} = 0$。
现在假设$1_{[z_t=m]}$已知，即$x_t$属于哪一个component是已知的话，我们就可以统计每一个component对应的帧数：
$$ N_m=\sum^N_{t=1} 1_{[z_t=m]} $$
那么GMM参数可以估计为：
$$ \begin {aligned} &amp;amp;c_m = \frac{\sum_{t=1}^N 1_{[z_t=m]}}{N} = \frac{N_m}{N} \cr &amp;amp;\mu_m= \frac{\sum_{t=1}^N 1_{[z_t=m]} \mathbf{x_t}}{N_m} \cr &amp;amp;\Sigma_m= \frac{\sum_{t=1}^N 1_{[z_t=m]}(\mathbf{x_t}-\mu_m)(\mathbf{x_t}-\mu_m)^T}{N_m} &amp;amp;\end {aligned} $$
然而$1_{[z_t=m]}$是未知的，我们并不知道每一帧$x_t$属于哪一个component。但是，我们可以计算后验概率$P(m|x_t)$，即第m个 component生成$x_t$的概率。 $$ P(m|x_t) = \frac{P(x_t|m)P(m)} {P(x_t)} = \frac{P(x_t|m)P(m)} {\sum^M_{n=1}P(x_t|n)P(n)} $$
其中后验概率$P(m|\mathbf{x_t})$也称为component occupation probability或者responsibility。
此时，我们可以通过后验概率$P(m|x_t)$计算每一个component对应的&amp;quot;soft counts&amp;quot;: $N_m^*$
$$ N_m^* = \sum_{t=1}^N P(m|x_t) $$</description>
    </item>
    
    <item>
      <title>Kaldi源码之gmm_1</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_1/</link>
      <pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_1/</guid>
      <description>full-gmm和diag-gmm定义了GMM对象，两者不同之处在于协方差矩阵$\Sigma$的表达形式。diag-gmm为了减少计算量，认为输入特征的各个维度间是彼此独立的， 此时协方差矩阵变成对角矩阵，对角元素即为方差。
am-diag-gmm中定义了一个向量densities_，向量中存储的是diag-gmm对象，am-diag-gmm针对的是声学模型中的所有GMM。
full-gmm &amp;amp; diag-gmm 在FullGmm类中，成员变量为
Vector&amp;lt;BaseFloat&amp;gt; gconsts_; bool valid_gconsts_; Vector&amp;lt;BaseFloat&amp;gt; weights_; std::vector&amp;lt;SpMatrix&amp;lt;BaseFloat&amp;gt; &amp;gt; inv_covars_; Matrix&amp;lt;BaseFloat&amp;gt; means_invcovars_; 在DiagGmm类中，成员变量为
Vector&amp;lt;BaseFloat&amp;gt; gconsts_; bool valid_gconsts_; Vector&amp;lt;BaseFloat&amp;gt; weights_; Matrix&amp;lt;SpMatrix&amp;lt;BaseFloat&amp;gt; &amp;gt; inv_covars_; Matrix&amp;lt;BaseFloat&amp;gt; means_invcovars_; full-gmm和diag-gmm中并没有直接定义均值向量$\mu$和协方差矩阵$\Sigma$，而是定义了协方差矩阵的逆矩阵inv_covars——$\Sigma^{-1}$，以及均值向量和协方差逆矩阵的乘积向量means_invcovars——$\mu^T\Sigma^{-1}$。
在full-gmm中inv_covars中存储的是$\Sigma^{-1}$的下三角矩阵，而diag-gmm中存储的则为$\Sigma^{-1}$的对角向量。
注：
PackedMatrix：压缩矩阵，仅存储矩阵的下三角部分，对于$n\times n$的矩阵仅需存储$ \frac{n(n+1)}{2} $个元素
SpMatrix： SymmetricPackedMatrix 对称矩阵，继承自PackedMatrix
TpMatrix： TriangularPackedMatrix 三角矩阵，继承自PackedMatrix
由于对称矩阵和三角矩阵都只需存储矩阵的上三角或者下三角，因此可以使用PackedMatrix节约存储空间。
gconsts_ GMM的概率密度函数$(pdf)$为：
$$ \begin {aligned} f(x;\mu,\Sigma) &amp;amp;= \sum_{m=1}^{M} c_m \mathcal{N} (x;\mu_m,\Sigma_m) \cr &amp;amp;= \sum_{m=1}^{M}\frac{c_m}{(2\pi)^{\frac{D}{2}}{\lvert{\Sigma_m}\rvert}^{\frac{1}{2}}}exp[-\frac{1}{2}(x -\mu_m)^T\Sigma^{-1}_m(\mathbf x-\mu_m)] \end {aligned} $$
其中，$c_m$为GMM第$m$个分量(component)的混合系数，${\Sigma_m}$是协方差矩阵，${\mu_m}$是均值向量，$D$是数据的维度，并且有$\sum_{m=1}^M {c_m}= 1$。
对于离散语音数据，$f(\mathbf x;\mu,\Sigma)即P(\mathbf x|\mu,\Sigma)$，$loglike$为
$$ log P(x|\mu,\Sigma) = \sum_{t=1}^N log \sum_{m=1}^{M} c_m \mathcal{N} (x_t;\mu_m,\Sigma_m) $$</description>
    </item>
    
  </channel>
</rss>
