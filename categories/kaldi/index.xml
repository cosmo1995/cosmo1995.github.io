<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kaldi on Cosmo</title>
    <link>https://cosmo1995.github.io/categories/kaldi/</link>
    <description>Recent content in kaldi on Cosmo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://cosmo1995.github.io/categories/kaldi/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kaldi中的特征变换</title>
      <link>https://cosmo1995.github.io/p/kaldi%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2/</guid>
      <description>Kaldi中的特征空间变换 Kaldi中的特征空间变化的方法包含两类：
Speaker Independent：
 线性判别分析LDA(Linear Discriminant Analysis) 拼帧Frame Splicing &amp;amp; 差分Delta 最大似然线性变换MLLT(Maximum Likelihood Linear Transform)/半绑定协方差STC(Semi-tied Covariance)  Speaker Adaptive：
 特征空间最大似然线性回归fMLLR(feature-space Maximum Likelihood Linear Regression) 线性声道长度归一化LVTLN(Linear Vocal Tract Length Normalization) 倒谱均值方差归一化CMVN(Cepstral Mean and Variance Normalization)  Speaker Independent： Delta 为了更好的识别语音，我们在静态MFCC特征的基础上附加一阶（delta）和二阶（delta-delta）差分，以补偿HMM的声学模型做出的条件独立性假设。
LDA+MLLT 首先对原始的MFCC特征进行拼帧(Frame Splicing)，将相邻的N帧拼接起来，如下面的例子将7帧拼接到一起。
steps/train_lda_mllt.sh --cmd &amp;#34;train.cmd&amp;#34; \  --splice-opts &amp;#34;--left-context=3 --right-context=3&amp;#34; \  2500 15000 $data $lang $tri1_ali $tri_2b 在HMM-GMM模型中，通过GMM对HMM的发射概率进行建模。为了减少GMM模型的参数量，我们通常使用对角协方差(diagonal covariance)矩阵，这就要特征的各个维度之间是独立的(independent)。MFCC特征即使通过DCT去相关后仍存在一定的相关性，而拼帧后的特征间的相关性更强了。
因此将拼接后的帧通过LDA进行降维和去相关。LDA的核心思想为投影后类内方差最小，类间的方差最大，这里的类指的是声学模型的状态(如pdf-id)。
LDA降维后的特征通过MLLT再次进行去相关，MLLT引入了一种新形式的协方差矩阵，这种方法的每个高斯分量有两个方差矩阵:
 对角协方差矩阵$\Sigma_{diag}^{(m)}$ 半绑定非对角矩阵$H$，可以在多个高斯分量之间共享  第m个高斯分量的协方差矩阵可以描述为： $$ \Sigma^{(m)}=H\Sigma^{(m)}{diag}H^{T} $$ 令$A=H^{-1}$，则有： $$ \Sigma^{(m)-1}=A^{T}\Sigma^{(m-1)}{diag}A $$ 训练过程为EM算法：</description>
    </item>
    
    <item>
      <title>MFCC和FBANK特征</title>
      <link>https://cosmo1995.github.io/p/mfcc%E5%92%8Cfbank%E7%89%B9%E5%BE%81/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/mfcc%E5%92%8Cfbank%E7%89%B9%E5%BE%81/</guid>
      <description>MFCC（Mel Frequency Cepstral Coefficent）和FBANK特征在语音和说话人识别中被广泛使用。FBANK和MFCC计算的主要过程一致，MFCC是在FBANK的基础上做DCT变换。MFCC特征提取的过程可以分为预处理和倒谱两部分。
预处理 预加重 第一步是对语音信号应用预加重，以放大高频部分。将语音信号通过一个高通滤波器：
$$ y(t) = x(t) - \alpha x(t-1) $$ 其中滤波系数$\alpha$一般取0.95或0.97。
预加重滤波器在几种方面有用：（1）平衡频谱，因为高频通常比低频具有较小的能量；（2）避免在傅立叶变换操作期间出现数值问题；（3）还可改善信噪比（SNR）。
分帧 因为语音信号是快速变化的，而FFT适用于分析平稳的信号。为了简化起见，我们假设音频信号在短时间范围内变化不大（当我们说它不变时，我们指的是统计上的，即统计上是平稳的，显然样本在不断变化。即使是短时间尺度）。我们将语音分成20-40ms帧（一般取帧长为25ms），如果帧过短，将没有足够的样本来获得可靠的频谱估计；如果帧过长，则信号在整个帧中变化太大。 为确保声学特征参数的平滑性，帧移一般为10ms，即相邻两帧之间有15ms的重叠。
加窗 特征提取时，每次取长为25ms的语音，进行离散傅立叶变换计算出一帧，接着步移10ms继续计算下一帧，相当于加了矩形窗。而棱角分明的矩形窗容易造成频谱泄露，可以选择使用汉明窗（Hamming Window）、汉宁窗(Hanning Window)等。
汉明窗： $$ w[n]=(1-\alpha)-\alpha cos(\frac{2\pi n}{N-1}) $$ 实践中$\alpha$一般取0.46，其中$0\leq n \leq N-1$，$N$为窗的长度。
通过应用汉明窗，可以降低傅立叶变换后旁瓣的强度(主瓣是变换为频谱之后振幅最大的那个波峰部分，而周围的小的波峰部分叫旁瓣)，取得更高质量的频谱。
这里也解释了为什么要帧移是10ms, 相邻帧之间有15ms的重叠, 由于帧与帧连接处的信号因为加窗而弱化。
倒谱 在对原始语音进行预加重、分帧、加窗等预处理后，就可以进行倒谱分析了。我们对每一帧语音进行FFT，得到每一帧语音的频谱图(spectrum)。

频谱图中，峰值表示语音的主要频率成分，我们把这些峰值称为共振峰（formants），而共振峰包含了声音的辨识属性。共振峰点的通过一条平滑曲线连接起来，这条曲线叫做频谱的包络（Spectral Envelope）。

我们可以这么理解，将原始的频谱由两部分组成：包络和细节。包络包含了语音的主要信息，因此我们的目的就是把包络部分提取出来，即分离频谱的包络和细节。
频谱用$X[k]$表示，包络为$H[k]$，细节为$E[k]$，则有 $$ X[k]=H[k]*E[k] $$ 在两边取对数，将乘法转换为加法，便于分离，有： $$ logX[k]=logH[k] + logE[k] $$ 因此我们的目的是在给定$logX[k]$的基础上，求$logH[k]$和$logE[k]$，使得$logX[k]=logH[k] + logE[k]$。
为了达到这一目的，我们在对数频谱上做FFT，频谱上做FFT相当于IFFT。在对数频谱上做IFFT就相当于在一个伪频率（pseudo-frequency）坐标轴上面描述信号。

由上面这个图我们可以看到，包络是主要是低频成分，我们把它看成是一个每秒4个周期的正弦信号。这样我们在伪坐标轴上面的4Hz的地方给它一个峰值。而频谱的细节部分主要是高频。我们把它看成是一个每秒100个周期的正弦信号。这样我们在伪坐标轴上面的100Hz的地方给它一个峰值。把它俩叠加起来就是原来的频谱信号了。
在实际中我们已经知道$logX[k]$，所以可以得到$x[k]$。而$h[k]$是$x[k]$的低频部分，那么我们将$x[k]$通过一个低通滤波器就可以得到$h[k]$了。
$x[k]$实际上就是倒谱Cepstrum（这个是一个新造出来的词，把频谱的单词spectrum的前面四个字母顺序倒过来就是倒谱的单词了）。而我们所关心的$h[k]$就是倒谱的低频部分，它在语音识别中被广泛用于描述特征。
那现在总结下倒谱分析，它实际上是这样一个过程：
1）将原语音信号经过傅里叶变换得到频谱：$X[k]=H[k]E[k]$
只考虑幅度就是：$||X[k] ||=||H[k]||\ ||E[k]||$
2）我们在两边取对数：$log||X[k] ||= log ||H[k] ||+ log ||E[k] ||$</description>
    </item>
    
    <item>
      <title>Kaldi源码之决策树4_构建决策树</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%914_%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91/</link>
      <pubDate>Sun, 08 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%914_%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91/</guid>
      <description>到现在为止，程序acc-tree-stats累积好了构建决策树所需的统计量，程序cluster-phones和compile-questions自动生成好了构建决策树所需的问题集。我们根据sets.int生成好了roots.int文件，那么我们就可以开始构建决策树，对三音素GMM的状态进行绑定。这次笔记的主要内容是讲解Kaldi如何构建决策树，实现对三音素GMM状态的绑定。 在这个笔记中，首先我会介绍构建决策树的主程序build-tree和主函数BuildTree，然后介绍主函数中用到的核心函数GetStubMap和SplitDecisionTree。
build-tree  作用：构建决策树 输入：累积的统计量treeacc、问题集questions.qst、roots.int、topo 输出：决策树tree  build-tree $context_opts --verbose=1 --max-leaves=$numleaves \  --cluster-thresh=$cluster_thresh $dir/treeacc $lang/phones/roots.int \  $dir/questions.qst $lang/topo $dir/tree   读取roots.int，得到
1)vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt; phone_sets，其一个元素包含roots.int的一行上的所有音素
2)vector&amp;lt;bool&amp;gt; is_shared_root，其一个元素指明该行的音素是否共享三个HMM状态的决策树树根
3)vector&amp;lt;bool&amp;gt; is_split_root，其一个元素指明是否对该行音素对应的决策树树根进行划分
  读取topo文件，得到保存HMM拓扑结构的对象HmmTopology topo
  读取treeacc，得到累积的统计量BuildTreeStatsType stats
  读取questions.qst，得到Questions qo
  std::vector&amp;lt;int32&amp;gt; phone2num_pdf_classes; topo.GetPhoneToNumPdfClasses(&amp;amp;phone2num_pdf_classes); 调用topo.GetPhoneToNumPdfClasses得到phone2num_pdf_classes，其元素保存每个音素对应的HMM状态数。
to_pdf = BuildTree(qo, phone_sets, phone2num_pdf_classes, is_shared_root, is_split_root, stats, thresh, max_leaves, cluster_thresh, P) 调用BuildTree，返回保存整个大决策树的 to_pdf
BuildTree EventMap *tree_stub = GetStubMap(P phone_sets phone2num_pdf_classes share_roots &amp;amp;num_leaves); .</description>
    </item>
    
    <item>
      <title>Kaldi源码之决策树3_构建问题集</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%913_%E6%9E%84%E5%BB%BA%E9%97%AE%E9%A2%98%E9%9B%86/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%913_%E6%9E%84%E5%BB%BA%E9%97%AE%E9%A2%98%E9%9B%86/</guid>
      <description>前面我们已经通过acc-tree-stats累积好了构建决策树所需的统计量，要建立一颗决策树，我们还需要构建问题集。在HTK中，问题集是人工定义的；而在kaldi中，问题集是通过训练数据自动生成的。kaldi中，通过cluster-phones生成问题集。
cluster-phones  作用：多个音素或多个音素集进行聚类。 输入：决策树相关统计量treeacc，多个音素集sets.int 输出：自动生成的问题集（每个问题由多个音素组成） 示例：  cluster-phones $context_opts $dir/treeacc $lang/phones/sets.int \  $dir/questions.int  过程：   context_opts指定context-width和central-position，默认的三音素参数N=3，P=1；从treeacc中读取统计量到BuildTreeStatsType stats；从sets.int读取phone set 保存到phone_sets；读取pdf_class_list，该变量指定构建问题集所考虑的HMM状态，默认为1，也就是只考虑三状态HMM的中间状态。 若指定的mode为questions，调用AutomaticallyObtainQuestions()自动生成问题集保存到phone_sets_out；若指定的model为k-means，调用KMeansClusterPhones()。 将上述函数自动生成的phone_sets_out写到questions.int。  sets.txt通过utils/prepare_lang.sh生成，其中每一行为相同base的音素，sets.int 和sets.txt实例：
sil	1spn	2AA0 AA1 AA2	3 4 5AE0 AE1 AE2	6 7 8...	...AutomaticallyObtainQuestions void AutomaticallyObtainQuestions(BuildTreeStatsType &amp;amp;stats, const std::vector&amp;lt;std::vector&amp;lt;int32&amp;gt; &amp;gt; &amp;amp;phone_sets_in, const std::vector&amp;lt;int32&amp;gt; &amp;amp;all_hmm_positions_in, int32 P, std::vector&amp;lt;std::vector&amp;lt;int32&amp;gt; &amp;gt; *questions_out ) 通过对音素自动进行聚类，从而获取问题集；它把音素聚类成一棵树，并且对树中的每一个结点，把从该结点可以到达的所有叶子结点合在一起构成一个问题（该树的一个叶子结点保存着一些音素，一个问题就是一个音素的集合）。
std::vector&amp;lt;std::vector&amp;lt;int32&amp;gt; &amp;gt; phone_sets(phone_sets_in); std::vector&amp;lt;int32&amp;gt; phones; for (size_t i = 0; i &amp;lt; phone_sets.</description>
    </item>
    
    <item>
      <title>Kaldi源码之决策树2_累积统计量</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%912_%E7%B4%AF%E7%A7%AF%E7%BB%9F%E8%AE%A1%E9%87%8F/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%912_%E7%B4%AF%E7%A7%AF%E7%BB%9F%E8%AE%A1%E9%87%8F/</guid>
      <description>Build-Tree-Questions 在构建决策树时，我们需要知道的所有信息就是从训练数据的对齐中得到的所有EventType（三音素+HMM状态id），和每个EventType对应的统计量即Clusterable对象。很自然的，我们可以把这两者的对应关系保存成一个对pair&amp;lt;EventType, Clusterable*&amp;gt;，然后把所有的这些对保存成一个向量BuildTreeStatsType。
typedef std::vector&amp;lt;std::pair&amp;lt;EventType, Clusterable*&amp;gt; &amp;gt; BuildTreeStatsType; acc-tree-stats   作用：为决策树的构建累积相关的统计量
  输入：声学模型、特征、对齐
  输出：统计量tree.acc文件
  acc-tree-stats $context_opts \  --ci-phones=$ciphonelist $alidir/final.mdl &amp;#34;$feats&amp;#34; \  &amp;#34;ark:gunzip -c $alidir/ali.JOB.gz|&amp;#34; $dir/JOB.treeacc 输入的声学模型一般为单音素训练得到的GMM模型。acc-tree-stats的核心就是AccumulateTreeStats函数。
AccumulateTreeStats void AccumulateTreeStats(const TransitionModel &amp;amp;trans_model, BaseFloat var_floor, int N, // context window size.  int P, // central position.  const std::vector&amp;lt;int32&amp;gt; &amp;amp;ci_phones, //const AccumulateTreeStatsInfo &amp;amp;info,  const std::vector&amp;lt;int32&amp;gt; &amp;amp;alignment, const Matrix&amp;lt;BaseFloat&amp;gt; &amp;amp;features, std::map&amp;lt;EventType, GaussClusterable*&amp;gt; *stats){ .</description>
    </item>
    
    <item>
      <title>Kaldi源码之决策树1_理论和描述</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%911_%E7%90%86%E8%AE%BA%E5%92%8C%E6%8F%8F%E8%BF%B0/</link>
      <pubDate>Sat, 08 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%911_%E7%90%86%E8%AE%BA%E5%92%8C%E6%8F%8F%E8%BF%B0/</guid>
      <description>triphone模型中，为了解决数据稀疏和参数量太大的问题，需要进行聚类和状态绑定。kaldi通过决策树进行状态绑定，即将相似的HMM状态聚类到同一个pdf，以减少总的pdf数。
决策树理论 决策树是一种自上而下的聚类方法。决策树包含叶子结点和非叶子结点，每一个叶子结点代表一类。
决策树的每一个结点包含一些状态的集合，我们可以计算该状态集生成对应观测帧的似然。在triphone模型中，将一系列HMM states输入到决策树的根节点中，对这些状态进行提问，根据问题的答案分为左子结点或者右子结点。对于每一个子结点，可以计算该结点的新似然。决策树分类的根据就是使得分裂后的子节点似然之和相比分裂之前增量最大。同时我们通过设置某些阈值来控制决策树何时停止分裂，例如分裂前后似然的增量阈值，结点对应的state occupancy等。
一般我们会将具有相同中间音素的triphone的同一个位置的所有状态构建一个决策树，如果有63个monophone，那么我们会构建189个决策树(kaldi中这189个决策树会放在一个大的决策树下)。下图为所有中间音素为&amp;quot;zh&amp;quot;的triphone第3个HMM state进行决策树聚类的例子。
决策树的似然等于每一个叶子结点的似然之和。若$L$为决策树某一个结点的似然，则有 $$ L=\sum_{t \in T}{\sum_{s \in S}\gamma_s(t) logP(x_t;\mu_S,\Sigma_S)} $$ 其中$S$为该结点对应的状态集，$T$为状态集$S$对应的所有帧，$\gamma_s(t)$为帧$x_t$由状态$s$生成的后验概率，$P$为概率分布。
若$P$为高斯分布，则有 $$ \begin {aligned} log P (x_t;\mu_S,\Sigma_S) &amp;amp;= log \frac{1}{(2\pi)^{\frac{D}{2}}{\lvert{\Sigma_S}\rvert}^{\frac{1}{2}}}exp[-\frac{1}{2}(x_t-\mu_S)^T\Sigma_S^{-1}(x_t-\mu_S)] \cr &amp;amp;= -\frac{1}{2}[Dlog2\pi + log\lvert{\Sigma_S}\rvert + (x_t-\mu_S)^T\Sigma_S^{-1}(x_t-\mu_S)] \end {aligned} $$ 那么有： $$ \begin {aligned} L &amp;amp;= -\frac{1}{2} (log[(2\pi)^D|\Sigma_S|] + D) \cdot \sum_{s \in S}\sum_{t \in T} \gamma_s(t) \cr &amp;amp;= -\frac{1}{2}(Dlog(2\pi) + \sum_{i=1}^D{log\Sigma_{ii}} + D) \cdot \sum_{s \in S}\sum_{t \in T} \gamma_s(t) \end {aligned} $$</description>
    </item>
    
    <item>
      <title>Kaldi源码之hmm</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bhmm/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bhmm/</guid>
      <description>kaldi中TransitionModel用来更新HMM的转移概率，通过hmm-topology描述每个phone的拓扑结构，包括phone对应的state和以及state之间的初始转移概率。通过transition-model定义了transition的基本单元transition-state和转移路径transition-id。
hmm-topology 以yesno的topo为例，yesno中有三个音素：SIL、Y、N ，phone id 分别为1、2、3。其中SIL有0~4共5个状态，5为non-emitting。yes/no均含有0~2共3个状态，3为non-emitting。topo中还定义了每个phone对应的状态之间的初始转移概率。
Kaldi通过HmmTopology类表示topo结构，在HmmTopology中定义了HmmState结构体来描述HMM-state，包含HMM-state对应的pdf-class和转移的目标状态、初始转移概率。
struct HmmState { int32 pdf_class; std::vector&amp;lt;std::pair&amp;lt;int32, BaseFloat&amp;gt; &amp;gt; transitions; ... } typedef std::vector&amp;lt;HmmState&amp;gt; TopologyEntry; TopologyEntry中包含一个phone对应的所有HMM-state以及这些HMM-state之间的转移路径和转移概率，即一个phone的完整topo结构。
hmm-topology的成员变量为：
std::vector&amp;lt;int32&amp;gt; phones_; std::vector&amp;lt;int32&amp;gt; phone2idx_; std::vector&amp;lt;TopologyEntry&amp;gt; entries_; hmm-topology类包含了上图中完整的topo信息，包括所有音素、每个音素对应的HMM-state和HMM-state间的转移路径和转移概率。
transition-model transition中相关的概念：
 phone：音素，id是从1开始的整数。 HMM-state：每个音素的state的id，从0开始的整数。 pdf-id：每个state相对应的GMM概率密度函数id，这个值是全局唯一从0开始的整数。在triphone中，pdf-id会替换为决策树聚类后的实际pdf-id。pdf-id又可以分为forward-pdf-id和self-loop-pdf-id，默认pdf-id=forward-pdf-id=self-loop-pdf-id。 transition-state：抽象出来的转移状态，对于monophone，和HMM-state一一对应；对于triphone，对应于上下文音素绑定的状态。用(phone,HMM-state,pdf-id)表示，从1开始。 transition-index：表示一个transition-state的转移路径的index，在每个状态内从0开始的整数。 transition-id：所有transition-state的转移路径的id，全局唯一从1开始的整数，跟(transition-state,transition-index)一一对应。  通过show-transitions可以查看这些变量的具体值，如下表。
transition-model中定义了triples,state2id_和id2state，分别表示transition-state对应的triples，每个transition-state对应的的第一个transition-id，每个transition-id对应的transition-state。
std::vector&amp;lt;Triple&amp;gt; triples_; // indexed by transition-state - 1 std::vector&amp;lt;int32&amp;gt; state2id_; // indexed by transition-state std::vector&amp;lt;int32&amp;gt; id2state_; // indexed by transition-id ... 通过这三个变量，transition-model中实现了一系列transition概念的转换，包括：
(phone, HMM-state, pdf-id) &amp;lt;&amp;mdash;&amp;gt; transition-state
(transition-state, transition-index) &amp;lt;&amp;mdash;&amp;gt; transition-id</description>
    </item>
    
    <item>
      <title>Kaldi源码之gmm_2</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_2/</link>
      <pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_2/</guid>
      <description>estimate-full-gmm和estimate-diag-gmm用来对GMM进行极大似然估计(MLE)，二者仅在协方差矩阵数据结构不同，下面均以estimate-full-gmm为例。
GMM GMM的分布函数为
$$ p(x|\mu,\Sigma) = \sum_{m=1}^{M} c_m \mathcal{N} (x;\mu_m,\Sigma_m) $$
其中$c_m$为第$m$个component的混合系数,$\sum_{m=1}^M c_m =1 $。
定义指示函数$1_{[z_t=m]}$，如果$x_t$来自第m个component，则$1_{[z_t=m]} = 1$，否则$1_{[z_t=m]} = 0$。
现在假设$1_{[z_t=m]}$已知，即$x_t$属于哪一个component是已知的话，我们就可以统计每一个component对应的帧数：
$$ N_m=\sum^N_{t=1} 1_{[z_t=m]} $$
那么GMM参数可以估计为：
$$ \begin {aligned} &amp;amp;c_m = \frac{\sum_{t=1}^N 1_{[z_t=m]}}{N} = \frac{N_m}{N} \cr &amp;amp;\mu_m= \frac{\sum_{t=1}^N 1_{[z_t=m]} \mathbf{x_t}}{N_m} \cr &amp;amp;\Sigma_m= \frac{\sum_{t=1}^N 1_{[z_t=m]}(\mathbf{x_t}-\mu_m)(\mathbf{x_t}-\mu_m)^T}{N_m} &amp;amp;\end {aligned} $$
然而$1_{[z_t=m]}$是未知的，我们并不知道每一帧$x_t$属于哪一个component。但是，我们可以计算后验概率$P(m|x_t)$，即第m个 component生成$x_t$的概率。 $$ P(m|x_t) = \frac{P(x_t|m)P(m)} {P(x_t)} = \frac{P(x_t|m)P(m)} {\sum^M_{n=1}P(x_t|n)P(n)} $$
其中后验概率$P(m|\mathbf{x_t})$也称为component occupation probability或者responsibility。
此时，我们可以通过后验概率$P(m|x_t)$计算每一个component对应的&amp;quot;soft counts&amp;quot;: $N_m^*$
$$ N_m^* = \sum_{t=1}^N P(m|x_t) $$</description>
    </item>
    
    <item>
      <title>Kaldi源码之gmm_1</title>
      <link>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_1/</link>
      <pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_1/</guid>
      <description>full-gmm和diag-gmm定义了GMM对象，两者不同之处在于协方差矩阵$\Sigma$的表达形式。diag-gmm为了减少计算量，认为输入特征的各个维度间是彼此独立的， 此时协方差矩阵变成对角矩阵，对角元素即为方差。
am-diag-gmm中定义了一个向量densities_，向量中存储的是diag-gmm对象，am-diag-gmm针对的是声学模型中的所有GMM。
full-gmm &amp;amp; diag-gmm 在FullGmm类中，成员变量为
Vector&amp;lt;BaseFloat&amp;gt; gconsts_; bool valid_gconsts_; Vector&amp;lt;BaseFloat&amp;gt; weights_; std::vector&amp;lt;SpMatrix&amp;lt;BaseFloat&amp;gt; &amp;gt; inv_covars_; Matrix&amp;lt;BaseFloat&amp;gt; means_invcovars_; 在DiagGmm类中，成员变量为
Vector&amp;lt;BaseFloat&amp;gt; gconsts_; bool valid_gconsts_; Vector&amp;lt;BaseFloat&amp;gt; weights_; Matrix&amp;lt;SpMatrix&amp;lt;BaseFloat&amp;gt; &amp;gt; inv_covars_; Matrix&amp;lt;BaseFloat&amp;gt; means_invcovars_; full-gmm和diag-gmm中并没有直接定义均值向量$\mu$和协方差矩阵$\Sigma$，而是定义了协方差矩阵的逆矩阵inv_covars——$\Sigma^{-1}$，以及均值向量和协方差逆矩阵的乘积向量means_invcovars——$\mu^T\Sigma^{-1}$。
在full-gmm中inv_covars中存储的是$\Sigma^{-1}$的下三角矩阵，而diag-gmm中存储的则为$\Sigma^{-1}$的对角向量。
注：
PackedMatrix：压缩矩阵，仅存储矩阵的下三角部分，对于$n\times n$的矩阵仅需存储$ \frac{n(n+1)}{2} $个元素
SpMatrix： SymmetricPackedMatrix 对称矩阵，继承自PackedMatrix
TpMatrix： TriangularPackedMatrix 三角矩阵，继承自PackedMatrix
由于对称矩阵和三角矩阵都只需存储矩阵的上三角或者下三角，因此可以使用PackedMatrix节约存储空间。
gconsts_ GMM的概率密度函数$(pdf)$为：
$$ \begin {aligned} f(x;\mu,\Sigma) &amp;amp;= \sum_{m=1}^{M} c_m \mathcal{N} (x;\mu_m,\Sigma_m) \cr &amp;amp;= \sum_{m=1}^{M}\frac{c_m}{(2\pi)^{\frac{D}{2}}{\lvert{\Sigma_m}\rvert}^{\frac{1}{2}}}exp[-\frac{1}{2}(x -\mu_m)^T\Sigma^{-1}_m(\mathbf x-\mu_m)] \end {aligned} $$
其中，$c_m$为GMM第$m$个分量(component)的混合系数，${\Sigma_m}$是协方差矩阵，${\mu_m}$是均值向量，$D$是数据的维度，并且有$\sum_{m=1}^M {c_m}= 1$。
对于离散语音数据，$f(\mathbf x;\mu,\Sigma)即P(\mathbf x|\mu,\Sigma)$，$loglike$为
$$ log P(x|\mu,\Sigma) = \sum_{t=1}^N log \sum_{m=1}^{M} c_m \mathcal{N} (x_t;\mu_m,\Sigma_m) $$</description>
    </item>
    
  </channel>
</rss>
