[{"content":"Kaldi中的特征空间变换 Kaldi中的特征空间变化的方法包含两类：\nSpeaker Independent：\n 线性判别分析LDA(Linear Discriminant Analysis) 拼帧Frame Splicing \u0026amp; 差分Delta 最大似然线性变换MLLT(Maximum Likelihood Linear Transform)/半绑定协方差STC(Semi-tied Covariance)  Speaker Adaptive：\n 特征空间最大似然线性回归fMLLR(feature-space Maximum Likelihood Linear Regression) 线性声道长度归一化LVTLN(Linear Vocal Tract Length Normalization) 倒谱均值方差归一化CMVN(Cepstral Mean and Variance Normalization)  Speaker Independent： Delta 为了更好的识别语音，我们在静态MFCC特征的基础上附加一阶（delta）和二阶（delta-delta）差分，以补偿HMM的声学模型做出的条件独立性假设。\nLDA+MLLT 首先对原始的MFCC特征进行拼帧(Frame Splicing)，将相邻的N帧拼接起来，如下面的例子将7帧拼接到一起。\nsteps/train_lda_mllt.sh --cmd \u0026#34;train.cmd\u0026#34; \\  --splice-opts \u0026#34;--left-context=3 --right-context=3\u0026#34; \\  2500 15000 $data $lang $tri1_ali $tri_2b 在HMM-GMM模型中，通过GMM对HMM的发射概率进行建模。为了减少GMM模型的参数量，我们通常使用对角协方差(diagonal covariance)矩阵，这就要特征的各个维度之间是独立的(independent)。MFCC特征即使通过DCT去相关后仍存在一定的相关性，而拼帧后的特征间的相关性更强了。\n因此将拼接后的帧通过LDA进行降维和去相关。LDA的核心思想为投影后类内方差最小，类间的方差最大，这里的类指的是声学模型的状态(如pdf-id)。\nLDA降维后的特征通过MLLT再次进行去相关，MLLT引入了一种新形式的协方差矩阵，这种方法的每个高斯分量有两个方差矩阵:\n 对角协方差矩阵$\\Sigma_{diag}^{(m)}$ 半绑定非对角矩阵$H$，可以在多个高斯分量之间共享  第m个高斯分量的协方差矩阵可以描述为： $$ \\Sigma^{(m)}=H\\Sigma^{(m)}{diag}H^{T} $$ 令$A=H^{-1}$，则有： $$ \\Sigma^{(m)-1}=A^{T}\\Sigma^{(m-1)}{diag}A $$ 训练过程为EM算法：\n 初始化变换矩阵$A$为单位矩阵$I$ 使用标准HMM公式估算均值和高斯分量权重 根据变换矩阵$A$估计每个高斯分量协方差矩阵的对角元素${\\Sigma^{m}_{diag}}$ 根据${\\Sigma^{m}_{diag}}$估计变换矩阵$A$ 循环3-4步，直至收敛或达到最大迭代次数  【3-4步的具体估计公式见参考文献4或5】\n在获得最终的变换矩阵后，转换后的特征重新训练GMM。\nSpeaker Adaptive 在训练自动语音识别模型的时候，训练数据和测试数据往往存在不匹配的现象。这些不匹配包括包括说话人特性（说话方式、口音等）及环境特性（如录音设备、房间混响等）。\n为了解决训练数据和测试数据往往存在不匹配，有两种方法：\n 模型空间——修改模型以更好地适应特征=\u0026gt;Adaptation 特征空间——变换特征以更好地适应模型=\u0026gt;Normalization  语音识别系统可以分为两类：\n 说话人无关(Speaker Independent，SI)，不需要每个说话人的大量训练数据，WER较高 说话人相关(Speaker Dependent, SD)，需要每个说话人的大量训练数据，WER低  说话人自适应SA的目的就是在SI（即不需要特定说话人的大量训练数据）的基础上达到SD的错误率。\nfMLLR MLLR有两种变体：unconstrained 和 constrained， MLLR是基于模型空间的方法，constrained MLLR(cMLLR)又叫fMLLR，是基于特征空间的方法。我们主要讨论fMLLR，即对均值和方差应用相同的变换矩阵$\\tilde{A}$。 $$ \\mu^{(sm)} = \\tilde{A}^{(s)}\\mu^{(m)} + \\tilde{b}^{(s)};\\ \\Sigma^{(sm)} = \\tilde{A}^{(s)}\\Sigma^{(m)} \\tilde{A}^{(s)T} $$ 其中$s$代表说话人。此时，似然可以描述为： $$ \\mathcal{N}(y;\\mu^{(sm)},\\Sigma^{(sm)})=|A^{(s)}| \\mathcal{N}(A^{(s)}y +b^{(s)};\\mu^{(m)},\\Sigma^{(m)}) $$\n其中， $$ A^{(s)} = \\tilde{A}^{(s)-1};\\ b^{(s)} = -\\tilde{A}^{(s)-1}\\tilde{b^{(s)}} $$\n线性变换矩阵$\\tilde{A}$可以是全矩阵，分块对角矩阵或对角矩阵。\nkaldi中一般在LDA+MLLT后训练SAT模型，在SAT中应用fMLLR。\nsteps/train_sat.sh --cmd \u0026#34;$train_cmd\u0026#34; \\  11500 200000 data/train_nodup data/lang exp/tri3_ali_nodup exp/tri4 SAT的训练过程：\n 训练speaker independent模型 使用简单的目标模型计算维特比路径 使用维特比路径估计每个说话人的fMLLR变换矩阵 使用估计的fMLLR变换矩阵对特征进行变换 在变换后的特征基础上训练SAT模型  SAT的识别过程：\n 首先使用speaker independent模型进行识别 使用简单目标模型在无监督的情况下估计fMLLR变换矩阵 使用估计的fMLLR变换矩阵对特征进行变换 变换后的特征输入SAT模型进行识别  CMVN Kaldi中CMVN首先根据spk2utt计算每个说话人对应语音特征的均值和方差，然后将特征值减去均值再除以标准差。CMVN归一化后的特征均值为0，方差为1。经验上表明，CMVN使得特征对说话人的变化、加性噪声等更具鲁棒性。 $$ \\begin{aligned} \\mu_x \u0026amp;= \\frac{1}{T}\\sum_tx_t \\cr \\sigma_x^2 \u0026amp;= \\frac{1}{T}\\sum_{t=0}^{T-1}x^2_t-\\mu_x^2 \\cr x_t \u0026amp;= \\frac{x_t-\\mu_x}{\\sigma_x} \\end{aligned} $$\nsteps/compute_cmvn_stats.sh data/train Kaldi中并没有直接保存CMVN变换后的特征，而是通过compute_cmvn_stats计算并保存每个说话人对应的统计量，该统计量包括：该说话人对应的所有语音帧特征值的和、帧数以及平方和。\nfeats=\u0026#34;ark,s,cs:apply-cmvn $cmvn_opts--utt2spk=ark:$sdata/JOB/utt2spk scp:$sdata/JOB/cmvn.scp scp:$sdata/JOB/feats.scp ark:- |\u0026#34; 通过on the fly的方式计算CMVN特征。\nReference  https://kaldi-asr.org/doc/transform.html http://mi.eng.cam.ac.uk/~sjy/papers/gayo07.pdf http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr12-adapt.pdf http://www.iiisci.org/journal/CV$/sci/pdfs/P896582.pdf http://smil.csie.ntnu.edu.tw/ppt/20070727_Winston_Semi-Tied%20Covariance%20Matrices%20for%20Hidden%20Markov%20Models.pdf https://www.ee.columbia.edu/~stanchen/spring16/e6870/slides/lecture9.pdf https://zhuanlan.zhihu.com/p/264157113?utm_source=wechat_session  ","date":"2021-02-24T00:00:00Z","permalink":"https://cosmo1995.github.io/p/kaldi%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2/","title":"Kaldi中的特征变换"},{"content":"MFCC（Mel Frequency Cepstral Coefficent）和FBANK特征在语音和说话人识别中被广泛使用。FBANK和MFCC计算的主要过程一致，MFCC是在FBANK的基础上做DCT变换。MFCC特征提取的过程可以分为预处理和倒谱两部分。\n预处理 预加重 第一步是对语音信号应用预加重，以放大高频部分。将语音信号通过一个高通滤波器：\n$$ y(t) = x(t) - \\alpha x(t-1) $$ 其中滤波系数$\\alpha$一般取0.95或0.97。\n预加重滤波器在几种方面有用：（1）平衡频谱，因为高频通常比低频具有较小的能量；（2）避免在傅立叶变换操作期间出现数值问题；（3）还可改善信噪比（SNR）。\n分帧 因为语音信号是快速变化的，而FFT适用于分析平稳的信号。为了简化起见，我们假设音频信号在短时间范围内变化不大（当我们说它不变时，我们指的是统计上的，即统计上是平稳的，显然样本在不断变化。即使是短时间尺度）。我们将语音分成20-40ms帧（一般取帧长为25ms），如果帧过短，将没有足够的样本来获得可靠的频谱估计；如果帧过长，则信号在整个帧中变化太大。 为确保声学特征参数的平滑性，帧移一般为10ms，即相邻两帧之间有15ms的重叠。\n加窗 特征提取时，每次取长为25ms的语音，进行离散傅立叶变换计算出一帧，接着步移10ms继续计算下一帧，相当于加了矩形窗。而棱角分明的矩形窗容易造成频谱泄露，可以选择使用汉明窗（Hamming Window）、汉宁窗(Hanning Window)等。\n汉明窗： $$ w[n]=(1-\\alpha)-\\alpha cos(\\frac{2\\pi n}{N-1}) $$ 实践中$\\alpha$一般取0.46，其中$0\\leq n \\leq N-1$，$N$为窗的长度。\n通过应用汉明窗，可以降低傅立叶变换后旁瓣的强度(主瓣是变换为频谱之后振幅最大的那个波峰部分，而周围的小的波峰部分叫旁瓣)，取得更高质量的频谱。\n这里也解释了为什么要帧移是10ms, 相邻帧之间有15ms的重叠, 由于帧与帧连接处的信号因为加窗而弱化。\n倒谱 在对原始语音进行预加重、分帧、加窗等预处理后，就可以进行倒谱分析了。我们对每一帧语音进行FFT，得到每一帧语音的频谱图(spectrum)。\n\n频谱图中，峰值表示语音的主要频率成分，我们把这些峰值称为共振峰（formants），而共振峰包含了声音的辨识属性。共振峰点的通过一条平滑曲线连接起来，这条曲线叫做频谱的包络（Spectral Envelope）。\n\n我们可以这么理解，将原始的频谱由两部分组成：包络和细节。包络包含了语音的主要信息，因此我们的目的就是把包络部分提取出来，即分离频谱的包络和细节。\n频谱用$X[k]$表示，包络为$H[k]$，细节为$E[k]$，则有 $$ X[k]=H[k]*E[k] $$ 在两边取对数，将乘法转换为加法，便于分离，有： $$ logX[k]=logH[k] + logE[k] $$ 因此我们的目的是在给定$logX[k]$的基础上，求$logH[k]$和$logE[k]$，使得$logX[k]=logH[k] + logE[k]$。\n为了达到这一目的，我们在对数频谱上做FFT，频谱上做FFT相当于IFFT。在对数频谱上做IFFT就相当于在一个伪频率（pseudo-frequency）坐标轴上面描述信号。\n\n由上面这个图我们可以看到，包络是主要是低频成分，我们把它看成是一个每秒4个周期的正弦信号。这样我们在伪坐标轴上面的4Hz的地方给它一个峰值。而频谱的细节部分主要是高频。我们把它看成是一个每秒100个周期的正弦信号。这样我们在伪坐标轴上面的100Hz的地方给它一个峰值。把它俩叠加起来就是原来的频谱信号了。\n在实际中我们已经知道$logX[k]$，所以可以得到$x[k]$。而$h[k]$是$x[k]$的低频部分，那么我们将$x[k]$通过一个低通滤波器就可以得到$h[k]$了。\n$x[k]$实际上就是倒谱Cepstrum（这个是一个新造出来的词，把频谱的单词spectrum的前面四个字母顺序倒过来就是倒谱的单词了）。而我们所关心的$h[k]$就是倒谱的低频部分，它在语音识别中被广泛用于描述特征。\n那现在总结下倒谱分析，它实际上是这样一个过程：\n1）将原语音信号经过傅里叶变换得到频谱：$X[k]=H[k]E[k]$\n只考虑幅度就是：$||X[k] ||=||H[k]||\\ ||E[k]||$\n2）我们在两边取对数：$log||X[k] ||= log ||H[k] ||+ log ||E[k] ||$\n3）再在两边取傅里叶逆变换得到：$x[k]=h[k]+e[k]$\nMel滤波 通过以上的步骤，给我们一段语音，我们可以得到了它的频谱包络。但是，对于人类听觉感知的实验表明，人类听觉的感知只聚焦在某些特定的区域，而不是整个频谱包络。同时人耳对不同频率的敏感程度不同（人耳对低频声音的变化比高频的变化更敏感），且成非线性关系。\n因此在FFT获得频谱后，首先通过Mel滤波，然后在Mel频谱上进行对数运算和IFFT。我们将频谱按人耳敏感程度分为多个Mel滤波器组，在Mel刻度范围内，各个滤波器的中心频率是相等间隔的线性分布，但在频率范围不是相等间隔的。 $$ mel(f)=2595*log_{10}(1+f/700) $$ 将频谱通过一组Mel尺度的三角形滤波器组，一般用40个滤波器，每个滤波在中心频率的响应都是1，然后线性下降，一直到相邻三角滤波的中心频率处为0，如图所示： 总结 至此，我们已经给出MFCC特征计算的全过程，FBANK特征相比于MFCC少了最后一步的IFFT变换。\nFBANK 1）先对语音进行预加重、分帧和加窗；\n2）对每一个短时分析窗，通过FFT得到对应的频谱；\n3）将上面的频谱通过Mel滤波器组得到Mel频谱；\n4）在Mel频谱取对数\nMFCC MFCC特征在FBANK即基础上做IFFT变换，实际一般用DCT变换代替IFFT。DCT是FFT的一个变种，好处是结果是实数，没有虚部。DCT还有一个特点是，对于一般的语音信号，这一步的结果的前几个系数比较大，后面的系数比较小，可以忽略。上面说了一般取40个三角形，所以DCT的结果也是40个点；实际中，一般仅保留前12~20个，这就进一步压缩了数据。\nFBANK与MFCC 由于Mel相邻的滤波器存在重叠部分，FBANK特征之间存在高度的相关性。MFCC需要离散余弦变换（DCT）来使FBANK系数去相关，该过程也称为白化。MFCC特征一般用于GMM-HMM模型，而FBANK特征一般用于DNN模型，因为深度神经网络不太容易受到高度相关的输入的影响。\n结论 如果机器学习算法不易受到高度相关的输入的影响，推荐使用FBANK。如果机器学习算法易受相关输入的影响，则使用MFCC更好。\nReference  https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html https://blog.csdn.net/wbgxx333/article/details/10020449 http://www.speech.cs.cmu.edu/15-492/slides/03_mfcc.pdf https://www.jianshu.com/p/8369b39f362f  ","date":"2021-02-24T00:00:00Z","permalink":"https://cosmo1995.github.io/p/mfcc%E5%92%8Cfbank%E7%89%B9%E5%BE%81/","title":"MFCC和FBANK特征"},{"content":"到现在为止，程序acc-tree-stats累积好了构建决策树所需的统计量，程序cluster-phones和compile-questions自动生成好了构建决策树所需的问题集。我们根据sets.int生成好了roots.int文件，那么我们就可以开始构建决策树，对三音素GMM的状态进行绑定。这次笔记的主要内容是讲解Kaldi如何构建决策树，实现对三音素GMM状态的绑定。 在这个笔记中，首先我会介绍构建决策树的主程序build-tree和主函数BuildTree，然后介绍主函数中用到的核心函数GetStubMap和SplitDecisionTree。\nbuild-tree  作用：构建决策树 输入：累积的统计量treeacc、问题集questions.qst、roots.int、topo 输出：决策树tree  build-tree $context_opts --verbose=1 --max-leaves=$numleaves \\  --cluster-thresh=$cluster_thresh $dir/treeacc $lang/phones/roots.int \\  $dir/questions.qst $lang/topo $dir/tree   读取roots.int，得到\n1)vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; phone_sets，其一个元素包含roots.int的一行上的所有音素\n2)vector\u0026lt;bool\u0026gt; is_shared_root，其一个元素指明该行的音素是否共享三个HMM状态的决策树树根\n3)vector\u0026lt;bool\u0026gt; is_split_root，其一个元素指明是否对该行音素对应的决策树树根进行划分\n  读取topo文件，得到保存HMM拓扑结构的对象HmmTopology topo\n  读取treeacc，得到累积的统计量BuildTreeStatsType stats\n  读取questions.qst，得到Questions qo\n  std::vector\u0026lt;int32\u0026gt; phone2num_pdf_classes; topo.GetPhoneToNumPdfClasses(\u0026amp;phone2num_pdf_classes); 调用topo.GetPhoneToNumPdfClasses得到phone2num_pdf_classes，其元素保存每个音素对应的HMM状态数。\nto_pdf = BuildTree(qo, phone_sets, phone2num_pdf_classes, is_shared_root, is_split_root, stats, thresh, max_leaves, cluster_thresh, P) 调用BuildTree，返回保存整个大决策树的 to_pdf\nBuildTree EventMap *tree_stub = GetStubMap(P phone_sets phone2num_pdf_classes share_roots \u0026amp;num_leaves); ... std::vector\u0026lt;int32\u0026gt; nonsplit_phones; for (size_t i = 0; i \u0026lt; phone_sets.size(); i++) if (!do_split[i]) nonsplit_phones.insert(nonsplit_phones.end(), phone_sets[i].begin(), phone_sets[i].end()); ... BuildTreeStatsType filtered_stats; if (!nonsplit_phones.empty()) FilterStatsByKey(stats, P, nonsplit_phones, false, \u0026amp;filtered_stats); //retain only those not in \u0026#34;nonsplit_phones\u0026#34;  EventMap *tree_split = SplitDecisionTree(*tree_stub, nonsplit_phones.empty() ? stats : filtered_stats, qopts, thresh, max_leaves, \u0026amp;num_leaves, \u0026amp;impr, \u0026amp;smallest_split);  调用GetStubMap得到初始的决策树tree_stub，也就是扩展前的决策树。扩展前的决策树的一个叶子结点对应roots.int中的一行的决策树的树根 FilterStatsByKey过滤phone_sets中not split的元素 调用SplitDecisionTree，将tree_stub的每个叶子结点扩展成决策树，对每一个音素生成实际的决策树  GetStubMap  作用：构建初始决策树 输入：音素中间位置P、roots.int中每一行音素phone_sets、每个音素对应的HMM state数phone2num_pdf_classes、roots.int中每一行的三个HMM state是否共享树根share_roots 输出：决策树*tree_stub  EventMap *GetStubMap(int32 P, const std::vector\u0026lt;std::vector\u0026lt;int32\u0026gt; \u0026gt; \u0026amp;phone_sets, const std::vector\u0026lt;int32\u0026gt; \u0026amp;phone2num_pdf_classes, const std::vector\u0026lt;bool\u0026gt; \u0026amp;share_roots, int32 *num_leaves_out){ if (phone_sets.size() == 1) { if (share_roots[0]) { return new ConstantEventMap( (*num_leaves_out)++ ); }else{ ... std::map\u0026lt;EventValueType, EventAnswerType\u0026gt; m; for (EventAnswerType p = 0; p \u0026lt; max_len; p++) m[p] = (*num_leaves_out)++; return new TableEventMap(kPdfClass, m); //split on hmm-position  } else if (max_set_size == 1 \u0026amp;\u0026amp; static_cast\u0026lt;int32\u0026gt;(phone_sets.size()) \u0026lt;= 2*highest_numbered_phone) { std::map\u0026lt;EventValueType, EventMap*\u0026gt; m; for (size_t i = 0; i \u0026lt; phone_sets.size(); i++) { std::vector\u0026lt;std::vector\u0026lt;int32\u0026gt; \u0026gt; phone_sets_tmp; phone_sets_tmp.push_back(phone_sets[i]); std::vector\u0026lt;bool\u0026gt; share_roots_tmp; share_roots_tmp.push_back(share_roots[i]); EventMap *this_stub = GetStubMap(P, phone_sets_tmp, phone2num_pdf_classes, share_roots_tmp, num_leaves_out) m[phone_sets_tmp[0][0]] = this_stub; } return new TableEventMap(P, m); } else { size_t half_sz = phone_sets.size() / 2; std::vector\u0026lt;std::vector\u0026lt;int32\u0026gt; \u0026gt;::const_iterator half_phones = phone_sets.begin() + half_sz; std::vector\u0026lt;std::vector\u0026lt;int32\u0026gt; \u0026gt;::const_iterator half_share = share_roots.begin() + half_sz; std::vector\u0026lt;std::vector\u0026lt;int32\u0026gt; \u0026gt; phone_sets_1, phone_sets_2; std::vector\u0026lt;bool\u0026gt; share_roots_1, share_roots_2; phone_sets_1.insert(phone_sets_1.end(), phone_sets.begin(), half_phones); phone_sets_2.insert(phone_sets_2.end(), half_phones, phone_sets.end()); share_roots_1.insert(share_roots_1.end(), share_roots.begin(), half_share); share_roots_2.insert(share_roots_2.end(), half_share, share_roots.end()); EventMap *map1 = GetStubMap(P,phone_sets_1, phone2num_pdf_classes, share_roots_1, num_leaves_out); EventMap *map2 = GetStubMap(P,phone_sets_2, phone2num_pdf_classes, share_roots_2, num_leaves_out); std::vector\u0026lt;EventKeyType\u0026gt; all_in_first_set; for (size_t i = 0; i \u0026lt; half_sz; i++) for (size_t j = 0; j \u0026lt; phone_sets[i].size(); j++) all_in_first_set.push_back(phone_sets[i][j]); ... return new SplitEventMap(P, all_in_first_set, map1, map2); } } phone_sets的一个元素是roots.int的一行上的全部音素，roots.int有63行，则phone_sets有63个元素。\n根据phone_sets可以分为三种情况：\n  phone_sets大小为1，此时进入第6行if\n  若该行音素为shared，则创建叶子结点ConstantEventMap，其answer_为num_leaves_out，结点创建后num_leaves_out++\n  若该行音素为not shared，则首先获得所有音素的最大HMM state数max_len，m中保存每个HMM state的num_leaves_out，创建TableEventMap，\n其key_为-1，table_为m\n    phone_sets大小不为1，但是每个元素均只有1个音素，此时进入第16行else if\n 遍历phone_sets每个元素，调用GetStubMap，由于每个元素大小为1，进入情况1，返回ConstantEventMap结点，保存到m中 遍历结束后，m中保存了phone_sets每个音素及其对应的叶子结点ConstantEventMap 创建TableEventMap，其key_为P，table_为m    phone_sets不满足上述2中情况，此时进入第29行else。\n 将phone_sets均分为两半phone_sets_1和phone_sets_2，share_roots同样分为两半share_roots_1和share_roots_2。将前后两半的phone_sets和share_roots作为参数递归调用GetStubMap，分别返回左右子树map1和map2 创建SplitEventMap，其key_为P，yes_set为phone_sets前一半元素中所有的音素，yes_结点指向map1，no_结点指向map2    调用GetStubMap生成的最终的决策树中叶子结点数等于roots.int行数，随后把每个叶子结点扩展成各自的决策树。\nTableEventMap作用：\n从上面的过程我们发现，SplitEventMap每次划分我们只能创建两个孩子结点，包含63个元素的phone_sets要划分5次才能到达第一个叶子结点，太慢了。当某一次递归调用GetStubMap时我们发现phone_sets的大小不为一但是其每一个元素只包含一个音素，这时我们进入最外层的else if分支，生成一个TE。假设此时的phone_sets中保存的是roots.int的第41行到第46行，生成的TE的table_包含150个元素，第0到144个都为NULL，第145到150个都是CE，我们一下子生成了第41行到第46行对应的6个叶子结点，比用SE划分快多了。\nSplitDecisionTree  作用：扩展原始决策树，构建完整决策树 输入：  原始决策树input_map roots.int中每一行统计量stats 问题集q_opts 决策树分裂导致的似然提升阈值thresh 最大叶子结点数max_leaves roots.int中每一行对应的叶子结点num_leaves   输出：决策树*tree_stub  EventMap *SplitDecisionTree(const EventMap \u0026amp;input_map, const BuildTreeStatsType \u0026amp;stats, Questions \u0026amp;q_opts, BaseFloat thresh, int32 max_leaves, // max_leaves\u0026lt;=0 -\u0026gt; no maximum.  int32 *num_leaves, BaseFloat *obj_impr_out, BaseFloat *smallest_split_change_out){ std::vector\u0026lt;DecisionTreeSplitter*\u0026gt; builders; { std::vector\u0026lt;BuildTreeStatsType\u0026gt; split_stats; SplitStatsByMap(stats, input_map, \u0026amp;split_stats); builders.resize(split_stats.size)); for (size_t i = 0;i \u0026lt; split_stats.size();i++) { EventAnswerType leaf = static_cast\u0026lt;EventAnswerType\u0026gt;(i); if (split_stats[i].size() == 0) num_empty_leaves++; builders[i] = new DecisionTreeSplitter(leaf, split_stats[i], q_opts); } } } 首先调用SplitStatsByMap，根据tree_stub对stats进行划分得到split_stats，将roots.int中同一行，即同一个叶子结点的统计量放在一起。\n对tree_stub的每一个叶子结点初始化一个DecisionTreeSplitter对象，使用该对象构建叶子结点对应的决策树。我们注意到初始化DecisionTreeSplitter对象时传递的参数包括属于该叶子结点的统计量split_stats[i]、问题集q_opts，有这两者我们就可以构建起决策树。最终的builders包含63个DecisionTreeSplitter且一直包含63个DecisionTreeSplitter。\nvoid FindBestSplit() { std::vector\u0026lt;EventKeyType\u0026gt; all_keys; q_opts_.GetKeysWithQuestions(\u0026amp;all_keys); ... for (size_t i = 0;i \u0026lt; all_keys.size();i++) { if (q_opts_.HasQuestionsForKey(all_keys[i])) { std::vector\u0026lt;EventValueType\u0026gt; temp_yes_set; BaseFloat split_improvement = FindBestSplitForKey(stats_, q_opts_, all_keys[i], \u0026amp;temp_yes_set); if (split_improvement \u0026gt; best_split_impr_) { best_split_impr_ = split_improvement; yes_set_ = temp_yes_set; key_ = all_keys[i]; } } } } DecisionTreeSplitter对象初始化时，会调用FindBestSplit寻找该对象对应叶子结点的最优化分。FindBestSplit首先通过GetKeysWithQuestions获取问题集的所有all_keys（-1，0，1，2）。对于all_keys中每一个key，在该key对应的问题集中找到一个问题FindBestSplitForKey，使得对叶子结点划分后获得的似然提升split_improvement最大；现在对每个key都找到了该key对应的似然最大提升，然后比较这几个key的似然最大提升，再找出其中的最大似然提升best_split_impr_。将DecisionTreeSplitter的key_设置为似然提升最大的key；yes_set_设置为该key上取得最大似然提升的问题；best_split_impr_设置为 最大的似然提升。因为我们要对决策树进行持续划分，所以DecisionTreeSplitter还保存着指向两个子树的指针 *yes_, *no_。当第key个位置的音素属于yes_set_时进入yes_子树，否则进入no_子树。\n{ // Do the splitting.  int32 count = 0; std::priority_queue\u0026lt;std::pair\u0026lt;BaseFloat, size_t\u0026gt; \u0026gt; queue; // use size_t because logically these  // are just indexes into the array, not leaf-ids (after splitting they are no longer leaf id\u0026#39;s).  // Initialize queue.  for (size_t i = 0; i \u0026lt; builders.size(); i++) queue.push(std::make_pair(builders[i]-\u0026gt;BestSplit(), i)); // Note-- queue\u0026#39;s size never changes from now. All the alternatives leaves to split are  // inside the \u0026#34;DecisionTreeSplitter*\u0026#34; objects, in a tree structure.  while (queue.top().first \u0026gt; thresh \u0026amp;\u0026amp; (max_leaves\u0026lt;=0 || *num_leaves \u0026lt; max_leaves)) { smallest_split_change = std::min(smallest_split_change, queue.top().first); size_t i = queue.top().second; like_impr += queue.top().first; builders[i]-\u0026gt;DoSplit(num_leaves); queue.pop(); queue.push(std::make_pair(builders[i]-\u0026gt;BestSplit(), i)); count++; } } 把63个决策树（builders）放在一起比较，每次对似然提升最大的叶子结点进行划分，这种策略通过优先队列实现。划分具体就是指，对处于叶子位置的DecisionTreeSplitter找到第key_个位置和在该位置所问的问题yes_set_，同时将该DecisionTreeSplitter上的统计量根据划分结果分配到两个孩子*yes_和*no_。我们对所有的builders持续进行划分，在tree_stub的每个叶子结点就生成了一棵结点全是DecisionTreeSplitter的树。\nEventMap *answer = NULL; { // Create the output EventMap.  std::vector\u0026lt;EventMap*\u0026gt; sub_trees(builders.size()); for (size_t i = 0; i \u0026lt; sub_trees.size();i++) sub_trees[i] = builders[i]-\u0026gt;GetMap(); answer = input_map.Copy(sub_trees); for (size_t i = 0; i \u0026lt; sub_trees.size();i++) delete sub_trees[i]; } 对tree_stub的每个叶子结点，我们通过调用GetMap创建对应的子树，最终得到roots.int每一行的的决策树sub_trees。生成好属于roots.int每一行的决策树之后，通过copy对tree_stub进行扩展，把tree_stub中叶子结点替换成对应的子树sub_trees，于是，完整的大决策树就生成了。\ntree文件 ContextDependency 3 1 ToPdf SE 1 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \\\r26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\\\r60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 9\\\r3 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 1\\\r20 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 14\\\r5 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170\\\r171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 \\\r196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 ]\r{ SE 1 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\\\r35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 6\\\r8 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 10\\\r1 102 103 104 105 106 107 108 109 110 111 ]\r{ SE 1 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\\\r35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 ]\r{ SE 1 [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ]\r{ SE 1 [ 1 2 3 ]\r{ TE -1 5 ( CE 0 CE 1 CE 2 CE 3 CE 4 )\rSE -1 [ 0 ]\r{ SE 2 [ 220 221 222 223 ]\r{ SE 0 [ 104 105 106 107 112 113 114 115 172 173 174 175 208 209 210 211 212 213 214 215 264 265 266 \\\r267 280 281 282 283 284 285 286 287 ]\r{ CE 5 CE 696 }\rSE 2 [ 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 132 \\\r133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 248 249 250 251 252 253 254 255 256 257 2\\\r58 259 260 261 262 263 268 269 270 271 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 30\\\r3 ]\rtree文件以对象的名字ContextDependency开始；然后是N（上下文窗的大小），这里是3；接着是P（上下文窗的中心位置），这里是1。\n树顶层的EventMap是一个以key_ 为1进行分裂的SplitEventMap，也就是按中心音素分裂。在方括号中是一系列连续范围的phone-ids。然而，这些并不表示一个问题，它们只是音素分裂的一种方法，因此我们可以得到每个音素真正的决策树。\n文件后面的一些“SE”标签也是quasi-tree的一部分，它们都是首先按中心音素进行分裂（当我们顺着文件往下看时我们进入了树的更深处；注意这个花括号“{”一直是打开的，还没有关闭）。然后我们看到17行“TE -1 5 ( CE 0 CE 1 CE 2 CE 3 CE 4 ) ”，HMM状态5进行分裂，表示通过TableEventMap对pdf-class -1进行分裂，并且返回从0到4的值。这5个值表示的是静音和噪声音素SIL，NSN和SPN的5个pdf-ids。在我们的设定中，这三个非语音音素的pdfs是共享的（只有转移矩阵是不同的）。注意：对于这些音素我们用5状态而不是3状态的HMM，所以这里有5个不同的pdf-ids。\n接下来18行是“SE -1 [ 0 ] ”，这可以被认为是这棵树中第一个真正的问题。是不是最左边的HMM-state,我们可以从上面的SE问题看出这个问题被应用于中心音素为4到19时候，也就是音素AA的不同版本（注：原文写的是5到19，不过我认为原文有问题，改成了4到19）。这个问题问的是pdf-class（key -1）是不是0（即是不是最左边的HMM-state）。下一个问题是“SE 2 [ 220 221 222 223 ]”，问的是音素右上下文是不是音素“M”不同形式中的一个（这是一个非常有效的问题，因为我们是在最左边的HMM-state）；如果问题的答案是yes，我们继续问“SE 0 [ 104 105 106 107… 286 287 ]”，这是一个关于音素左上下文的问题（注：原文写的是右上下文，但应该是左上下文）；如果答案是yes，则pdf-id就是5（“CE 5”），否则就是696（“CE 696”）。\nContextDependency 1 0 ToPdf TE 0 49 ( NULL TE -1 3 ( CE 0 CE 1 CE 2 )\rTE -1 3 ( CE 3 CE 4 CE 5 )\rTE -1 3 ( CE 6 CE 7 CE 8 )\rTE -1 3 ( CE 9 CE 10 CE 11 )\rTE -1 3 ( CE 12 CE 13 CE 14 )\r上面是一个更简单的例子。顶层的EventMap是一个TableEventMap（“TE 0 49 …”）。key_ 0是音素位置0，表示中心（并且只有这一个）音素，因为上下文窗大小（N）为1。TE的条目数量是49（音素的数量加1）。表中第一个EventMap是NULL，因为没有序号为0的音素。下一个EventMap是一个有三个元素的TableEventMap，关联到第一个音素的三个HMM状态（技术上来说，是pdf-class）：“TE -1 3 ( CE 0 CE 1 CE 2 )”。\nReference  https://blog.csdn.net/u010731824/article/details/69666560  ","date":"2019-09-08T00:00:00Z","permalink":"https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%914_%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91/","title":"Kaldi源码之决策树4_构建决策树"},{"content":"前面我们已经通过acc-tree-stats累积好了构建决策树所需的统计量，要建立一颗决策树，我们还需要构建问题集。在HTK中，问题集是人工定义的；而在kaldi中，问题集是通过训练数据自动生成的。kaldi中，通过cluster-phones生成问题集。\ncluster-phones  作用：多个音素或多个音素集进行聚类。 输入：决策树相关统计量treeacc，多个音素集sets.int 输出：自动生成的问题集（每个问题由多个音素组成） 示例：  cluster-phones $context_opts $dir/treeacc $lang/phones/sets.int \\  $dir/questions.int  过程：   context_opts指定context-width和central-position，默认的三音素参数N=3，P=1；从treeacc中读取统计量到BuildTreeStatsType stats；从sets.int读取phone set 保存到phone_sets；读取pdf_class_list，该变量指定构建问题集所考虑的HMM状态，默认为1，也就是只考虑三状态HMM的中间状态。 若指定的mode为questions，调用AutomaticallyObtainQuestions()自动生成问题集保存到phone_sets_out；若指定的model为k-means，调用KMeansClusterPhones()。 将上述函数自动生成的phone_sets_out写到questions.int。  sets.txt通过utils/prepare_lang.sh生成，其中每一行为相同base的音素，sets.int 和sets.txt实例：\nsil\t1\rspn\t2\rAA0 AA1 AA2\t3 4 5\rAE0 AE1 AE2\t6 7 8\r...\t...\rAutomaticallyObtainQuestions void AutomaticallyObtainQuestions(BuildTreeStatsType \u0026amp;stats, const std::vector\u0026lt;std::vector\u0026lt;int32\u0026gt; \u0026gt; \u0026amp;phone_sets_in, const std::vector\u0026lt;int32\u0026gt; \u0026amp;all_hmm_positions_in, int32 P, std::vector\u0026lt;std::vector\u0026lt;int32\u0026gt; \u0026gt; *questions_out ) 通过对音素自动进行聚类，从而获取问题集；它把音素聚类成一棵树，并且对树中的每一个结点，把从该结点可以到达的所有叶子结点合在一起构成一个问题（该树的一个叶子结点保存着一些音素，一个问题就是一个音素的集合）。\nstd::vector\u0026lt;std::vector\u0026lt;int32\u0026gt; \u0026gt; phone_sets(phone_sets_in); std::vector\u0026lt;int32\u0026gt; phones; for (size_t i = 0; i \u0026lt; phone_sets.size() ;i++) { std::sort(phone_sets[i].begin(), phone_sets[i].end()); for (size_t j = 0; j \u0026lt; phone_sets[i].size(); j++) phones.push_back(phone_sets[i][j]); } std::sort(phones.begin(), phones.end()); std::vector\u0026lt;int32\u0026gt; all_hmm_positions_in(all_hmm_positions_in); SortAndUniq(\u0026amp;all_hmm_positions_in); 读取sets.int中的音素到phone_sets，并将phone_sets中所有音素排序后保存在phones中；读取hmm-position-list参数保存到all_hmm_positions_in中，该参数指定了聚类时所考虑的HMM状态，默认为1，也就是只考虑三状态HMM的中间状态。\nBuildTreeStatsType retained_stats; FilterStatsByKey(stats, kPdfClass, all_hmm_positions_in, true, // retain only the listed positions  \u0026amp;retained_stats); FilterStatsByKey挑选出三音素第二个HMM状态对应的stats保存到retained_stats中，即包含\u0026lt;-1，1\u0026gt;的stats。通过累积统计量部分我们知道，三音素的三个HMM状态都有对应的统计量，但是这里只把与第二个HMM状态相关的统计量留下进行聚类，其他的都暂时扔掉不用。若有100个phone，则有$100^3$个triphone，$3\\times100^3$个HMM state，由于我们只保留中间的HMM state，因此retained_stats中只保存了$100^3$个HMM状态及其统计量。\nstd::vector\u0026lt;BuildTreeStatsType\u0026gt; split_stats; // split by phone. SplitStatsByKey(retained_stats, P, \u0026amp;split_stats); SplitStatsByKey根据三音素的中间音素对retained_stats进行划分，中间音素相同的分到一起，保存到split_stats向量中。由参数P指定根据三音素的第几个音素进行划分，因为此处P是1，所以是三音素的中间音素。retained_stats中$100^3$个统计量按照中间音素划分为100部分保存到split_stats中，每一个部分保存着$100^2$个状态及其统计量。\nstd::vector\u0026lt;Clusterable*\u0026gt; summed_stats; // summed up by phone. SumStatsVec(split_stats, \u0026amp;summed_stats); SumStatsVec把split_stats每个元素中的所有统计量加起来，也就是把中间音素相同的统计量全部相加到一起，得到summed_stats。若有100个phone，那么summed_stats向量就只有100个元素，每个元素保存着x音素作为中间音素的第二个HMM状态的所有统计量的累积。\nstd::vector\u0026lt;Clusterable*\u0026gt; summed_stats_per_set(phone_sets.size(), NULL); for (size_t i = 0; i \u0026lt; phone_sets.size(); i++) { const std::vector\u0026lt;int32\u0026gt; \u0026amp;this_set = phone_sets[i]; summed_stats_per_set[i] = summed_stats[this_set[0]]-\u0026gt;Copy(); for (size_t j = 1; j \u0026lt; this_set.size(); j++) summed_stats_per_set[i]-\u0026gt;Add(*(summed_stats[this_set[j]])); } 根据sets.int指定的集合，累加同一个集合中音素的统计量保存到summed_stats_per_set中。从上面sets.int文件的图片可以看出，该文件的一行就是一个音素的集合，这块代码的作用就是把属于sets.int文件同一行的音素的统计量累加在一起，所以最后summed_stats的维数就是sets.int的行数，一行对应一个统计量。\nTreeClusterOptions topts; topts.kmeans_cfg.num_tries = 10; // This is a slow-but-accurate setting,  // we do it this way since there are typically few phones.  std::vector\u0026lt;int32\u0026gt; assignments; //assignment of phones to clusters. dim == summed_stats.size(). std::vector\u0026lt;int32\u0026gt; clust_assignments; // Parent of each cluster. Dim == #clusters. int32 num_leaves; // number of leaf-level clusters. TreeCluster(summed_stats_per_set, summed_stats_per_set.size(), // max-#clust is all of the points.  NULL, // don\u0026#39;t need the clusters out.  \u0026amp;assignments, \u0026amp;clust_assignments, \u0026amp;num_leaves, topts); 调用TreeCluster，对summed_stats_per_set进行聚类，生成相关信息。TreeCluster是AutomaticallyObtainQuestions最核心的部分。\nTreeCluster BaseFloat TreeCluster(const std::vector\u0026lt;Clusterable*\u0026gt; \u0026amp;points, int32 max_clust, // this is a max only.  std::vector\u0026lt;Clusterable*\u0026gt; *clusters_out, std::vector\u0026lt;int32\u0026gt; *assignments_out, std::vector\u0026lt;int32\u0026gt; *clust_assignments_out, int32 *num_leaves_out, TreeClusterOptions cfg) { TreeClusterer tc(points, max_clust, cfg); BaseFloat ans = tc.Cluster(clusters_out, assignments_out, clust_assignments_out, num_leaves_out); } 该函数首先初始化一个TreeClusterer对象，把统计量points传给该对象；然后调用该对象的Cluster方法获取关于聚类结果的相关信息。ObtainSetsOfPhones根据这些信息就可以生成问题集。\nTreeClusterer对象和Node数据结构 TreeClusterer是自顶向下的树进行聚类的一个对象，TreeClusterer类中定义了树的结点Node结构体。\nstruct Node { bool is_leaf; int32 index; // index into leaf_nodes or nonleaf_nodes as applicable.  Node *parent; Clusterable *node_total; // sum of all data with this node.  struct { std::vector\u0026lt;Clusterable*\u0026gt; points; std::vector\u0026lt;int32\u0026gt; point_indices; BaseFloat best_split; std::vector\u0026lt;Clusterable*\u0026gt; clusters; // [branch_factor]... if we do split.  std::vector\u0026lt;int32\u0026gt; assignments; // assignments of points to clusters.  } leaf; std::vector\u0026lt;Node*\u0026gt; children; // vector of size branch_factor. if non-leaf.  // pointers not owned here but in vectors leaf_nodes_, nonleaf_nodes_. };  Node保存着指向其父亲结点parent和孩子结点children的指针，其中children是一个Node指针类型的vector，vector的大小由TreeClusterOptions中的branch_factor参数指定，这个值默认为2，所以我们这里使用的树是二叉树，每个结点最多只有两个孩子结点。 Node保存着属于该结点的所有统计量之和node_total。统计量就是该结点中的音素对应的所有特征向量的出现次数count_、特征向量之和以及特征向量的平方和，统计量用来计算该结点的似然L(s)。 Node还保存着该结点是否是叶子结点is_leaf，以及是叶子结点时在leaf_nodes中的索引和不是叶子结点时在nonleaf_nodes中的索引index。 如果是叶子结点，保存着leaf对象，leaf成员变量包含：属于该叶子的那些点的统计量points，以及该叶子上拥有的那些点在所有点组成的vector中的索引point_indices（也就是在TreeClusterer对象points成员中的索引）。用best_split保存着对该叶子结点进行最优划分时，获得的最大的似然提升。对该叶子结点划分意味着生成两个新的簇（或者说两个新的孩子结点），assignments中就保存着对该叶子结点进行最优划分后，该叶子结点中的点分别被划分到哪个簇（或者说分别被划分到哪个孩子结点）。  下面看一下TreeClusterer的成员变量\nstd::vector\u0026lt;Node*\u0026gt; leaf_nodes_; std::vector\u0026lt;Node*\u0026gt; nonleaf_nodes_; const std::vector\u0026lt;Clusterable*\u0026gt; \u0026amp;points_; int32 max_clust_; BaseFloat ans_; // objf improvement. std::priority_queue\u0026lt;std::pair\u0026lt;BaseFloat, Node*\u0026gt; \u0026gt; queue_; // contains leaves. TreeClusterOptions cfg_;  TreeClusterer中构造的树的结点分为两类：叶子结点和非叶子结点。叶子结点放在leaf_nodes_中，非叶子结点放在nonleaf_nodes_中，每个结点Node的数据结构中保存着该Node是否为叶子结点以及在这两个向量中的索引。 points中保存着初始化TreeClusterer对象时传递进来的summed_stats_per_set统计量，该对象的聚类过程，就是为了把这些统计量分成一簇簇(cluster)。 queue_是一个优先队列，队列中的每个元素是一个对，这个对的第二个数据保存着结点信息，这个对的第一个数据是对该结点进行划分时所获得的似然的最大提升。使用优先队列则说明，对似然提升最大的结点优先进行划分  TreeClusterer对象初始化 TreeClusterer(const std::vector\u0026lt;Clusterable*\u0026gt; \u0026amp;points, int32 max_clust, TreeClusterOptions cfg): points_(points), max_clust_(max_clust), ans_(0.0), cfg_(cfg) { KALDI_ASSERT(cfg_.branch_factor \u0026gt; 1); Init(); } 首先初始化TreeClusterer对象的一些数据成员，然后调用Init()完成剩余的初始化工作。\nvoid Init() { // Initializes top node  Node *top_node = new Node; top_node-\u0026gt;index = leaf_nodes_.size(); // ==0 currently  top_node-\u0026gt;parent = NULL; // no parent since root of tree  top_node-\u0026gt;is_leaf = true; leaf_nodes_.push_back(top_node); top_node-\u0026gt;leaf.points = points_; top_node-\u0026gt;node_total = SumClusterable(points_); top_node-\u0026gt;leaf.point_indices.resize(points_.size()); for (size_t i = 0;i\u0026lt;points_.size();i++) top_node-\u0026gt;leaf.point_indices[i] = i; FindBestSplit(top_node); // this should always be called when new node is created.  } Init()生成树的根结点top_node，根结点包含传递给该对象的所有点points_，初始化根结点top_node的信息，包括is_leaf=true、index=0、parent=NULL、node_total和leaf中的成员，并把该根结点top_node放进leaf_nodes_中。 每当创建新结点node的时候（一般为叶子结点），应该总是调用FindBestSplit(node)函数。该函数的作用是找到对新结点node的最优划分，即采取该划分时，获得的似然提升最大；并把最优划分时获得的似然提升记录在该node的leaf.best_split中。若该best_split超过cfg指定的似然阈值thresh，则把(best_split, node)对放进优先队列queue中。该函数调用ClusterKMeans找到对属于该node的点的最优的划分和对应的似然提升，ClusterKMeans的细节我们在后面再提及，这里可以先略过。 因为top_node是新建的且唯一的叶子结点，所以在Init()的末尾，调用FindBestSplit(top_node)，找到对top_node的最优划分，将属于top_node的点划分成两簇，每一簇对应一个孩子结点，同时将该划分的最大似然提升记录在top_node-\u0026gt;leaf.best_split中，并把对pair\u0026lt;leaf.best_split, top_node\u0026gt;放进优先队列queue_中。\nTreeClusterer.ClusterKMeans ClusterKMeans是类似K-Means的聚类算法，作用为将当前结点聚类为branch_factor个簇，使得聚类后的簇的似然提升最大。由于K-Means聚类依赖初始值的设定，因此重复调用cfg.num_tries次聚类ClusterKMeansOnce，取ClusterKMeansOnce返回值即似然提升最大的那次。\nClusterKMeansOnce即为单次聚类算法。\nfor (i = 0, j = 0; count != num_points;i = (i+skip)%num_points, j = (j+1)%num_clust, count++){ if ((*clusters_out)[j] == NULL) (*clusters_out)[j] = points[i]-\u0026gt;Copy(); else (*clusters_out)[j]-\u0026gt;Add(*(points[i])); (*assignments_out)[i] = j; } 首先将所有点随机分到num_clust个簇中，num_clust即为接收的branch_factor参数，默认为2。skip是在1~num_points之间且与num_points互质的数，\n每个点的聚类结果保存到assignments_out中，即0或者1分别表示分到左结点或者右结点。每个簇的统计量的累加结果保存到clusters_out中。\nfor (int32 iter = 0;iter \u0026lt; cfg.num_iters;iter++) { // Keep refining clusters by reassigning points.  BaseFloat objf_before; if (cfg.verbose) objf_before = SumClusterableObjf(*clusters_out); BaseFloat impr = RefineClusterers(points, clusters_out, assignments_out, cfg.refine_cfg); BaseFloat objf_after; if (cfg.verbose) objf_after = SumClusterableObjf(*clusters_out); ans += impr; if (impr == 0) break; } 再对每个点随机分配到簇中后，循环调用cfg.num_iters次优化聚类函数RefineClusterers，直至优化后的似然不再增加，返回最终的似然值。\nRefineClusterers通过定义了RefineClusterer对象来调整每个点所属的簇，其过程和K-Means类似，具体原理如下。\nRefineClusterer对象 RefineClusterer的成员变量\ntypedef struct { LocalInt clust; LocalInt time; BaseFloat objf; // Objf of this cluster plus this point (or minus, if own cluster).  } point_info; const std::vector\u0026lt;Clusterable*\u0026gt; points_; std::vector\u0026lt;Clusterable*\u0026gt; *clusters_; std::vector\u0026lt;int32\u0026gt; *assignments_; std::vector\u0026lt;point_info\u0026gt; info_; // size is [num_points_ * cfg_.top_n] std::vector\u0026lt;ClustIndexInt\u0026gt; my_clust_index_; // says for each point, which index 0...cfg_.top_n-1 current \t// corresponds to its own cluster std::vector\u0026lt;LocalInt\u0026gt; clust_time_;\t// Modification time of cluster std::vector\u0026lt;BaseFloat\u0026gt; clust_objf_;\t// [clust], objf for cluster  BaseFloat ans_; // objf improvment  int32 num_clust_; int32 num_points_; int32 t_; RefineClustersOptions cfg_; point_info结构体表示一个\u0026lt;点——簇\u0026gt;的对应信息，包括点对应的簇索引clust、修改次数time和修改引起的似然变化objf。对于每个点，info_保存了top_n-1个似然提升最大的\u0026lt;点——簇\u0026gt;信息，以及当前所属簇的\u0026lt;点——簇\u0026gt;信息，因此point_info的总大小为num_points_ * top_n。\n举例来说，若有100个点，top_n为3，则point_info大小为300。前两列为将该点划分到其它簇，其它簇似然提升最大的2(top_n-1)个簇的相关信息，最后一列为该点当前所属簇的相关信息。如下表所示，point0目前在簇0中，将point0分到簇2和簇1中似然提升最大，分别为0.26和0.22，则info中存储信息为第一行所示。\n注意，前两列是按照似然的提升值$\\Delta objf$从大到小排序，并不是按照提升后的似然绝对值从大到小排序，表中存储的值是提升后(最后一列为下降后)的似然值。\n   point_info index0 index1 index2     point 0 2，0，0.22 1，0，0.28 0，0，0.36   \u0026hellip;      point 99 0，0，0.46 1，0，0.32 2，0，0.25    RefineClusterer对象初始化 RefineClusterer(const std::vector\u0026lt;Clusterable*\u0026gt; \u0026amp;points, std::vector\u0026lt;Clusterable*\u0026gt; *clusters, std::vector\u0026lt;int32\u0026gt; *assignments, RefineClustersOptions cfg) : points_(points), clusters_(clusters), assignments_(assignments), cfg_(cfg) { if (cfg_.top_n \u0026gt; (int32) num_clust_) cfg_.top_n = static_cast\u0026lt;int32\u0026gt; (num_clust_); t_ = 0; my_clust_index_.resize(num_points_); clust_time_.resize(num_clust_,0); clust_objf_.resize(num_clust_); for (int32 i = 0; i \u0026lt; num_clust_; i++) clust_objf_[i] = (*clusters_)[i]-\u0026gt;Objf(); info_.resize(num_points_ * cfg_.top_n); ans_ = 0; InitPoints(); } 首先通过传递的参数初始化RefineClusterer对象的数据成员points_、clusters_和assignments_，分别表示每个点的统计量，每个簇的统计量和每个点对应簇的索引。clust_objf_保存每个簇的似然值，最后InitPoints()为对每个点调用InitPoint()。\nvoid InitPoint(int32 point) { // Find closest clusters to this point.  // distances are really negated objf changes, ignoring terms that don\u0026#39;t vary with the \u0026#34;other\u0026#34; cluster.  std::vector\u0026lt;std::pair\u0026lt;BaseFloat, LocalInt\u0026gt; \u0026gt; distances; distances.reserve(num_clust_-1); int32 my_clust = (*assignments_)[point]; Clusterable *point_cl = points_[point]; for (int32 clust = 0;clust \u0026lt; num_clust_;clust++) { if (clust != my_clust) { Clusterable *tmp = (*clusters_)[clust]-\u0026gt;Copy(); tmp-\u0026gt;Add(*point_cl); BaseFloat other_clust_objf = clust_objf_[clust]; BaseFloat other_clust_plus_me_objf = (*clusters_)[clust]-\u0026gt;ObjfPlus(* (points_[point])); BaseFloat distance = other_clust_objf - other_clust_plus_me_objf; distances.push_back(std::make_pair(distance, (LocalInt)clust)); delete tmp; } } if ((cfg_.top_n-1-1) \u0026gt;= 0) { std::nth_element(distances.begin(), distances.begin() + (cfg_.top_n-1-1), distances.end()); } for (int32 index = 0;index \u0026lt; cfg_.top_n-1;index++) { point_info \u0026amp;info = GetInfo(point, index); int32 clust = distances[index].second; info.clust = clust; BaseFloat distance = distances[index].first; BaseFloat other_clust_objf = clust_objf_[clust]; BaseFloat other_clust_plus_me_objf = -(distance - other_clust_objf); info.objf = other_clust_plus_me_objf; info.time = 0; } // now put the last element in, which is my current cluster.  point_info \u0026amp;info = GetInfo(point, cfg_.top_n-1); info.clust = my_clust; info.time = 0; info.objf = (*clusters_)[my_clust]-\u0026gt;ObjfMinus(*(points_[point])); my_clust_index_[point] = cfg_.top_n-1; } InitPoint目的是计算和当前点距离最小的top_n-1个簇。这里的距离指的是别的簇的似然other_clust_objf减去将当前点归类到别的簇的后的似然other_clust_plus_me_objf，那么距离最小簇的即为似然变化最大的簇。首先将当前点和其他所有簇的\u0026lt;距离，索引\u0026gt;以pair的形式保存到distances中。然后通过std::nth_element使得distances中前面top_n-1个值即为我们想要的距离最小的值。\n对每一个点，将距离最近的top_n-1个簇以及目前所属的簇的信息保存到point_info中。每个点的info的索引为0~top_n-1，top_n-1为目前所属簇的info索引。\nvoid Iterate() { for (iter = 0;iter \u0026lt; num_iters;iter++) { int32 cur_t = t_; for (int32 point = 0;point \u0026lt; num_points_;point++) { ... ProcessPoint(point); } } Init()后，调用Iterate()函数，迭代num_iters次。迭代通过调用ProcessPoint函数，依次处理每一个点。\nint32 self_index = my_clust_index_[point]; point_info \u0026amp;self_info = GetInfo(point, self_index); int32 self_clust = self_info.clust; float own_clust_objf = clust_objf_[self_clust]; float own_clust_minus_me_objf = self_info.objf; for (int32 index = 0;index \u0026lt; cfg_.top_n;index++) { if (index != self_index) { UpdateInfo(point, index); point_info \u0026amp;other_info = GetInfo(point, index); BaseFloat other_clust_objf = clust_objf_[other_info.clust]; BaseFloat other_clust_plus_me_objf = other_info.objf; BaseFloat impr = other_clust_plus_me_objf + own_clust_minus_me_objf - other_clust_objf - own_clust_objf; if (impr \u0026gt; 0) { // better to switch  ans_ += impr; MovePoint(point, index); return; } } } 注意self_index和self_clust的区别，前者是info索引，后者则是目前所属的clust索引。遍历该点对应的point_info，若将当前点划分到别的簇似然有提升，即impr为正，则调用MovePoint将该点移动到别的簇中。每次遍历前，调用UpdateInfo，根据修改次数clust_time_决定是否要更新point_info。\nvoid MovePoint(int32 point, int32 new_index) { t_++; int32 old_index = my_clust_index_[point]; point_info \u0026amp;old_info = GetInfo(point, old_index), \u0026amp;new_info = GetInfo(point, new_index); my_clust_index_[point] = new_index; int32 old_clust = old_info.clust, new_clust = new_info.clust; (*assignments_)[point] = new_clust; (*clusters_)[old_clust]-\u0026gt;Sub( *(points_[point]) ); (*clusters_)[new_clust]-\u0026gt;Add( *(points_[point]) ); UpdateClust(old_clust); UpdateClust(new_clust); } void UpdateClust(int32 clust) { clust_objf_[clust] = (*cluster_)[clust]-\u0026gt;Objf(); clust_time_[clust] = t_; } MovePoint更新my_clust_index_为新的index，更新assignments_为新的clust，新旧簇的统计量分别增加和减去该点的统计量，最后更新新旧簇的似然和修改次数。\nTreeClusterer 至此，TreeClusterer的Init()结束，初始根节点top_node和根节点的似然最大提升保存到优先队列queue_中。接下来调用TreeClusterer的Cluster函数。\nBaseFloat Cluster(std::vector\u0026lt;Clusterable*\u0026gt; *clusters_out, std::vector\u0026lt;int32\u0026gt; *assignments_out, std::vector\u0026lt;int32\u0026gt; *clust_assignments_out, int32 *num_leaves_out) { while (static_cast\u0026lt;int32\u0026gt;(leaf_nodes_.size()) \u0026lt; max_clust_ \u0026amp;\u0026amp; !queue_.empty()) { std::pair\u0026lt;BaseFloat, Node*\u0026gt; pr = queue_.top(); queue_.pop(); ans_ += pr.first; DoSplit(pr.second); } } 依次取出优先队列中元素，调用DoSplit进行分裂。DoSplit函数的作用为生成节点的子节点，然后对每个子节点分别调用FindBestSplit进行分裂，分裂结果如果大于阈值，继续保存到优先队列中。\n至此TreeCluster函数结束，assignments中保存summed_stats_per_set每个元素的叶子结点索引，clust_assignments保存树中每个结点的父结点索引，num_leaves_out保存叶子结点的数目。\nObtainSetsOfPhones // process the information obtained by TreeCluster into the // form we want at output. ObtainSetsOfPhones(phone_sets, assignments, clust_assignments, num_leaves, questions_out); ObtainSetsOfPhones，由TreeCluster得到的信息，生成问题集。先解释下该函数的几个参数：\n  phone_sets：由sets.int生成，每个元素代表sets.int中一行上的音素集\n  assignments：phone_sets中每个元素所属的cluster(叶子结点)\n  clust_assignments：TreeCluster生成的树的每个结点的父结点\n  num_leaves：TreeCluster生成的树的叶子个数\n  question_out：生成的问题集\n  函数内容： a) 得到每个cluster（叶子结点）中的音素集； b) 将子结点的音素集加入到其父结点的音素集中（实现了“把从该结点可以到达的所有叶子结点合在一起构成一个问题”）； c) 把原始的phone_set插入到问题集； d) 过滤问题集的重复项、空项，生成最终的问题集。\n在获得问题集questions.txt后，通过compile-questions设置问题，生成questions.fst。\nQuestionsForKey struct QuestionsForKey { std::vector\u0026lt;std::vector\u0026lt;EventValueType\u0026gt; \u0026gt; initial_questions; RefineClustersOptions refine_opts; ... } QuestionsForKey结构体中定义了key（即phone或state）对应的问题集initial_questions，以及优化问题集时的参数refine_opts。\nQuestions 成员变量 std::vector\u0026lt;QuestionsForKey*\u0026gt; key_options_; std::map\u0026lt;EventKeyType, size_t\u0026gt; key_idx_; key_options_保存某个key的问题集和参数，key_idx_则以\u0026lt;key，index\u0026gt;的形式保存了key以及其在key_options_中的索引。\nSetQuestionsOf void SetQuestionsOf(EventKeyType key, const QuestionsForKey \u0026amp;options_of_key){ options_of_key.Check(); if (key_idx_.count(key) == 0) { key_idx_[key] = key_options_.size(); key_options_.push_back(new QuestionsForKey()); *(key_options_.back()) = options_of_key; }else{ size_t idx = key_idx_[key]; assert(idx \u0026lt; key_options_.size()); *(key_options_[idx]) = options_of_key; } } SetQuestionsOf设置给定key的问题集，若给定的key不在key_idx_中，则插入问题集；否则修改key对应的问题集。\ncompile-questions compile-questions读取上一步生成的questions.txt作为问题集，同时读取topo文件获得每个音素的状态数，生成qustions.fst。\nint32 max_num_pdfclasses = ProcessTopo(topo, questions); 读取qusetions.txt保存到questions，topo保存到topo，检查topo中的每个音素至少出现在一个问题中，返回ProcessTopo获得topo中音素的最大的pdf class数max_num_pdfclasses。\nQuestionsForKey phone_opts(num_iters_refine); phone_opts.initial_questions = questions; for (int32 n = 0; n \u0026lt; N; n++) { qo.SetQuestionsOf(n, phone_opts); } 首先设定三音素中每个位置(0，1，2)的问题集，初始问题集均为questions.txt。\nQuestionsForKey pdfclass_opts(num_iters_refine); std::vector\u0026lt;std::vector\u0026lt;int32\u0026gt; \u0026gt; pdfclass_questions(max_num_pdfclasses-1); for (int32 i = 0; i \u0026lt; max_num_pdfclasses - 1; i++) for (int32 j = 0; j \u0026lt;= i; j++) pdfclass_questions[i].push_back(j); pdfclass_opts.initial_questions = pdfclass_questions; qo.SetQuestionsOf(kPdfClass, pdfclass_opts); 其次设定三音素的hmm状态的问题集，若max_num_pdfclasses为5，则初始问题集均为[[0]，[0，1]，[0，1，2]，[0，1，2，3]]。\n","date":"2019-08-08T00:00:00Z","permalink":"https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%913_%E6%9E%84%E5%BB%BA%E9%97%AE%E9%A2%98%E9%9B%86/","title":"Kaldi源码之决策树3_构建问题集"},{"content":"Build-Tree-Questions 在构建决策树时，我们需要知道的所有信息就是从训练数据的对齐中得到的所有EventType（三音素+HMM状态id），和每个EventType对应的统计量即Clusterable对象。很自然的，我们可以把这两者的对应关系保存成一个对pair\u0026lt;EventType, Clusterable*\u0026gt;，然后把所有的这些对保存成一个向量BuildTreeStatsType。\ntypedef std::vector\u0026lt;std::pair\u0026lt;EventType, Clusterable*\u0026gt; \u0026gt; BuildTreeStatsType; acc-tree-stats   作用：为决策树的构建累积相关的统计量\n  输入：声学模型、特征、对齐\n  输出：统计量tree.acc文件\n  acc-tree-stats $context_opts \\  --ci-phones=$ciphonelist $alidir/final.mdl \u0026#34;$feats\u0026#34; \\  \u0026#34;ark:gunzip -c $alidir/ali.JOB.gz|\u0026#34; $dir/JOB.treeacc 输入的声学模型一般为单音素训练得到的GMM模型。acc-tree-stats的核心就是AccumulateTreeStats函数。\nAccumulateTreeStats void AccumulateTreeStats(const TransitionModel \u0026amp;trans_model, BaseFloat var_floor, int N, // context window size.  int P, // central position.  const std::vector\u0026lt;int32\u0026gt; \u0026amp;ci_phones, //const AccumulateTreeStatsInfo \u0026amp;info,  const std::vector\u0026lt;int32\u0026gt; \u0026amp;alignment, const Matrix\u0026lt;BaseFloat\u0026gt; \u0026amp;features, std::map\u0026lt;EventType, GaussClusterable*\u0026gt; *stats){ ... bool ans = SplitToPhones(trans_model, alignment, \u0026amp;split_alignemnt); ... for (int i = -N; i \u0026lt; static_cast\u0026lt;int\u0026gt;(split_alignment.size()); i++) { // consider window starting at i  if (i + P \u0026gt;= 0 \u0026amp;\u0026amp; i + P \u0026lt; static_cast\u0026lt;int\u0026gt;(split_alignment.size())) { int32 central_phone = trans_model.TransitionIdToPhone(split_alignment[i+P][0]); bool is_ctx_dep = ! std::binary_search(ci_phones.begin(),ci_phones.end(),central_phone); EventType evec; for (int j = 0; j \u0026lt; N; j++) { int phone; if (i + j \u0026gt;= 0 \u0026amp;\u0026amp; i + j \u0026lt; static_cast\u0026lt;int\u0026gt;(split_alignment.size())) phone = trans_model.TransitionIdToPhone(split_alignment[i+j][0]); else phone = 0; if (is_ctx_dep || j == P) evec.push_back(std::make_pair\u0026lt;EventKeyType, EventValueType\u0026gt;(j, phone)); // now for each sub-hmm-position in the window...  } for (int j = 0; j \u0026lt; static_cast\u0026lt;int\u0026gt;(split_alignment[i+P].size());j++) { // for central phone of this window  EventType evec_more(evec); int32 pdf_class = trans_model.TransitionIdToPdfClass(split_alignment[i+P][j]); std::pair\u0026lt;EventKeyType, EventValueType\u0026gt; pr(kPdfClass, pdf_class); evec_more.push_back(pr); std::sort(evec_more.begin(), evec_more.end()); if (stats-\u0026gt;count(evec_more) == 0) { GaussClusterable *new_stats = new GaussClusterable(dim, var_floor); (*stats)[evec_more] = new_stats; } (*stats)[evec_more]-\u0026gt;AddStats(features.Row(cur_pos), 1.0); cur_pos++; } } AccumulateTreeStats函数的形参中，ci_phones指的是上下文无关的音素，一般为silence_phones。在新版本中，var_floor，N，P，ci_phones统一到info中。\n 举例：alignment为[ 3 12 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 18 17 17 17 17 17 17 17 17 17 17 ]\n[ 1202 1201 1201 1204 1203 1203 1203 1206 1205 ]\n[ 806 805 805 805 805 805 805 808 807 810 ]\n[ 794 796 798 797 797 ]\n[ 590 589 589 589 589 589 589 589 589 589 589 589 589 589 589 589 589 589 589 592 594 593 593 593 593 593 593 593 593 593 593 ]\n[ 3 12 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 18]\n对应的phone分别为[sil，n，i3，h， ao3，sil]，phone_id为[1，197，132，129，95，1]。\n 11行首先调用SplitToPhones，将alignment中transition-id根据音素转换为vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;的形式保存到split_alignemnt中，每个元素中保存着隶属于同一个phone的transition-ids。\n13行通过滑动窗口的方式遍历split_alignemnt中每一个音素，每一个音素作为中间音素。滑动窗口的大小即为N，i为三音素窗口的第一个音素，i+P为三音素窗口的中间音素。16行根据transition-id获取对应的phone，17行判断该phone是否在ci_phones中，即是否是上下文无关的音素(一般silence phone为上下文无关的音素)。\n19行遍历该三音素窗口，20-24行获取三音素中每一个音素，将\u0026lt;index，phone-id\u0026gt;保存到evec中，其中index为当前音素在三音素中的索引0，1，2。\n 对于上述例子，evec中保存了\n\u0026laquo;1，1\u0026raquo;，三音素为\u0026lt;NULL，sil，n\u0026gt;\n\u0026laquo;0，1\u0026gt;，\u0026lt;1，197\u0026gt;，\u0026lt;2，132\u0026raquo;，三音素为\u0026lt;sil，n，i3\u0026gt;\n\u0026laquo;0，197\u0026gt;，\u0026lt;1，132\u0026gt;，\u0026lt;2，129\u0026raquo;，三音素为\u0026lt;n，i3，h\u0026gt;\n\u0026laquo;0，132\u0026gt;，\u0026lt;1，129\u0026gt;，\u0026lt;2，95\u0026raquo;，三音素为\u0026lt;i3，h，ao3\u0026gt;\n\u0026laquo;0，129\u0026gt;，\u0026lt;1，95\u0026gt;，\u0026lt;2，1\u0026raquo;，三音素为\u0026lt;h，ao3，sil\u0026gt;\n\u0026laquo;1，1\u0026raquo;，三音素为\u0026lt;ao3，sil，NULL\u0026gt;\n即将split_alignemnt中每一个phone作为central phone。\n 29行开始遍历split_alignemnt中音素的内部，32行根据transition-id获取对应的pdf_class，34行将\u0026lt;kPdfClass, pdf_class\u0026gt;保存到evec_more。因此，对于alignment中每一个transition-id，evec_more中保存了对应的EventType，即三音素和HMM state信息。\n 对于上述例子中transition-id 3，evec_more中保存了 \u0026laquo;-1，0\u0026gt;，\u0026lt;1，1\u0026raquo;\ntransition-id 1202，evec_more中保存了 \u0026laquo;-1，0\u0026gt;，\u0026lt;0，197\u0026gt;，\u0026lt;1，132\u0026gt;，\u0026lt;2，129\u0026raquo;\n 36行往后保存EventType的统计量，相同EventType的统计量累加到一起。\ntreeacc文件 kaldi中上述统计量保存在treeacc文件中，如下：\nBTS 9347 EV 4 -1 0 0 0 1 127 2 113\rT GCL 3 0.01 [\r-1.023983 8.608648 38.34841 ...\r122.1417 705.353 642.767 ... ]\rEV 4 -1 0 0 0 1 287 2 252\rT GCL 1 0.01 [\r-20.32862 25.84675 37.57724 ...\r413.2527 668.0544 1412.049 ... ]\r第1行：BTS 9347表示不同的EventType个数，EV 4 -1 0 0 0 1 127 2代表一个~，4表示后面有4对\u0026lt;int,int\u0026gt;pair，-1 0 0 0 1 29 2 134表示当前EventType为triphone 0/127/113的第0个HMM state。 第2行：GCL 3表示该EventType出现的次数为3(对应count_)； 0.01为var_floor，表示方差如果小于0.01，则等于0.01。 第3行：表示当前EventType对应的特征向量之和 第4行：表示当前EventType对应的特征向量平方之和\nReference  http://www.kaldi-asr.org/doc/clustering.html https://blog.csdn.net/u010731824/article/details/69668647 https://www.isip.piconepress.com/publications/reports/1999/isip/decoder_3/doc/report_061599.pdf https://dl.acm.org/doi/pdf/10.3115/1075812.1075885  ","date":"2019-07-08T00:00:00Z","permalink":"https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%912_%E7%B4%AF%E7%A7%AF%E7%BB%9F%E8%AE%A1%E9%87%8F/","title":"Kaldi源码之决策树2_累积统计量"},{"content":"triphone模型中，为了解决数据稀疏和参数量太大的问题，需要进行聚类和状态绑定。kaldi通过决策树进行状态绑定，即将相似的HMM状态聚类到同一个pdf，以减少总的pdf数。\n决策树理论 决策树是一种自上而下的聚类方法。决策树包含叶子结点和非叶子结点，每一个叶子结点代表一类。\n决策树的每一个结点包含一些状态的集合，我们可以计算该状态集生成对应观测帧的似然。在triphone模型中，将一系列HMM states输入到决策树的根节点中，对这些状态进行提问，根据问题的答案分为左子结点或者右子结点。对于每一个子结点，可以计算该结点的新似然。决策树分类的根据就是使得分裂后的子节点似然之和相比分裂之前增量最大。同时我们通过设置某些阈值来控制决策树何时停止分裂，例如分裂前后似然的增量阈值，结点对应的state occupancy等。\n一般我们会将具有相同中间音素的triphone的同一个位置的所有状态构建一个决策树，如果有63个monophone，那么我们会构建189个决策树(kaldi中这189个决策树会放在一个大的决策树下)。下图为所有中间音素为\u0026quot;zh\u0026quot;的triphone第3个HMM state进行决策树聚类的例子。\n决策树的似然等于每一个叶子结点的似然之和。若$L$为决策树某一个结点的似然，则有 $$ L=\\sum_{t \\in T}{\\sum_{s \\in S}\\gamma_s(t) logP(x_t;\\mu_S,\\Sigma_S)} $$ 其中$S$为该结点对应的状态集，$T$为状态集$S$对应的所有帧，$\\gamma_s(t)$为帧$x_t$由状态$s$生成的后验概率，$P$为概率分布。\n若$P$为高斯分布，则有 $$ \\begin {aligned} log P (x_t;\\mu_S,\\Sigma_S) \u0026amp;= log \\frac{1}{(2\\pi)^{\\frac{D}{2}}{\\lvert{\\Sigma_S}\\rvert}^{\\frac{1}{2}}}exp[-\\frac{1}{2}(x_t-\\mu_S)^T\\Sigma_S^{-1}(x_t-\\mu_S)] \\cr \u0026amp;= -\\frac{1}{2}[Dlog2\\pi + log\\lvert{\\Sigma_S}\\rvert + (x_t-\\mu_S)^T\\Sigma_S^{-1}(x_t-\\mu_S)] \\end {aligned} $$ 那么有： $$ \\begin {aligned} L \u0026amp;= -\\frac{1}{2} (log[(2\\pi)^D|\\Sigma_S|] + D) \\cdot \\sum_{s \\in S}\\sum_{t \\in T} \\gamma_s(t) \\cr \u0026amp;= -\\frac{1}{2}(Dlog(2\\pi) + \\sum_{i=1}^D{log\\Sigma_{ii}} + D) \\cdot \\sum_{s \\in S}\\sum_{t \\in T} \\gamma_s(t) \\end {aligned} $$\ntriphone模型通过决策树分裂时，首先将具有相同中间音素的triphone的同一个位置的所有状态作为根节点，然后生成一系列可选问题集。对每一个叶子结点，选择使得分裂前后的似然增量最大的问题，即选择$\\Delta L$最大的问题进行分裂。重复分裂步骤直到不满足设定的阈值。 $$ \\Delta L = L(parent) - L(leftchild) - L(rightchild) $$\n根据$L$的计算公式，要想计算决策树某个结点的似然，我们只需要知道该结点对应状态集$S$的协方差$\\Sigma_S$，以及状态集中所有状态的state occupancy之和$\\sum_{s \\in S}\\sum_{t \\in T} \\gamma_s(t)$。\n对于协方差矩阵$\\Sigma_S$，由于$\\Sigma_S$为对角阵，对角即为特征每一维的方差$D$，根据$D=E(X^2)-E(X)^2$，我们只需要知道特征向量的和以及特征向量的平方和。\n对于state occupancy，由于我们已经进行了Viterbi强制对齐，即已知每一帧对应的transition-id。根据transition-id，可以得到每一帧对应的triphone和HMM state，即可以统计每一个状态的出现次数，用出现次数代替state occupancy。\n于是，我们就可以发现，与一个状态相关的统计量包括：该状态对应的特征向量的个数(帧数)、特征向量的和、特征向量的平方和。\n那么这些统计量以什么样的形式保存呢，又如何计算呢？\n统计量的表示方法 kaldi中，通过Clusterable对象来描述状态的统计量，主要作用是累加这些统计量和计算目标函数。Clusterable是一个纯虚类，作为kaldi聚类机制的统一接口。在三音素决策树状态绑定这一块，我们主要用到的是继承自该基类的GuassClusterable。\nGuassClusterable GuassClusterable的成员变量为\nBaseFloat count_; Matrix\u0026lt;double\u0026gt; stats_; BaseFloat var_floor; count_保存状态出现的次数，stats_矩阵的第一行保存着该状态应的特征向量的和，stats_矩阵的第二行保存着该状态对应的特征向量的平方和。\n统计量的累加实现函数为GaussClusterable::AddStats\ncount_ += weight; stats_.Row(0).AddVec(weight, vec); stats_.Row(1).AddVec2(weight, vec); 把多个状态的统计量累加在一起，就可以计算这些状态组成的状态集的似然。\n目标函数计算实现函数为GaussClusterable::Objf()\nsize_t dim = stats_.NumCols(); Vector\u0026lt;BaseFloat\u0026gt; vars(dim); for (size_t d = 0; d \u0026lt; dim; d++) { BaseFloat mean(stats_(0, d) / count_), var = stats_(1, d) / count_ - mean * mean ; var = std::max(var, var_floor_); vars(d) = var; } BaseFloat ans = -0.5 * (vars.SumLog() + M_LOG_2PI * dim); ... return ans * count_; 统计量描述后，目标函数也可以计算了。那么问题来了，我们又如何描述triphone对应的某个状态呢，显然需要描述triphone的每个phone以及该状态的HMM state位置。\n状态的表示方法 EventType 首先描述状态对应的triphone，可以通过pair\u0026lt;int,int\u0026gt;来表示，其中第一个int值代表triphone的位置0，1，2；第二个int值为phone-id。\n有了triphone后，还需要确定该状态是triphone的第几个状态，即HMM state。同样可以通过pair\u0026lt;int,int\u0026gt;来表示，为了和音素区分，第一个值取-1表示描述的是状态信息，第二个值为状态编号(对于三状态HMM，取0，1，2)。通过这四对pair\u0026lt;int,int\u0026gt;即可描述triphone对应的某个状态。\nkaldi定义了EventType这一数据结构来描述状态：\ntypedef std::vector\u0026lt;std::pair\u0026lt;EventKeyType,EventValueType\u0026gt; \u0026gt; EventType; 这里的EventKeyType和EventValueType都是int32的别名。\n举个例子，假设我们当前的triphone为a-b+c，音素a,b,c的phone-id分别是10，11，12，假设使用标准的3状态HMM拓扑结构；那么该triphone的第二个HMM状态可表示为：\nEventType e = { (-1, 1), (0, 10), (1, 11), (2,12) };\r在可以描述状态及其统计量后，就可以通过决策树进行聚类了。那么问题又来了，如何表示决策树呢?\n决策树的表示方法 EventMap EventMap是保存决策树的一种方法，它是一个多态纯虚类，不能够被实例化。\nEventMap有三个子类ConstantEventMap、SplitEventMap、TableEventMap，实现了EventMap接口，每种子类都有不同的功能，下面具体介绍这三个子类。\nConstantEventMap ConstantEventMap表示决策树的叶子结点。\n假设我们已经构建好了一个决策树，对某一个EventType，我们从决策树的树根开始问问题，比如左边的音素属于问题集1吗（每个问题集都是一些音素的集合）？右边的音素属于问题集20吗？根据对问题的回答我们就会进入决策树的不同分支，直到到达这个决策树的某一叶子结点，若该叶子结点保存着pdf-id，那么我们就得到了EventType对应的pdf-id。 前面我们讲到，可以用EventAnwserType表示pdf-id，那么叶子结点就只需要保存一个EventAnwserType类型的变量answer_，用来保存该叶子结点对应的pdf-id即可。\nSplitEventMap SplitEventMap表示决策树的非叶子结点。\n给定一个EventType，在决策树的每一个非叶子结点，我们都会对其进行提问以决定进入该结点的哪个分支，比如“左边的音素属于问题集1吗？”、“右边的音素属于问题集20吗？”。那么我们该怎么表示“左边”、“右边”呢？我们可以用EventKeyType类型的变量来表示这个位置信息，我们将其命名为key_——当其取值为0时，表示对左边的音素提问；其取值1时，表示对中间的音素提问；其取值2时，表示对右边的音素提问；取值为-1时，表示对HMM state提问。 在论文中遇见的手工制作的问题类似这样：“左边的音素是鼻音吗？”、“右边的音素是元音吗？”。那么如何表示问题“鼻音”、“元音”这些概念呢？其实鼻音就是一些音素的集合，元音也是一些音素的集合。其实问题的本质其实就是一些音素的集合(或者状态的集合)。我们可以用EventValueType类型的变量表示一个音素，用vector类型的变量表示音素集，我们把这个变量命名为yes_set_。\n此时，我们在每个非叶子结点所问的问题变成“第key_个位置的音素属于音素集合yes_set_吗？” 。当第key_个位置的音素属于yes_set_时，我们进入命名为yes的孩子结点；否则进入命名为no的孩子结点。因为孩子结点可以是叶子结点也可以是非叶子结点，所以用EventMap 来表示指向两个孩子结点的指针。 综上所述，表示决策树非叶子结点的SplitEventMap所需的数据成员包括：提问的位置key_，问题yes_set_以及两个孩子指针*yes_和*no_。\nEventKeyType key_; ConstIntegerSet\u0026lt;EventValueType\u0026gt; yes_set_; // more efficient Map function. EventMap *yes_; EventMap *no_; TableEventMap 一般来说，对每个中间音素的每个状态都要建立一棵决策树进行状态绑定，比如说有63个不同音素，每个音素3个HMM状态，则需要建立63x3=189个决策树。但是Kaldi中想把这189个决策树放进一棵大树里面，这棵大树的189个叶子结点分别是189个决策树的起点；我们随后对这189个叶子结点的每一个进行扩展，每个叶子结点都扩展成一棵决策树，整个完整的大决策树就生成了。当然，这棵大树也用EventMap表示。注意在SplitEventMap上进行一次划分最多生成两个叶子结点，TableEventMap则可以直接生成多个叶子结点。 TableEventMap的数据成员包括进行划分的位置key_，以及指向对其划分后的所有子结点的指针向量std::vector\u0026lt;EventMap\u0026gt; table_。\n决策树现在也可以表示了，那么如何进行聚类呢。\n聚类 Map 在给定EventType后，我们希望通过决策树得到这个EventType对应的pdf-id。EventMap中定义了EventAnswerType表示pdf-id， EventAnswerType也是int32的别名。因此，决策树聚类可以表示为EventType到EventAnwserType的映射。\nEventMap中Map接口来完成这种映射：\nvirtual bool Map(const EventType \u0026amp;event, EventAnswerType *ans) const = 0; 三种EventMap具有相同的成员函数接口，但是其具体实现不太一样，具体实现和不同EventMap的功能有关。\nSplitEventMap::Map() 首先调用LookUp查找EventType第key个位置的值value，若该value在yes_set_中，则递归调用yes_孩子的Map()；若该value不在yes_set_中，则递归调用no_孩子的Map()。\nvirtual bool Map(const EventType \u0026amp;event, EventAnswerType *ans) const { EventValueType value; if (Lookup(event, key_, \u0026amp;value)) { // if (std::binary_search(yes_set_.begin(), yes_set_.end(), value)) {  if (yes_set_.count(value)) { return yes_-\u0026gt;Map(event, ans); } return no_-\u0026gt;Map(event, ans); } return false; } ConstantEventMap::Map() 直接将ans_置为answer_\nvirtual bool Map(const EventType \u0026amp;event, EventAnswerType *ans) const { *ans = answer_; return true; } TableEventMap::Map() 若EventType第key个位置的值tmp在table的范围内，当table的第tmp个元素存在且不为空时，对TE的第tmp个孩子结点，也就是table的第tmp个元素递归调用Map()函数，直到到达叶子结点CE，返回该CE的pdf-id。\nvirtual bool Map(const EventType \u0026amp;event, EventAnswerType *ans) const { EventValueType tmp; *ans = -1; // means no answer  if (Lookup(event, key_, \u0026amp;tmp) \u0026amp;\u0026amp; tmp \u0026gt;= 0 \u0026amp;\u0026amp; tmp \u0026lt; (EventValueType)table_.size() \u0026amp;\u0026amp; table_[tmp] != NULL) { return table_[tmp]-\u0026gt;Map(event, ans); } return false; } 至此，状态、统计量、决策树都可以描述了，下面可以开始进行统计量的累积。\nReference  https://blog.csdn.net/u010731824/article/details/69668647  ","date":"2019-06-08T00:00:00Z","permalink":"https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%911_%E7%90%86%E8%AE%BA%E5%92%8C%E6%8F%8F%E8%BF%B0/","title":"Kaldi源码之决策树1_理论和描述"},{"content":"kaldi中TransitionModel用来更新HMM的转移概率，通过hmm-topology描述每个phone的拓扑结构，包括phone对应的state和以及state之间的初始转移概率。通过transition-model定义了transition的基本单元transition-state和转移路径transition-id。\nhmm-topology 以yesno的topo为例，yesno中有三个音素：SIL、Y、N ，phone id 分别为1、2、3。其中SIL有0~4共5个状态，5为non-emitting。yes/no均含有0~2共3个状态，3为non-emitting。topo中还定义了每个phone对应的状态之间的初始转移概率。\nKaldi通过HmmTopology类表示topo结构，在HmmTopology中定义了HmmState结构体来描述HMM-state，包含HMM-state对应的pdf-class和转移的目标状态、初始转移概率。\nstruct HmmState { int32 pdf_class; std::vector\u0026lt;std::pair\u0026lt;int32, BaseFloat\u0026gt; \u0026gt; transitions; ... } typedef std::vector\u0026lt;HmmState\u0026gt; TopologyEntry; TopologyEntry中包含一个phone对应的所有HMM-state以及这些HMM-state之间的转移路径和转移概率，即一个phone的完整topo结构。\nhmm-topology的成员变量为：\nstd::vector\u0026lt;int32\u0026gt; phones_; std::vector\u0026lt;int32\u0026gt; phone2idx_; std::vector\u0026lt;TopologyEntry\u0026gt; entries_; hmm-topology类包含了上图中完整的topo信息，包括所有音素、每个音素对应的HMM-state和HMM-state间的转移路径和转移概率。\ntransition-model transition中相关的概念：\n phone：音素，id是从1开始的整数。 HMM-state：每个音素的state的id，从0开始的整数。 pdf-id：每个state相对应的GMM概率密度函数id，这个值是全局唯一从0开始的整数。在triphone中，pdf-id会替换为决策树聚类后的实际pdf-id。pdf-id又可以分为forward-pdf-id和self-loop-pdf-id，默认pdf-id=forward-pdf-id=self-loop-pdf-id。 transition-state：抽象出来的转移状态，对于monophone，和HMM-state一一对应；对于triphone，对应于上下文音素绑定的状态。用(phone,HMM-state,pdf-id)表示，从1开始。 transition-index：表示一个transition-state的转移路径的index，在每个状态内从0开始的整数。 transition-id：所有transition-state的转移路径的id，全局唯一从1开始的整数，跟(transition-state,transition-index)一一对应。  通过show-transitions可以查看这些变量的具体值，如下表。\ntransition-model中定义了triples,state2id_和id2state，分别表示transition-state对应的triples，每个transition-state对应的的第一个transition-id，每个transition-id对应的transition-state。\nstd::vector\u0026lt;Triple\u0026gt; triples_; // indexed by transition-state - 1 std::vector\u0026lt;int32\u0026gt; state2id_; // indexed by transition-state std::vector\u0026lt;int32\u0026gt; id2state_; // indexed by transition-id ... 通过这三个变量，transition-model中实现了一系列transition概念的转换，包括：\n(phone, HMM-state, pdf-id) \u0026lt;\u0026mdash;\u0026gt; transition-state\n(transition-state, transition-index) \u0026lt;\u0026mdash;\u0026gt; transition-id\nAccumulate 和GMM一样，HMM也有累积值用于参数更新，HMM的累积值比较简单，即每一个transition-id对应的帧数或者weights。\nvoid Accumulate(BaseFloat prob, int32 trans_id, Vector\u0026lt;double\u0026gt; *stats) const { (*stats)(trans_id) += prob; } HMM的累积值和GMM的三个累积值一起写入到acc文件中。一个acc文件实例：\n [ 0 5404 267 563 4 2336 7 298 1 14 4942 1 571 25 16 4069 26 ...... 18 4 2]\r\u0026lt;NUMPDFS\u0026gt; 1336 \u0026lt;GMMACCS\u0026gt; \u0026lt;VECSIZE\u0026gt; 39 \u0026lt;NUMCOMPONENTS\u0026gt; 5 \u0026lt;FLAGS\u0026gt; 15 \u0026lt;OCCUPANCY\u0026gt; [ 1561.451 779.4418 1555.19 1559.775 782.1424 ]\r\u0026lt;MEANACCS\u0026gt; [\r-28744.81 -7369.43 3880.741 15083.26 13627.56 10871.53 .... -14880.82 -4227.551 1365.723 6937.694 6367.03 4801.088 ...\r... ]\r\u0026lt;DIAGVARACCS\u0026gt; [\r768666.9 205197.4 161435.7 286381.9 285285.2 266058 ...\r... ]\r\u0026lt;/GMMACCS\u0026gt; \u0026lt;GMMACCS\u0026gt; \u0026lt;VECSIZE\u0026gt; 39 \u0026lt;NUMCOMPONENTS\u0026gt; 2 \u0026lt;FLAGS\u0026gt; 15 \u0026lt;OCCUPANCY\u0026gt; [ 16.8601 18.1399 ]\r\u0026lt;MEANACCS\u0026gt; [\r...\r第一行为HMM累积值向量，即每一个transiton-id对应的帧数，向量的长度比transiton-id个数多1，第一位固定为0。下面则为每一个GMM的累积值\u0026lt;OCCUPANCY\u0026gt;、\u0026lt;MEANACCS\u0026gt;和\u0026lt;DIAGVARACCS\u0026gt;。\nUpdate 由于已知观测序列和状态序列，HMM的参数更新过程为监督学习过程，因此根据伯努利大数定律的结论\u0026quot;频率的极限是概率\u0026quot;可以求出HMM的转移概率。\n$$ a_{ij} = \\frac{A_{ij}}{\\sum_{j=1}^N{A_{ij}}} $$\n其中$a_{ij}$为状态$i$到状态$j$的转移概率，$A_{ij}$为状态$i$到状态$j$的转移的频数。\nKaldi从累积量stat中获取和transiton-state对应的每一个transition-id的帧数。\nReference  https://blog.csdn.net/david_tym/article/details/98994879 http://www.kaldi-asr.org/doc/classkaldi_1_1HmmTopology.html  ","date":"2019-05-08T00:00:00Z","permalink":"https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bhmm/","title":"Kaldi源码之hmm"},{"content":"estimate-full-gmm和estimate-diag-gmm用来对GMM进行极大似然估计(MLE)，二者仅在协方差矩阵数据结构不同，下面均以estimate-full-gmm为例。\nGMM GMM的分布函数为\n$$ p(x|\\mu,\\Sigma) = \\sum_{m=1}^{M} c_m \\mathcal{N} (x;\\mu_m,\\Sigma_m) $$\n其中$c_m$为第$m$个component的混合系数,$\\sum_{m=1}^M c_m =1 $。\n定义指示函数$1_{[z_t=m]}$，如果$x_t$来自第m个component，则$1_{[z_t=m]} = 1$，否则$1_{[z_t=m]} = 0$。\n现在假设$1_{[z_t=m]}$已知，即$x_t$属于哪一个component是已知的话，我们就可以统计每一个component对应的帧数：\n$$ N_m=\\sum^N_{t=1} 1_{[z_t=m]} $$\n那么GMM参数可以估计为：\n$$ \\begin {aligned} \u0026amp;c_m = \\frac{\\sum_{t=1}^N 1_{[z_t=m]}}{N} = \\frac{N_m}{N} \\cr \u0026amp;\\mu_m= \\frac{\\sum_{t=1}^N 1_{[z_t=m]} \\mathbf{x_t}}{N_m} \\cr \u0026amp;\\Sigma_m= \\frac{\\sum_{t=1}^N 1_{[z_t=m]}(\\mathbf{x_t}-\\mu_m)(\\mathbf{x_t}-\\mu_m)^T}{N_m} \u0026amp;\\end {aligned} $$\n然而$1_{[z_t=m]}$是未知的，我们并不知道每一帧$x_t$属于哪一个component。但是，我们可以计算后验概率$P(m|x_t)$，即第m个 component生成$x_t$的概率。 $$ P(m|x_t) = \\frac{P(x_t|m)P(m)} {P(x_t)} = \\frac{P(x_t|m)P(m)} {\\sum^M_{n=1}P(x_t|n)P(n)} $$\n其中后验概率$P(m|\\mathbf{x_t})$也称为component occupation probability或者responsibility。\n此时，我们可以通过后验概率$P(m|x_t)$计算每一个component对应的\u0026quot;soft counts\u0026quot;: $N_m^*$\n$$ N_m^* = \\sum_{t=1}^N P(m|x_t) $$\n我们可以想象为对每一帧$x_t$，以$P(m|x_t)$的概率分配到第m个component上。这种分配是一种soft assignment，区别于k-means的hard assignment。\n在得到后验概率$P(m|x_t)$，参数可以估计为：\n$$ \\begin {aligned} \u0026amp;c_m = \\frac{\\sum_{t=1}^N P(m|x_t)}{N} = \\frac{N_m^*} {N} \\cr \u0026amp;\\mu_m= \\frac{\\sum_{t=1}^N P(m|x_t) x_t}{\\sum_{t=1}^N P(m|x_t)} = \\frac {\\sum_{t=1}^N P(m|x_t) x_t}{N_m^*} \\cr \u0026amp;\\Sigma_m= \\frac{\\sum_{t=1}^N P(m|x_t)(x_t-\\mu_m)(x_t-\\mu_m)^T}{\\sum_{t=1}^N P(m|x_t)} = \\frac{\\sum_{t=1}^N P(m|x_t)(x_t-\\mu_m)(x_t-\\mu_m)^T}{N_m^*} \u0026amp;\\end {aligned} $$\n由于：\n$$ P(m|x_t) = \\frac{P(x_t|m)P(m)} {P(x_t)} = \\frac{P(x_t|m)P(m)} {\\sum^M_{n=1}P(x_t|n)P(n)} $$\n因此，为了计算$P(m|x_t)$，必须知道$P(m)$和$P(x_t|m)$，其中$P(m)$为每一个component的先验概率，即为GMM参数中的$c_m$；$P(x_t|m)$为每一个component的分布函数，即为$\\mathcal{N} (x;\\mu_m,\\Sigma_m)$，因此必须知道GMM的参数。反过来，GMM的参数又得在知道$P(m|x_t)$后通过极大似然来估计。问题此时陷入了一个死循环。\n此时可以通过EM算法来解决该问题，首先初始化GMM参数，在E步通过现有的参数计算$P(m|x_t)$，在M步通过计算的$P(m|x_t)$来更新GMM的参数，然后不断迭代直至收敛。\nEM算法 EM算法是一种迭代算法，用于含有隐变量(Hidden variable)的概率模型参数的最大似然估计。\n 先初始化参数 E-step：依据当前参数，通过贝叶斯公式计算$x_t$的后验概率$P(m|x_t)$。  $$ \\begin {aligned} P(m|x_t) \u0026amp;= \\frac{P(x_t|m)P(m)} {\\sum^M_{n=1}P(x_t|n)P(n)} \\cr \u0026amp;=\\frac{c_m \\mathcal{N}(x_t;\\mu_m,\\Sigma_m)}{\\sum_{n=1}^M c_n \\mathcal{N}(_t;\\mu_n,\\Sigma_n)} \\end {aligned} $$\n通过gmm.ComponentPosteriors函数，首先计算第m个component的loglike,然后进行softmax归一化即得到posterior。\n M-step：计算新一轮迭代的模型参数  $$ \\begin {aligned} c_m \u0026amp;= \\frac{\\sum_{t=1}^N P(m|x_t)}{N} \\cr \\mu_m \u0026amp;= \\frac{\\sum_{t=1}^N P(m|x_t) x_t}{\\sum_{t=1}^N P(m|x_t)} \\cr \\Sigma_m \u0026amp;= \\frac{\\sum_{t=1}^N P(m|x_t)(x_t-\\mu_m)(x_t-\\mu_m)^T}{\\sum_{t=1}^N P(m|x_t)} \\cr \u0026amp;\\end {aligned} $$\n 重复计算 E-step 和 M-step 直至收敛  类MlEstimateFullGmm (AccumFullGmm) Vector\u0026lt;double\u0026gt; occupancy_;\rMatrix\u0026lt;double\u0026gt; mean_accumulator_;\rstd::vector\u0026lt;SpMatrix\u0026lt;double\u0026gt; \u0026gt; covariance_accumulator_;\r类中定义了参数更新时所需的三个累积量，其中occupancy_可以视为GMM每个component对应的\u0026quot;帧数\u0026quot;。\nAccumulateForComponent 对于单个component而言，上述三个累积量为：\n$$ \\begin {aligned} occupancy_m \u0026amp;= \\sum_{t=1}^N P(m|x_t) \\cr mean_accumulator_m \u0026amp;= \\sum_{t=1}^N P(m|x_t) x_t \\cr covariance_accumulator_m \u0026amp;= \\sum_{t=1}^N P(m|x_t) x_t x_t^T \\end {aligned} $$\nAccumulateFromPosteriors 计算GMM的累积量，原理和AccumulateForComponent一致，只是计算的对象变为了GMM的所有分量。\nAccumulateFromFull \u0026amp; AccumulateFromDiag 首先通过gmm.ComponentPosteriors计算GMM每一个component的后验，然后调用AccumulateFromPosteriors计算GMM的累积量。\nUpdate 根据上诉三个累积值更新GMM参数：\n$$ \\begin {aligned} \u0026amp;c_m=\\frac{occupancy_m}{N} \\cr \u0026amp;\\mu_m=\\frac{mean_accumulator_m} {occupancy_m} \\cr \u0026amp;\\Sigma_m=\\frac{covariance_accumulator_m} {occupancy_m} - \\mu_m \\mu_m^T \u0026amp;\\end {aligned} $$\n$$ \\begin {aligned} 注： \\Sigma_m \u0026amp;= \\frac{\\sum_{t=1}^N P(m|\\mathbf{x_t})(\\mathbf{x_t}-\\mu_m)(\\mathbf{x_t}-\\mu_m)^T}{\\sum_{t=1}^N P(m|\\mathbf{x_t})} \\cr \u0026amp;= \\frac{ \\sum_{t=1}^N P(m|\\mathbf{x_t}) (\\mathbf{x_t} \\mathbf{x_t^T} - \\mathbf{x_t} \\mu_m^T -\\mu_m\\mathbf{x_t^T} + \\mu_m \\mu_m^T)} {\\sum_{t=1}^N P(m|\\mathbf{x_t})} \\cr \u0026amp;= \\frac{ \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t} \\mathbf{x_t^T} - \\mu_m^T \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t} - \\mu_m \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t^T} + \\mu_m \\mu_m^T \\sum_{t=1}^N P(m|\\mathbf{x_t}) } {\\sum_{t=1}^N P(m|\\mathbf{x_t})} \\cr \u0026amp;= \\frac{ \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t} \\mathbf{x_t^T} - \\mu_m^T \\mu_m \\sum_{t=1}^N P(m|\\mathbf{x_t}) - \\mu_m\\mu_m^T \\sum_{t=1}^N P(m|\\mathbf{x_t}) + \\mu_m \\mu_m^T \\sum_{t=1}^N P(m|\\mathbf{x_t}) } {\\sum_{t=1}^N P(m|\\mathbf{x_t})} \\cr \u0026amp;= \\frac{ \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t} \\mathbf{x_t^T} - \\mu_m \\mu_m^T \\sum_{t=1}^N P(m|\\mathbf{x_t}) } {\\sum_{t=1}^N P(m|\\mathbf{x_t})} \\cr \u0026amp;= \\frac{ \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t} \\mathbf{x_t^T}} {\\sum_{t=1}^N P(m|\\mathbf{x_t})} - \\mu_m \\mu_m^T \\cr \\end {aligned} $$\nestimate-am-diag-gmm $$ \\begin{matrix} diag-gmm \\xrightarrow{update \\ by} estimate-diag-gmm \\cr \\downarrow{1\\rightarrow*} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\downarrow{1\\rightarrow*} \\cr am-diag-gmm \\xrightarrow{update \\ by} estimate-am-diag-gmm \\end{matrix} $$\nestimate-am-diag-gmm中定义了一个gmm_estimators向量，用于对整个声学模型的所有GMM进行更新。\nfor (size_t i = 0; i \u0026lt; gmm_estimators_.size(); i++) {\rgmm_estimators_[i]-\u0026gt;Update(config, flags, \u0026amp;(am_gmm-\u0026gt;GetPdf(i)), p_obj,\rp_count);\r...\rReference  http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr02-hmmgmm.pdf https://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/13_mog.pdf https://kivy-cn.github.io/Stanford-CS-229-CN/#/Markdown/cs229-notes7b https://zhuanlan.zhihu.com/p/30483076 https://notes.funcwj.cn/2017/05/28/kaldi-gmm/  ","date":"2019-04-06T00:00:00Z","permalink":"https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_2/","title":"Kaldi源码之gmm_2"},{"content":"full-gmm和diag-gmm定义了GMM对象，两者不同之处在于协方差矩阵$\\Sigma$的表达形式。diag-gmm为了减少计算量，认为输入特征的各个维度间是彼此独立的， 此时协方差矩阵变成对角矩阵，对角元素即为方差。\nam-diag-gmm中定义了一个向量densities_，向量中存储的是diag-gmm对象，am-diag-gmm针对的是声学模型中的所有GMM。\nfull-gmm \u0026amp; diag-gmm 在FullGmm类中，成员变量为\nVector\u0026lt;BaseFloat\u0026gt; gconsts_; bool valid_gconsts_; Vector\u0026lt;BaseFloat\u0026gt; weights_; std::vector\u0026lt;SpMatrix\u0026lt;BaseFloat\u0026gt; \u0026gt; inv_covars_; Matrix\u0026lt;BaseFloat\u0026gt; means_invcovars_; 在DiagGmm类中，成员变量为\nVector\u0026lt;BaseFloat\u0026gt; gconsts_; bool valid_gconsts_; Vector\u0026lt;BaseFloat\u0026gt; weights_; Matrix\u0026lt;SpMatrix\u0026lt;BaseFloat\u0026gt; \u0026gt; inv_covars_; Matrix\u0026lt;BaseFloat\u0026gt; means_invcovars_; full-gmm和diag-gmm中并没有直接定义均值向量$\\mu$和协方差矩阵$\\Sigma$，而是定义了协方差矩阵的逆矩阵inv_covars——$\\Sigma^{-1}$，以及均值向量和协方差逆矩阵的乘积向量means_invcovars——$\\mu^T\\Sigma^{-1}$。\n在full-gmm中inv_covars中存储的是$\\Sigma^{-1}$的下三角矩阵，而diag-gmm中存储的则为$\\Sigma^{-1}$的对角向量。\n注：\nPackedMatrix：压缩矩阵，仅存储矩阵的下三角部分，对于$n\\times n$的矩阵仅需存储$ \\frac{n(n+1)}{2} $个元素\nSpMatrix： SymmetricPackedMatrix 对称矩阵，继承自PackedMatrix\nTpMatrix： TriangularPackedMatrix 三角矩阵，继承自PackedMatrix\n由于对称矩阵和三角矩阵都只需存储矩阵的上三角或者下三角，因此可以使用PackedMatrix节约存储空间。\ngconsts_ GMM的概率密度函数$(pdf)$为：\n$$ \\begin {aligned} f(x;\\mu,\\Sigma) \u0026amp;= \\sum_{m=1}^{M} c_m \\mathcal{N} (x;\\mu_m,\\Sigma_m) \\cr \u0026amp;= \\sum_{m=1}^{M}\\frac{c_m}{(2\\pi)^{\\frac{D}{2}}{\\lvert{\\Sigma_m}\\rvert}^{\\frac{1}{2}}}exp[-\\frac{1}{2}(x -\\mu_m)^T\\Sigma^{-1}_m(\\mathbf x-\\mu_m)] \\end {aligned} $$\n其中，$c_m$为GMM第$m$个分量(component)的混合系数，${\\Sigma_m}$是协方差矩阵，${\\mu_m}$是均值向量，$D$是数据的维度，并且有$\\sum_{m=1}^M {c_m}= 1$。\n对于离散语音数据，$f(\\mathbf x;\\mu,\\Sigma)即P(\\mathbf x|\\mu,\\Sigma)$，$loglike$为\n$$ log P(x|\\mu,\\Sigma) = \\sum_{t=1}^N log \\sum_{m=1}^{M} c_m \\mathcal{N} (x_t;\\mu_m,\\Sigma_m) $$\n其中$N$为帧数。\n首先针对某一帧$x_t$，考虑单个分量，$loglike$为：\n$$ \\begin {aligned} log c_m \\mathcal{N} (x_t|\\mu_m,\\Sigma_m) \u0026amp;= log \\frac{c_m}{(2\\pi)^{\\frac{D}{2}}{\\lvert{\\Sigma_m}\\rvert}^{\\frac{1}{2}}}exp[-\\frac{1}{2}(x_t-\\mu_m)^T\\Sigma_m^{-1}(x_t-\\mu_m)] \\cr \u0026amp;= logc_m-\\frac{D}{2}log2\\pi-\\frac{1}{2}log\\lvert{\\Sigma_m}\\rvert-\\frac{1}{2}{x_t^T}\\Sigma_m^{-1}x_t-\\frac{1}{2}\\mu_m^T\\Sigma_m^{-1}\\mu_m+\\mu_m^T\\Sigma_m^{-1}x_t \\end {aligned} $$\n去除和$x$相关的项，得到常量：\n$$ logc_m-\\frac{D}{2}log2\\pi-\\frac{1}{2}log\\lvert{\\Sigma_m}\\rvert-\\frac{1}{2}\\mu_m^T\\Sigma_m^{-1}\\mu_m $$\n即为gconsts_。\n在得到GMM中每个分量的$loglike$后，在进行$log(sum(exp()))$即可得到GMM的$loglike$，再对每一帧进行求和即可得到所有帧的$loglike$。\n函数 ComputeGconsts full-gmm和diag-gmm中计算gconsts_的基本原理一致。\n full-gmm  BaseFloat offset = -0.5 * M_LOG_2PI * dim BaseFloat gc = log(weights_(mix)) + offset BaseFloat logdet = covar.LogPosDefDet() gc -= 0.5*(logdet + VecSpVec(means_invcovars_.Row(mix),covar, means_invcovars_.Row(mix))) offset为$-\\frac{D}{2}log2\\pi$，LogPosDefDet即求解$log\\lvert{\\Sigma_m}\\rvert$的过程，由于协方差矩阵$\\Sigma_m$是对称(半)正定矩阵，因此可以对$\\Sigma_m$进行Cholesky分解: $$ \\Sigma_m = LL^T $$\n其中$L$为下三角矩阵。故：\n$$ \\lvert{\\Sigma_m}\\rvert = \\lvert LL^T \\rvert = \\lvert L \\rvert \\lvert L^T \\rvert = \\prod_{i=1}^n{L^2_{ii}} $$\n$$ log\\lvert{\\Sigma_m}\\rvert = 2 \\sum_{i=1}^n{log L_{ii}} $$\n也就是说，$log\\lvert{ {\\Sigma_m}}\\rvert$的值等价于对${ {\\Sigma_m}}$的Cholesky分解的L矩阵对角线元素之和的2倍。\n$$ \\begin {aligned} \\mu_m^T\\Sigma_m^{-1} \\Sigma_m {(\\mu_m^T \\Sigma_m^{-1}})^T \u0026amp;=\\mu_m^T\\Sigma_m^{-1}\\Sigma_m\\Sigma_m^{-1}\\mu_m \\cr \u0026amp;=\\mu_m^T\\Sigma_m^{-1}\\mu_m \\cr \\end {aligned} $$\n diag-gmm  BaseFloat offset = -0.5 * M_LOG_2PI * dim BaseFloat gc = log(weights_(mix)) + offset for (int32 d = 0; d \u0026lt; dim; d++) { gc += 0.5 * log(inv_vars_(mix, d)) - 0.5 * means_invvars_(mix, d) * means_invvars_(mix, d) / inv_vars_(mix, d) } 由于 $|\\Sigma|*|\\Sigma^{-1}|=1$，故$log|\\Sigma| = -log|\\Sigma^{-1}|$；并且$\\Sigma^{-1}$为对角阵，$log|\\Sigma^{-1}| =\\sum_{i=1}^D log\\Sigma_{ii}$\n在full-gmm和diag-gmm中，$$gc=logc_m -\\frac{D}{2}log2\\pi - \\frac{1}{2}(log\\lvert{{\\Sigma_m}}\\rvert + {\\mu_m^T}{\\Sigma_m^{-1}}{\\mu_m})$$，和上述推导值一致。\nSplit \u0026amp;\u0026amp; Merge split和merge分别用来分裂/合并GMM的分量以增加/减少分量数，最终达到目标分量数。\nsplit函数通过遍历GMM的所有分量，每次对权重最大的分量进行分裂，权重均分为两份，然后对split后的GMM重新计算gconsts_。\n若分裂前分量的参数为$\\lbrace c_1,\\Sigma^{-1}_1,\\mu_1^T\\Sigma^{-1}_1 \\rbrace$ ，分裂后分量的参数变为$$\\lbrace c_1^{'},{\\Sigma^{-1}_1}^{'},{\\mu_1^T\\Sigma^{-1}_1}^{'} \\rbrace$$和$\\lbrace c_2,\\Sigma^{-1}_2,\\mu_2^T\\Sigma^{-1}_2 \\rbrace$，则有:\n$$ \\begin {aligned} \u0026amp;c^{'}_1=c_2=\\frac{c_1}{2} \\cr \u0026amp;{\\Sigma^{-1}_1}^{'}=\\Sigma^{-1}_2=\\Sigma^{-1}_1 \\cr \u0026amp;{\\mu_1^T\\Sigma^{-1}_1}^{'}={\\mu_1^T\\Sigma^{-1}_1} - perturb_factor \\cdot rand_vector \\cr \u0026amp;{\\mu_2^T\\Sigma^{-1}_2}={\\mu_1^T\\Sigma^{-1}_1} + perturb_factor \\cdot rand_vector \\end {aligned} $$\nmerge函数的原理是层次聚类，首先统计每一对分量合并前后$loglike$减小的值delta_like，然后依次选取delta_like最小的一对分量进行合并，合并后更新delta_like并重新计算GMM的gconsts_。\n在merge的过程中，若分量1的参数为$\\lbrace \\mu_1,\\Sigma_1,c_1 \\rbrace$，分量2的参数为$\\lbrace \\mu_2,\\Sigma_2,c_2 \\rbrace$，合并分量1和分量2后的分量参数为$\\lbrace \\mu_3,\\Sigma_3,c_3 \\rbrace$，则有: $$ \\begin {aligned} 令：\u0026amp;p_1=\\frac{c_1}{c_1+c_2},p_2=\\frac{c_2}{c_1+c_2} \\cr 则：\u0026amp;c_3=c_1+c_2 \\cr \u0026amp;\\mu_3=p_1\\mu_1+p_2\\mu_2 \\cr \u0026amp;\\Sigma_3=p_1(\\mu_1\\mu_1^{T}+\\Sigma_1)+p_2(\\mu_2\\mu_2^{T}+\\Sigma_2)-\\mu_3\\mu_3^{T} \\end {aligned} $$\n求得$\\mu_3,\\Sigma_3$后，再计算$\\Sigma_3^{-1}$和$\\mu_3^T\\Sigma_3^{-1}$，并更新delta_like和gconsts_。\nComponentLogLikelihood ComponentLogLikelihood计算单个高斯分量的log-likelihood，前面gconsts_计算了log-likelihood的常量部分，该函数加上和特征$x$相关的部分得到最终的log-likelihood值。 $$ loglike_m=gconsts_m + \\mu_m^T\\Sigma_m^{-1}x - \\frac{1}{2}x^T\\Sigma_m^{-1}x $$\nloglike = VecVec(means_invcovars_.Row(comp_id), data) loglike -= 0.5 * VecSpVec(data, inv_covars_[comp_id], data) return loglike + gconsts_(comp_id) LogLikelihoods LogLikelihoods计算每一个分量的log-likelihood值，原理和ComponentLogLikelihood基本一致，不同的是在计算$x^T\\Sigma_m^{-1}x$时，转为求两个对称矩阵相乘的迹，进而转为对两个对称矩阵做点乘运算，从而利用cblas优化运算。 $$ x^T\\Sigma_m^{-1}x = tr(xx^T\\Sigma_m^{-1}) = xx^T \\cdot \\Sigma_m^{-1} $$\nSpMatrix\u0026lt;BaseFloat\u0026gt; data_sq(dim); data_sq.AddVec2(1.0, data); data_sq.ScaleDiag(0.5); loglikes-\u0026gt;AddMatVec(1.0, means_invcovars_, kNoTrans, data, 1.0); int32 num_comp = NumGauss(); for (int32 mix = 0; mix \u0026lt; num_comp; ++mix) { (*loglikes)(mix) -= TraceSpSpLower(data_sq, inv_covars_[mix]); } data_sq即为$xx^T$，TraceSpSpLower求解两个对称矩阵乘积的迹。\nLogLikelihood LogLikelihood计算给定GMM的log-likelihood值，即对上面LogLikelihoods的计算结果进行$$log(sum(exp()))$$。\nLogLikelihoods(data, \u0026amp;loglikes); BaseFloat log_sum = loglikes.LogSumExp(); ... return log_sum; ComponentPosteriors ComponentPosteriors输出component的后验概率，首先调用LogLikelihoods计算每一个分量的log-likelihood值，然后进行softmax归一化得到每一个分量的posterior。\n根据贝叶斯公式可以计算第$t$帧语音$x_t$是来自第$m$个分量的后验概率$\\gamma_m(t)$：\n$$ \\begin {aligned} \\gamma_m(t) \u0026amp;= p(z_t=m|x_t;c,\\mu,\\Sigma) \\cr \u0026amp;= \\frac{p(x_t|z_t=m;\\mu,\\Sigma)p(z_t=m;c)}{\\sum_{n=1}^M p(x_t|z_t=n;\\mu,\\Sigma)p(z_t=n;c) } \\cr \u0026amp;=\\frac{c_m \\mathcal{N}(x_t;\\mu_m,\\Sigma_m)}{\\sum_{n=1}^M c_n \\mathcal{N}(x_t;\\mu_n,\\Sigma_n)} \\end {aligned} $$\n由于第$m$个分量的$loglike$为：\n$$ loglike(\\mu_m,\\Sigma_m ,c_m|x_t) = log c_m \\mathcal{N}(x_t;\\mu_m,\\Sigma_m) $$\n$$ 故：\\gamma_m(t) = \\frac{exp(loglike(\\mu_m,\\Sigma_m,c_m|x_t))} {\\sum_{n=1}^M exp(loglike(\\mu_n,\\Sigma_n,c_n|x_t))} $$\n因此，对每一个分量的loglike应用Softmax即可得到该分量的posterior。\nLogLikelihoods(data, \u0026amp;loglikes); BaseFloat log_sum = loglikes.ApplySoftMax(); ... posterior-\u0026gt;CopyFromVec(loglikes); return log_sum; am-diag-gmm am-diag-gmm中定义了一个CountStats结构体，结构体中定义了三个变量，分别为pdf(GMM)索引、GMM中分量数和GMM对应的occupancy。\nint32 pdf_index; int32 num_components; BaseFloat occupancy; occupancy实际为GMM对应的state_occs的pow次幂，state_occs中则存储了GMM对应的“帧数”，即该GMM每一个分量对应的“帧数”之和，这里的帧数均为“soft count”。\nstate_occs(i) = gmm_accs.GetAccs(i).occupancy().Sum(); ComputeTargetNumPdfs 该函数目标是计算每一个GMM的目标分量数。首先将每一个GMM的对应的CountStats结构体按照occupancy/component的大小存储到优先队列中，其中num_components为1。然后依次取优先队列中GMM，其对应的分量数+1，直至所有GMM的分量总数达到目标分量数。最后，更新每一个GMM的分量数存储到targets向量中。\nSplitByCount \u0026amp; MergeByCount 两个函数通过分裂或者合并分量使得每一个GMM的分量数等于其目标分量数。首先调用ComputeTargetNumPdfs计算每一个GMM的目标分量数，然后遍历GMM，若当前GMM的分量数小于目标分量数，则split；大于目标分量数则merge。\nClusterGaussiansToUbm 待续。。。\nReference  http://www.kaldi-asr.org/doc/classkaldi_1_1FullGmm.html https://blog.csdn.net/wd18508423052/article/details/94052701  ","date":"2019-03-08T00:00:00Z","permalink":"https://cosmo1995.github.io/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_1/","title":"Kaldi源码之gmm_1"}]