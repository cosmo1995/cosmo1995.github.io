[{"content":"estimate-full-gmm和estimate-diag-gmm用来对GMM进行极大似然估计(MLE)，二者仅在协方差矩阵数据结构不同，下面均以estimate-full-gmm为例。\nGMM GMM的分布函数为\n$$ p(x|\\mu,\\Sigma) = \\sum_{m=1}^{M} c_m \\mathcal{N} (x;\\mu_m,\\Sigma_m) $$\n其中$c_m$为第$m$个component的混合系数,$\\sum_{m=1}^M c_m =1 $。\n定义指示函数$1_{[z_t=m]}$，如果$x_t$来自第m个component，则$1_{[z_t=m]} = 1$，否则$1_{[z_t=m]} = 0$。\n现在假设$1_{[z_t=m]}$已知，即$x_t$属于哪一个component是已知的话，我们就可以统计每一个component对应的帧数：\n$$ N_m=\\sum^N_{t=1} 1_{[z_t=m]} $$\n那么GMM参数可以估计为：\n$$ \\begin {aligned} \u0026amp;c_m = \\frac{\\sum_{t=1}^N 1_{[z_t=m]}}{N} = \\frac{N_m}{N} \\cr \u0026amp;\\mu_m= \\frac{\\sum_{t=1}^N 1_{[z_t=m]} \\mathbf{x_t}}{N_m} \\cr \u0026amp;\\Sigma_m= \\frac{\\sum_{t=1}^N 1_{[z_t=m]}(\\mathbf{x_t}-\\mu_m)(\\mathbf{x_t}-\\mu_m)^T}{N_m} \u0026amp;\\end {aligned} $$\n然而$1_{[z_t=m]}$是未知的，我们并不知道每一帧$x_t$属于哪一个component。但是，我们可以计算后验概率$P(m|x_t)$，即第m个 component生成$x_t$的概率。 $$ P(m|x_t) = \\frac{P(x_t|m)P(m)} {P(x_t)} = \\frac{P(x_t|m)P(m)} {\\sum^M_{n=1}P(x_t|n)P(n)} $$\n其中后验概率$P(m|\\mathbf{x_t})$也称为component occupation probability或者responsibility。\n此时，我们可以通过后验概率$P(m|x_t)$计算每一个component对应的\u0026quot;soft counts\u0026quot;: $N_m^*$\n$$ N_m^* = \\sum_{t=1}^N P(m|x_t) $$\n我们可以想象为对每一帧$x_t$，以$P(m|x_t)$的概率分配到第m个component上。这种分配是一种soft assignment，区别于k-means的hard assignment。\n在得到后验概率$P(m|x_t)$，参数可以估计为：\n$$ \\begin {aligned} \u0026amp;c_m = \\frac{\\sum_{t=1}^N P(m|x_t)}{N} = \\frac{N_m^*} {N} \\cr \u0026amp;\\mu_m= \\frac{\\sum_{t=1}^N P(m|x_t) x_t}{\\sum_{t=1}^N P(m|x_t)} = \\frac {\\sum_{t=1}^N P(m|x_t) x_t}{N_m^*} \\cr \u0026amp;\\Sigma_m= \\frac{\\sum_{t=1}^N P(m|x_t)(x_t-\\mu_m)(x_t-\\mu_m)^T}{\\sum_{t=1}^N P(m|x_t)} = \\frac{\\sum_{t=1}^N P(m|x_t)(x_t-\\mu_m)(x_t-\\mu_m)^T}{N_m^*} \u0026amp;\\end {aligned} $$\n由于：\n$$ P(m|x_t) = \\frac{P(x_t|m)P(m)} {P(x_t)} = \\frac{P(x_t|m)P(m)} {\\sum^M_{n=1}P(x_t|n)P(n)} $$\n因此，为了计算$P(m|x_t)$，必须知道$P(m)$和$P(x_t|m)$，其中$P(m)$为每一个component的先验概率，即为GMM参数中的$c_m$；$P(x_t|m)$为每一个component的分布函数，即为$\\mathcal{N} (x;\\mu_m,\\Sigma_m)$，因此必须知道GMM的参数。反过来，GMM的参数又得在知道$P(m|x_t)$后通过极大似然来估计。问题此时陷入了一个死循环。\n此时可以通过EM算法来解决该问题，首先初始化GMM参数，在E步通过现有的参数计算$P(m|x_t)$，在M步通过计算的$P(m|x_t)$来更新GMM的参数，然后不断迭代直至收敛。\nEM算法 EM算法是一种迭代算法，用于含有隐变量(Hidden variable)的概率模型参数的最大似然估计。\n 先初始化参数 E-step：依据当前参数，通过贝叶斯公式计算$x_t$的后验概率$P(m|x_t)$。  $$ \\begin {aligned} P(m|x_t) \u0026amp;= \\frac{P(x_t|m)P(m)} {\\sum^M_{n=1}P(x_t|n)P(n)} \\cr \u0026amp;=\\frac{c_m \\mathcal{N}(x_t;\\mu_m,\\Sigma_m)}{\\sum_{n=1}^M c_n \\mathcal{N}(_t;\\mu_n,\\Sigma_n)} \\end {aligned} $$\n通过gmm.ComponentPosteriors函数，首先计算第m个component的loglike,然后进行softmax归一化即得到posterior。\n M-step：计算新一轮迭代的模型参数  $$ \\begin {aligned} c_m \u0026amp;= \\frac{\\sum_{t=1}^N P(m|x_t)}{N} \\cr \\mu_m \u0026amp;= \\frac{\\sum_{t=1}^N P(m|x_t) x_t}{\\sum_{t=1}^N P(m|x_t)} \\cr \\Sigma_m \u0026amp;= \\frac{\\sum_{t=1}^N P(m|x_t)(x_t-\\mu_m)(x_t-\\mu_m)^T}{\\sum_{t=1}^N P(m|x_t)} \\cr \u0026amp;\\end {aligned} $$\n 重复计算 E-step 和 M-step 直至收敛  类MlEstimateFullGmm (AccumFullGmm) Vector\u0026lt;double\u0026gt; occupancy_;\rMatrix\u0026lt;double\u0026gt; mean_accumulator_;\rstd::vector\u0026lt;SpMatrix\u0026lt;double\u0026gt; \u0026gt; covariance_accumulator_;\r类中定义了参数更新时所需的三个累积量，其中occupancy_可以视为GMM每个component对应的\u0026quot;帧数\u0026quot;。\nAccumulateForComponent 对于单个component而言，上述三个累积量为：\n$$ \\begin {aligned} occupancy_m \u0026amp;= \\sum_{t=1}^N P(m|x_t) \\cr mean_accumulator_m \u0026amp;= \\sum_{t=1}^N P(m|x_t) x_t \\cr covariance_accumulator_m \u0026amp;= \\sum_{t=1}^N P(m|x_t) x_t x_t^T \\end {aligned} $$\nAccumulateFromPosteriors 计算GMM的累积量，原理和AccumulateForComponent一致，只是计算的对象变为了GMM的所有分量。\nAccumulateFromFull \u0026amp; AccumulateFromDiag 首先通过gmm.ComponentPosteriors计算GMM每一个component的后验，然后调用AccumulateFromPosteriors计算GMM的累积量。\nUpdate 根据上诉三个累积值更新GMM参数：\n$$ \\begin {aligned} \u0026amp;c_m=\\frac{occupancy_m}{N} \\cr \u0026amp;\\mu_m=\\frac{mean_accumulator_m} {occupancy_m} \\cr \u0026amp;\\Sigma_m=\\frac{covariance_accumulator_m} {occupancy_m} - \\mu_m \\mu_m^T \u0026amp;\\end {aligned} $$\n$$ \\begin {aligned} 注： \\Sigma_m \u0026amp;= \\frac{\\sum_{t=1}^N P(m|\\mathbf{x_t})(\\mathbf{x_t}-\\mu_m)(\\mathbf{x_t}-\\mu_m)^T}{\\sum_{t=1}^N P(m|\\mathbf{x_t})} \\cr \u0026amp;= \\frac{ \\sum_{t=1}^N P(m|\\mathbf{x_t}) (\\mathbf{x_t} \\mathbf{x_t^T} - \\mathbf{x_t} \\mu_m^T -\\mu_m\\mathbf{x_t^T} + \\mu_m \\mu_m^T)} {\\sum_{t=1}^N P(m|\\mathbf{x_t})} \\cr \u0026amp;= \\frac{ \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t} \\mathbf{x_t^T} - \\mu_m^T \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t} - \\mu_m \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t^T} + \\mu_m \\mu_m^T \\sum_{t=1}^N P(m|\\mathbf{x_t}) } {\\sum_{t=1}^N P(m|\\mathbf{x_t})} \\cr \u0026amp;= \\frac{ \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t} \\mathbf{x_t^T} - \\mu_m^T \\mu_m \\sum_{t=1}^N P(m|\\mathbf{x_t}) - \\mu_m\\mu_m^T \\sum_{t=1}^N P(m|\\mathbf{x_t}) + \\mu_m \\mu_m^T \\sum_{t=1}^N P(m|\\mathbf{x_t}) } {\\sum_{t=1}^N P(m|\\mathbf{x_t})} \\cr \u0026amp;= \\frac{ \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t} \\mathbf{x_t^T} - \\mu_m \\mu_m^T \\sum_{t=1}^N P(m|\\mathbf{x_t}) } {\\sum_{t=1}^N P(m|\\mathbf{x_t})} \\cr \u0026amp;= \\frac{ \\sum_{t=1}^N P(m|\\mathbf{x_t}) \\mathbf{x_t} \\mathbf{x_t^T}} {\\sum_{t=1}^N P(m|\\mathbf{x_t})} - \\mu_m \\mu_m^T \\cr \\end {aligned} $$\nestimate-am-diag-gmm $$ \\begin{matrix} diag-gmm \\xrightarrow{update \\ by} estimate-diag-gmm \\cr \\downarrow{1\\rightarrow*} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\downarrow{1\\rightarrow*} \\cr am-diag-gmm \\xrightarrow{update \\ by} estimate-am-diag-gmm \\end{matrix} $$\nestimate-am-diag-gmm中定义了一个gmm_estimators向量，用于对整个声学模型的所有GMM进行更新。\nfor (size_t i = 0; i \u0026lt; gmm_estimators_.size(); i++) {\rgmm_estimators_[i]-\u0026gt;Update(config, flags, \u0026amp;(am_gmm-\u0026gt;GetPdf(i)), p_obj,\rp_count);\r...\rReference  http://www.inf.ed.ac.uk/teaching/courses/asr/2019-20/asr02-hmmgmm.pdf https://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/13_mog.pdf https://kivy-cn.github.io/Stanford-CS-229-CN/#/Markdown/cs229-notes7b https://zhuanlan.zhihu.com/p/30483076 https://notes.funcwj.cn/2017/05/28/kaldi-gmm/  ","date":"2019-04-06T00:00:00Z","permalink":"https://example.com/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_2/","title":"Kaldi源码之gmm_2"},{"content":"full-gmm和diag-gmm定义了GMM对象，两者不同之处在于协方差矩阵$\\Sigma$的表达形式。diag-gmm为了减少计算量，认为输入特征的各个维度间是彼此独立的， 此时协方差矩阵变成对角矩阵，对角元素即为方差。\nam-diag-gmm中定义了一个向量densities_，向量中存储的是diag-gmm对象，am-diag-gmm针对的是声学模型中的所有GMM。\nfull-gmm \u0026amp; diag-gmm 在FullGmm类中，成员变量为\nVector\u0026lt;BaseFloat\u0026gt; gconsts_; bool valid_gconsts_; Vector\u0026lt;BaseFloat\u0026gt; weights_; std::vector\u0026lt;SpMatrix\u0026lt;BaseFloat\u0026gt; \u0026gt; inv_covars_; Matrix\u0026lt;BaseFloat\u0026gt; means_invcovars_; 在DiagGmm类中，成员变量为\nVector\u0026lt;BaseFloat\u0026gt; gconsts_; bool valid_gconsts_; Vector\u0026lt;BaseFloat\u0026gt; weights_; Matrix\u0026lt;SpMatrix\u0026lt;BaseFloat\u0026gt; \u0026gt; inv_covars_; Matrix\u0026lt;BaseFloat\u0026gt; means_invcovars_; full-gmm和diag-gmm中并没有直接定义均值向量$\\mu$和协方差矩阵$\\Sigma$，而是定义了协方差矩阵的逆矩阵inv_covars——$\\Sigma^{-1}$，以及均值向量和协方差逆矩阵的乘积向量means_invcovars——$\\mu^T\\Sigma^{-1}$。\n在full-gmm中inv_covars中存储的是$\\Sigma^{-1}$的下三角矩阵，而diag-gmm中存储的则为$\\Sigma^{-1}$的对角向量。\n注：\nPackedMatrix：压缩矩阵，仅存储矩阵的下三角部分，对于$n\\times n$的矩阵仅需存储$ \\frac{n(n+1)}{2} $个元素\nSpMatrix： SymmetricPackedMatrix 对称矩阵，继承自PackedMatrix\nTpMatrix： TriangularPackedMatrix 三角矩阵，继承自PackedMatrix\n由于对称矩阵和三角矩阵都只需存储矩阵的上三角或者下三角，因此可以使用PackedMatrix节约存储空间。\ngconsts_ GMM的概率密度函数$(pdf)$为：\n$$ \\begin {aligned} f(x;\\mu,\\Sigma) \u0026amp;= \\sum_{m=1}^{M} c_m \\mathcal{N} (x;\\mu_m,\\Sigma_m) \\cr \u0026amp;= \\sum_{m=1}^{M}\\frac{c_m}{(2\\pi)^{\\frac{D}{2}}{\\lvert{\\Sigma_m}\\rvert}^{\\frac{1}{2}}}exp[-\\frac{1}{2}(x -\\mu_m)^T\\Sigma^{-1}_m(\\mathbf x-\\mu_m)] \\end {aligned} $$\n其中，$c_m$为GMM第$m$个分量(component)的混合系数，${\\Sigma_m}$是协方差矩阵，${\\mu_m}$是均值向量，$D$是数据的维度，并且有$\\sum_{m=1}^M {c_m}= 1$。\n对于离散语音数据，$f(\\mathbf x;\\mu,\\Sigma)即P(\\mathbf x|\\mu,\\Sigma)$，$loglike$为\n$$ log P(x|\\mu,\\Sigma) = \\sum_{t=1}^N log \\sum_{m=1}^{M} c_m \\mathcal{N} (x_t;\\mu_m,\\Sigma_m) $$\n其中$N$为帧数。\n首先针对某一帧$x_t$，考虑单个分量，$loglike$为：\n$$ \\begin {aligned} log c_m \\mathcal{N} (x_t|\\mu_m,\\Sigma_m) \u0026amp;= log \\frac{c_m}{(2\\pi)^{\\frac{D}{2}}{\\lvert{\\Sigma_m}\\rvert}^{\\frac{1}{2}}}exp[-\\frac{1}{2}(x_t-\\mu_m)^T\\Sigma_m^{-1}(x_t-\\mu_m)] \\cr \u0026amp;= logc_m-\\frac{D}{2}log2\\pi-\\frac{1}{2}log\\lvert{\\Sigma_m}\\rvert-\\frac{1}{2}{x_t^T}\\Sigma_m^{-1}x_t-\\frac{1}{2}\\mu_m^T\\Sigma_m^{-1}\\mu_m+\\mu_m^T\\Sigma_m^{-1}x_t \\end {aligned} $$\n去除和$x$相关的项，得到常量：\n$$ logc_m-\\frac{D}{2}log2\\pi-\\frac{1}{2}log\\lvert{\\Sigma_m}\\rvert-\\frac{1}{2}\\mu_m^T\\Sigma_m^{-1}\\mu_m $$\n即为gconsts_。\n在得到GMM中每个分量的$loglike$后，在进行$log(sum(exp()))$即可得到GMM的$loglike$，再对每一帧进行求和即可得到所有帧的$loglike$。\n函数 ComputeGconsts full-gmm和diag-gmm中计算gconsts_的基本原理一致。\n full-gmm  BaseFloat offset = -0.5 * M_LOG_2PI * dim BaseFloat gc = log(weights_(mix)) + offset BaseFloat logdet = covar.LogPosDefDet() gc -= 0.5*(logdet + VecSpVec(means_invcovars_.Row(mix),covar, means_invcovars_.Row(mix))) offset为$-\\frac{D}{2}log2\\pi$，LogPosDefDet即求解$log\\lvert{\\Sigma_m}\\rvert$的过程，由于协方差矩阵$\\Sigma_m$是对称(半)正定矩阵，因此可以对$\\Sigma_m$进行Cholesky分解: $$ \\Sigma_m = LL^T $$\n其中$L$为下三角矩阵。故：\n$$ \\lvert{\\Sigma_m}\\rvert = \\lvert LL^T \\rvert = \\lvert L \\rvert \\lvert L^T \\rvert = \\prod_{i=1}^n{L^2_{ii}} $$\n$$ log\\lvert{\\Sigma_m}\\rvert = 2 \\sum_{i=1}^n{log L_{ii}} $$\n也就是说，$log\\lvert{ {\\Sigma_m}}\\rvert$的值等价于对${ {\\Sigma_m}}$的Cholesky分解的L矩阵对角线元素之和的2倍。\n$$ \\begin {aligned} \\mu_m^T\\Sigma_m^{-1} \\Sigma_m {(\\mu_m^T \\Sigma_m^{-1}})^T \u0026amp;=\\mu_m^T\\Sigma_m^{-1}\\Sigma_m\\Sigma_m^{-1}\\mu_m \\cr \u0026amp;=\\mu_m^T\\Sigma_m^{-1}\\mu_m \\cr \\end {aligned} $$\n diag-gmm  BaseFloat offset = -0.5 * M_LOG_2PI * dim BaseFloat gc = log(weights_(mix)) + offset for (int32 d = 0; d \u0026lt; dim; d++) { gc += 0.5 * log(inv_vars_(mix, d)) - 0.5 * means_invvars_(mix, d) * means_invvars_(mix, d) / inv_vars_(mix, d) } 由于 $|\\Sigma|*|\\Sigma^{-1}|=1$，故$log|\\Sigma| = -log|\\Sigma^{-1}|$；并且$\\Sigma^{-1}$为对角阵，$log|\\Sigma^{-1}| =\\sum_{i=1}^D log\\Sigma_{ii}$\n在full-gmm和diag-gmm中，$$gc=logc_m -\\frac{D}{2}log2\\pi - \\frac{1}{2}(log\\lvert{{\\Sigma_m}}\\rvert + {\\mu_m^T}{\\Sigma_m^{-1}}{\\mu_m})$$，和上述推导值一致。\nSplit \u0026amp;\u0026amp; Merge split和merge分别用来分裂/合并GMM的分量以增加/减少分量数，最终达到目标分量数。\nsplit函数通过遍历GMM的所有分量，每次对权重最大的分量进行分裂，权重均分为两份，然后对split后的GMM重新计算gconsts_。\n若分裂前分量的参数为$\\lbrace c_1,\\Sigma^{-1}_1,\\mu_1^T\\Sigma^{-1}_1 \\rbrace$ ，分裂后分量的参数变为$$\\lbrace c_1^{'},{\\Sigma^{-1}_1}^{'},{\\mu_1^T\\Sigma^{-1}_1}^{'} \\rbrace$$和$\\lbrace c_2,\\Sigma^{-1}_2,\\mu_2^T\\Sigma^{-1}_2 \\rbrace$，则有:\n$$ \\begin {aligned} \u0026amp;c^{'}_1=c_2=\\frac{c_1}{2} \\cr \u0026amp;{\\Sigma^{-1}_1}^{'}=\\Sigma^{-1}_2=\\Sigma^{-1}_1 \\cr \u0026amp;{\\mu_1^T\\Sigma^{-1}_1}^{'}={\\mu_1^T\\Sigma^{-1}_1} - perturb_factor \\cdot rand_vector \\cr \u0026amp;{\\mu_2^T\\Sigma^{-1}_2}={\\mu_1^T\\Sigma^{-1}_1} + perturb_factor \\cdot rand_vector \\end {aligned} $$\nmerge函数的原理是层次聚类，首先统计每一对分量合并前后$loglike$减小的值delta_like，然后依次选取delta_like最小的一对分量进行合并，合并后更新delta_like并重新计算GMM的gconsts_。\n在merge的过程中，若分量1的参数为$\\lbrace \\mu_1,\\Sigma_1,c_1 \\rbrace$，分量2的参数为$\\lbrace \\mu_2,\\Sigma_2,c_2 \\rbrace$，合并分量1和分量2后的分量参数为$\\lbrace \\mu_3,\\Sigma_3,c_3 \\rbrace$，则有: $$ \\begin {aligned} 令：\u0026amp;p_1=\\frac{c_1}{c_1+c_2},p_2=\\frac{c_2}{c_1+c_2} \\cr 则：\u0026amp;c_3=c_1+c_2 \\cr \u0026amp;\\mu_3=p_1\\mu_1+p_2\\mu_2 \\cr \u0026amp;\\Sigma_3=p_1(\\mu_1\\mu_1^{T}+\\Sigma_1)+p_2(\\mu_2\\mu_2^{T}+\\Sigma_2)-\\mu_3\\mu_3^{T} \\end {aligned} $$\n求得$\\mu_3,\\Sigma_3$后，再计算$\\Sigma_3^{-1}$和$\\mu_3^T\\Sigma_3^{-1}$，并更新delta_like和gconsts_。\nComponentLogLikelihood ComponentLogLikelihood计算单个高斯分量的log-likelihood，前面gconsts_计算了log-likelihood的常量部分，该函数加上和特征$x$相关的部分得到最终的log-likelihood值。 $$ loglike_m=gconsts_m + \\mu_m^T\\Sigma_m^{-1}x - \\frac{1}{2}x^T\\Sigma_m^{-1}x $$\nloglike = VecVec(means_invcovars_.Row(comp_id), data) loglike -= 0.5 * VecSpVec(data, inv_covars_[comp_id], data) return loglike + gconsts_(comp_id) LogLikelihoods LogLikelihoods计算每一个分量的log-likelihood值，原理和ComponentLogLikelihood基本一致，不同的是在计算$x^T\\Sigma_m^{-1}x$时，转为求两个对称矩阵相乘的迹，进而转为对两个对称矩阵做点乘运算，从而利用cblas优化运算。 $$ x^T\\Sigma_m^{-1}x = tr(xx^T\\Sigma_m^{-1}) = xx^T \\cdot \\Sigma_m^{-1} $$\nSpMatrix\u0026lt;BaseFloat\u0026gt; data_sq(dim); data_sq.AddVec2(1.0, data); data_sq.ScaleDiag(0.5); loglikes-\u0026gt;AddMatVec(1.0, means_invcovars_, kNoTrans, data, 1.0); int32 num_comp = NumGauss(); for (int32 mix = 0; mix \u0026lt; num_comp; ++mix) { (*loglikes)(mix) -= TraceSpSpLower(data_sq, inv_covars_[mix]); } data_sq即为$xx^T$，TraceSpSpLower求解两个对称矩阵乘积的迹。\nLogLikelihood LogLikelihood计算给定GMM的log-likelihood值，即对上面LogLikelihoods的计算结果进行$$log(sum(exp()))$$。\nLogLikelihoods(data, \u0026amp;loglikes); BaseFloat log_sum = loglikes.LogSumExp(); ... return log_sum; ComponentPosteriors ComponentPosteriors输出component的后验概率，首先调用LogLikelihoods计算每一个分量的log-likelihood值，然后进行softmax归一化得到每一个分量的posterior。\n根据贝叶斯公式可以计算第$t$帧语音$x_t$是来自第$m$个分量的后验概率$\\gamma_m(t)$：\n$$ \\begin {aligned} \\gamma_m(t) \u0026amp;= p(z_t=m|x_t;c,\\mu,\\Sigma) \\cr \u0026amp;= \\frac{p(x_t|z_t=m;\\mu,\\Sigma)p(z_t=m;c)}{\\sum_{n=1}^M p(x_t|z_t=n;\\mu,\\Sigma)p(z_t=n;c) } \\cr \u0026amp;=\\frac{c_m \\mathcal{N}(x_t;\\mu_m,\\Sigma_m)}{\\sum_{n=1}^M c_n \\mathcal{N}(x_t;\\mu_n,\\Sigma_n)} \\end {aligned} $$\n由于第$m$个分量的$loglike$为：\n$$ loglike(\\mu_m,\\Sigma_m ,c_m|x_t) = log c_m \\mathcal{N}(x_t;\\mu_m,\\Sigma_m) $$\n$$ 故：\\gamma_m(t) = \\frac{exp(loglike(\\mu_m,\\Sigma_m,c_m|x_t))} {\\sum_{n=1}^M exp(loglike(\\mu_n,\\Sigma_n,c_n|x_t))} $$\n因此，对每一个分量的loglike应用Softmax即可得到该分量的posterior。\nLogLikelihoods(data, \u0026amp;loglikes); BaseFloat log_sum = loglikes.ApplySoftMax(); ... posterior-\u0026gt;CopyFromVec(loglikes); return log_sum; am-diag-gmm am-diag-gmm中定义了一个CountStats结构体，结构体中定义了三个变量，分别为pdf(GMM)索引、GMM中分量数和GMM对应的occupancy。\nint32 pdf_index; int32 num_components; BaseFloat occupancy; occupancy实际为GMM对应的state_occs的pow次幂，state_occs中则存储了GMM对应的“帧数”，即该GMM每一个分量对应的“帧数”之和，这里的帧数均为“soft count”。\nstate_occs(i) = gmm_accs.GetAccs(i).occupancy().Sum(); ComputeTargetNumPdfs 该函数目标是计算每一个GMM的目标分量数。首先将每一个GMM的对应的CountStats结构体按照occupancy/component的大小存储到优先队列中，其中num_components为1。然后依次取优先队列中GMM，其对应的分量数+1，直至所有GMM的分量总数达到目标分量数。最后，更新每一个GMM的分量数存储到targets向量中。\nSplitByCount \u0026amp; MergeByCount 两个函数通过分裂或者合并分量使得每一个GMM的分量数等于其目标分量数。首先调用ComputeTargetNumPdfs计算每一个GMM的目标分量数，然后遍历GMM，若当前GMM的分量数小于目标分量数，则split；大于目标分量数则merge。\nClusterGaussiansToUbm 待续。。。\nReference  http://www.kaldi-asr.org/doc/classkaldi_1_1FullGmm.html https://blog.csdn.net/wd18508423052/article/details/94052701  ","date":"2019-03-08T00:00:00Z","permalink":"https://example.com/p/kaldi%E6%BA%90%E7%A0%81%E4%B9%8Bgmm_1/","title":"Kaldi源码之gmm_1"}]